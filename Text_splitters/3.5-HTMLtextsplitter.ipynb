{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Header 1': 'Welcome to My Website'}, page_content='Welcome to My Website'),\n",
       " Document(metadata={'Header 1': 'Welcome to My Website'}, page_content='This is a basic webpage using HTML and CSS.')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "\n",
    "html_string = \"\"\"\" \n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>My Basic Webpage</title>\n",
    "    <style>\n",
    "        body {\n",
    "            font-family: Arial, sans-serif;\n",
    "            text-align: center;\n",
    "            margin: 50px;\n",
    "        }\n",
    "        h1 {\n",
    "            color: #333;\n",
    "        }\n",
    "        p {\n",
    "            color: #666;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to My Website</h1>\n",
    "    <p>This is a basic webpage using HTML and CSS.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "head_split = [(\"h1\",\"Header 1\")]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(head_split)\n",
    "\n",
    "html = html_splitter.split_text(html_string)\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Jump to content  \\nMain menu  \\nMain menu  \\nmove to sidebar  \\nhide  \\nNavigation  \\nMain page  \\nContents  \\nCurrent events  \\nRandom article  \\nAbout Wikipedia  \\nContact us  \\nContribute  \\nHelp  \\nLearn to edit  \\nCommunity portal  \\nRecent changes  \\nUpload file  \\nSpecial pages  \\nSearch  \\nSearch  \\nAppearance  \\nDonate  \\nCreate account  \\nLog in  \\nPersonal tools  \\nDonate  \\nCreate account  \\nLog in  \\nPages for logged out editors  \\nlearn more  \\nContributions  \\nTalk  \\nCentralNotice'),\n",
       " Document(metadata={'Header 2': 'Contents'}, page_content='Contents'),\n",
       " Document(metadata={}, page_content='move to sidebar  \\nhide  \\n(Top)  \\n1  \\nHistory  \\nToggle History subsection  \\n1.1  \\nPre-1970  \\n1.2  \\n1970–1990  \\n1.3  \\nPractical speech recognition  \\n1.3.1  \\n2000s  \\n1.3.2  \\n2010s  \\n2  \\nModels, methods, and algorithms  \\nToggle Models, methods, and algorithms subsection  \\n2.1  \\nHidden Markov models  \\n2.2  \\nDynamic time warping (DTW)-based speech recognition  \\n2.3  \\nNeural networks  \\n2.3.1  \\nDeep feedforward and recurrent neural networks  \\n2.4  \\nEnd-to-end automatic speech recognition  \\n3  \\nApplications  \\nToggle Applications subsection  \\n3.1  \\nIn-car systems  \\n3.2  \\nEducation  \\n3.3  \\nHealth care  \\n3.3.1  \\nMedical documentation  \\n3.3.2  \\nTherapeutic use  \\n3.4  \\nMilitary  \\n3.4.1  \\nHigh-performance fighter aircraft  \\n3.4.2  \\nHelicopters  \\n3.4.3  \\nTraining air traffic controllers  \\n3.5  \\nTelephony and other domains  \\n3.6  \\nPeople with disabilities  \\n3.7  \\nFurther applications  \\n4  \\nPerformance  \\nToggle Performance subsection  \\n4.1  \\nAccuracy  \\n4.2  \\nSecurity concerns  \\n5  \\nFurther information  \\nToggle Further information subsection  \\n5.1  \\nConferences and journals  \\n5.2  \\nBooks  \\n5.3  \\nSoftware  \\n6  \\nSee also  \\n7  \\nReferences  \\n8  \\nFurther reading  \\nToggle the table of contents  \\nSpeech recognition  \\n49 languages  \\nالعربية  \\nAzərbaycanca  \\nتۆرکجه  \\nবাংলা  \\nБеларуская  \\nCatalà  \\nČeština  \\nDansk  \\nDeutsch  \\nEesti  \\nΕλληνικά  \\nEspañol  \\nEsperanto  \\nEuskara  \\nفارسی  \\nFrançais  \\nGalego  \\n한국어  \\nՀայերեն  \\nहिन्दी  \\nBahasa Indonesia  \\nÍslenska  \\nItaliano  \\nעברית  \\nМакедонски  \\nBahasa Melayu  \\nNederlands  \\n日本語  \\nNorsk bokmål  \\nپښتو  \\nPolski  \\nPortuguês  \\nRomână  \\nРусский  \\nSimple English  \\nSlovenčina  \\nСрпски / srpski  \\nSuomi  \\nSvenska  \\nதமிழ்  \\nతెలుగు  \\nไทย  \\nTürkçe  \\nУкраїнська  \\nاردو  \\nTiếng Việt  \\nWalon  \\n粵語  \\n中文  \\nEdit links  \\nArticle  \\nTalk  \\nEnglish  \\nRead  \\nEdit  \\nView history  \\nTools  \\nTools  \\nmove to sidebar  \\nhide  \\nActions  \\nRead  \\nEdit  \\nView history  \\nGeneral  \\nWhat links here  \\nRelated changes  \\nUpload file  \\nPermanent link  \\nPage information  \\nCite this page  \\nGet shortened URL  \\nDownload QR code  \\nPrint/export  \\nDownload as PDF  \\nPrintable version  \\nIn other projects  \\nWikimedia Commons  \\nWikidata item  \\nAppearance  \\nmove to sidebar  \\nhide  \\nFrom Wikipedia, the free encyclopedia  \\nesi <esi:include src=\"/esitest-fa8a495983347898/content\" />  \\nAutomatic conversion of spoken language into text  \\n.mw-parser-output .hatnote{font-style:italic}.mw-parser-output div.hatnote{padding-left:1.6em;margin-bottom:0.5em}.mw-parser-output .hatnote i{font-style:normal}.mw-parser-output .hatnote+link+.hatnote{margin-top:-0.5em}@media print{body.ns-0 .mw-parser-output .hatnote{display:none!important}}  \\nFor the human linguistic concept, see  .  \\nSpeech perception  \\nis an   subfield of   and   that develops   and technologies that enable the recognition and   of spoken language into text by computers. It is also known as   ( ),   or   ( ). It incorporates knowledge and research in the  ,   and   fields. The reverse process is  .  \\nSpeech recognition  \\ninterdisciplinary  \\ncomputer science  \\ncomputational linguistics  \\nmethodologies  \\ntranslation  \\nautomatic speech recognition  \\nASR  \\ncomputer speech recognition  \\nspeech-to-text  \\nSTT  \\ncomputer science  \\nlinguistics  \\ncomputer engineering  \\nspeech synthesis  \\nSome speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated   into the system. The system analyzes the person\\'s specific voice and uses it to fine-tune the recognition of that person\\'s speech, resulting in increased accuracy. Systems that do not use training are called \"speaker-independent\"  systems. Systems that use training are called \"speaker dependent\".  \\nvocabulary  \\n1  \\n[  \\n]  \\nSpeech recognition applications include   such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"),   appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics,  speech-to-text processing (e.g.,   or  ), and   (usually termed  ). Automatic   is used in education such as for spoken language learning.  \\nvoice user interfaces  \\ndomotic  \\n2  \\n[  \\n]  \\nword processors  \\nemails  \\naircraft  \\ndirect voice input  \\npronunciation assessment  \\nThe term   or   refers to identifying the speaker, rather than what they are saying.   can simplify the task of   in systems that have been trained on a specific person\\'s voice or it can be used to   or verify the identity of a speaker as part of a security process.  \\nvoice recognition  \\n3  \\n[  \\n]  \\n4  \\n[  \\n]  \\n5  \\n[  \\n]  \\nspeaker identification  \\n6  \\n[  \\n]  \\n7  \\n[  \\n]  \\n8  \\n[  \\n]  \\nRecognizing the speaker  \\ntranslating speech  \\nauthenticate  \\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in   and  . The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.  \\ndeep learning  \\nbig data'),\n",
       " Document(metadata={'Header 2': 'History'}, page_content='History'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nThe key areas of growth were: vocabulary size, speaker independence, and processing speed.'),\n",
       " Document(metadata={'Header 3': 'Pre-1970'}, page_content='Pre-1970'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n– Three Bell Labs researchers, Stephen Balashek,  R. Biddulph, and K. H. Davis built a system called \"Audrey\"  for single-speaker digit recognition. Their system located the   in the power spectrum of each utterance.  \\n1952  \\n9  \\n[  \\n]  \\n10  \\n[  \\n]  \\nformants  \\n11  \\n[  \\n]  \\n–   developed and published the  .  \\n1960  \\nGunnar Fant  \\nsource-filter model of speech production  \\n–   demonstrated its 16-word \"Shoebox\" machine\\'s speech recognition capability at the  .  \\n1962  \\nIBM  \\n1962 World\\'s Fair  \\n12  \\n[  \\n]  \\n–   (LPC), a   method, was first proposed by   of   and Shuzo Saito of   (NTT), while working on speech recognition.  \\n1966  \\nLinear predictive coding  \\nspeech coding  \\nFumitada Itakura  \\nNagoya University  \\nNippon Telegraph and Telephone  \\n13  \\n[  \\n]  \\n– Funding at   dried up for several years when, in 1969, the influential   wrote an open letter that was critical of and defunded speech recognition research.  This defunding lasted until Pierce retired and   took over.  \\n1969  \\nBell Labs  \\nJohn Pierce  \\n14  \\n[  \\n]  \\nJames L. Flanagan  \\nwas the first person to take on continuous speech recognition as a graduate student at   in the late 1960s. Previous systems required users to pause after each word. Reddy\\'s system issued spoken commands for playing  .  \\nRaj Reddy  \\nStanford University  \\nchess  \\nAround this time Soviet researchers invented the   (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary.  DTW processed speech by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique carried on. Achieving speaker independence remained unsolved at this time period.  \\ndynamic time warping  \\n15  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': '1970–1990'}, page_content='1970–1990'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\n–   funded five years for  , speech recognition research seeking a minimum vocabulary size of 1,000 words. They thought   would be key to making progress in speech  , but this later proved untrue.   ,  ,   and   all participated in the program.  This revived speech recognition research post John Pierce's letter.  \\n1971  \\nDARPA  \\nSpeech Understanding Research  \\nspeech  \\nunderstanding  \\nrecognition  \\n16  \\n[  \\n]  \\nBBN  \\nIBM  \\nCarnegie Mellon  \\nStanford Research Institute  \\n17  \\n[  \\n]  \\n18  \\n[  \\n]  \\n– The IEEE Acoustics, Speech, and Signal Processing group held a conference in Newton, Massachusetts.  \\n1972  \\n– The first   was held in  , which since then has been a major venue for the publication of research on speech recognition.  \\n1976  \\nICASSP  \\nPhiladelphia  \\n19  \\n[  \\n]  \\nDuring the late 1960s   developed the mathematics of   at the  . A decade later, at CMU, Raj Reddy's students   and   began using the   (HMM) for speech recognition.  James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education.  The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.  \\nLeonard Baum  \\nMarkov chains  \\nInstitute for Defense Analysis  \\nJames Baker  \\nJanet M. Baker  \\nhidden Markov model  \\n20  \\n[  \\n]  \\n21  \\n[  \\n]  \\nBy the   IBM's   team created a voice activated typewriter called Tangora, which could handle a 20,000-word vocabulary  Jelinek's statistical approach put less emphasis on emulating the way the human brain processes and understands speech in favor of using statistical modeling techniques like HMMs. (Jelinek's group independently discovered the application of HMMs to speech. ) This was controversial with linguists since HMMs are too simplistic to account for many common features of human languages.  However, the HMM proved to be a highly useful way for modeling speech and replaced dynamic time warping to become the dominant speech recognition algorithm in the 1980s.  \\nmid-1980s  \\nFred Jelinek's  \\n22  \\n[  \\n]  \\n21  \\n[  \\n]  \\n23  \\n[  \\n]  \\n24  \\n[  \\n]  \\n25  \\n[  \\n]  \\n– Dragon Systems, founded by James and  ,  was one of IBM's few competitors.  \\n1982  \\nJanet M. Baker  \\n26  \\n[  \\n]\"),\n",
       " Document(metadata={'Header 3': 'Practical speech recognition'}, page_content='Practical speech recognition'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nThe 1980s also saw the introduction of the   language model.  \\nn-gram  \\n– The   allowed language models to use multiple length n-grams, and   used HMM to recognize languages (both in software and in hardware specialized processors, e.g.  ).  \\n1987  \\nback-off model  \\nCSELT  \\n27  \\n[  \\n]  \\nRIPAC  \\nMuch of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the   with 4 MB ram.  It could take up to 100 minutes to decode just 30 seconds of speech.  \\nPDP-10  \\n28  \\n[  \\n]  \\n29  \\n[  \\n]  \\nTwo practical products were:  \\n– was released the   with up to 4096 words support, of which only 64 could be held in   at a time.  \\n1984  \\nApricot Portable  \\nRAM  \\n30  \\n[  \\n]  \\n– a recognizer from Kurzweil Applied Intelligence  \\n1987  \\n– Dragon Dictate, a consumer product released in 1990    deployed the Voice Recognition Call Processing service in 1992 to route telephone calls without the use of a human operator.  The technology was developed by   and others at Bell Labs.  \\n1990  \\n31  \\n[  \\n]  \\n32  \\n[  \\n]  \\nAT&T  \\n33  \\n[  \\n]  \\nLawrence Rabiner  \\nBy this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary.  Raj Reddy's former student,  , developed the   system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the   in 1993. Raj Reddy's student   joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.  \\n28  \\n[  \\n]  \\nXuedong Huang  \\nSphinx-II  \\nspeech recognition group at Microsoft  \\nKai-Fu Lee  \\n, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the   operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became   in 2005.   originally licensed software from Nuance to provide speech recognition capability to its digital assistant  .  \\nLernout & Hauspie  \\nWindows XP  \\nNuance  \\nApple  \\nSiri  \\n34  \\n[  \\n]\"),\n",
       " Document(metadata={'Header 4': '2000s'}, page_content='2000s'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nIn the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and   (GALE). Four teams participated in the EARS program:  , a team led by   with   and  ,  , and a team composed of  ,   and  . EARS funded the collection of the Switchboard telephone   containing 260 hours of recorded conversations from over 500 speakers.  The GALE program focused on   and   broadcast news speech.  \\'s first effort at speech recognition came in 2007 after hiring some researchers from Nuance.  The first product was  , a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems.   is now supported in over 30 languages.  \\nGlobal Autonomous Language Exploitation  \\nIBM  \\nBBN  \\nLIMSI  \\nUniv. of Pittsburgh  \\nCambridge University  \\nICSI  \\nSRI  \\nUniversity of Washington  \\nspeech corpus  \\n35  \\n[  \\n]  \\nArabic  \\nMandarin  \\nGoogle  \\n36  \\n[  \\n]  \\nGOOG-411  \\nGoogle Voice Search  \\nIn the United States, the   has made use of a type of speech recognition for   since at least 2006.  This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA\\'s EARS\\'s program and  \\'s  .  \\nNational Security Agency  \\nkeyword spotting  \\n37  \\n[  \\n]  \\nIARPA  \\nBabel program  \\nIn the early 2000s, speech recognition was still dominated by traditional approaches such as   combined with feedforward  . \\nToday, however, many aspects of speech recognition have been taken over by a   method called   (LSTM), a   published by   &   in 1997.  LSTM RNNs avoid the   and can learn \"Very Deep Learning\" tasks  that require memories of events that happened thousands of discrete time steps ago, which is important for speech.\\nAround 2007, LSTM trained by Connectionist Temporal Classification (CTC)  started to outperform traditional speech recognition in certain applications.  In 2015, Google\\'s speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through   to all smartphone users.   , a type of neural network based solely on \"attention\", have been widely adopted in computer vision  and language modeling,  sparking the interest of adapting such models to new domains, including speech recognition.  Some recent papers reported superior performance levels using transformer models for speech recognition, but these models usually require large scale training datasets to reach high performance levels.  \\nhidden Markov models  \\nartificial neural networks  \\n38  \\n[  \\n]  \\ndeep learning  \\nLong short-term memory  \\nrecurrent neural network  \\nSepp Hochreiter  \\nJürgen Schmidhuber  \\n39  \\n[  \\n]  \\nvanishing gradient problem  \\n40  \\n[  \\n]  \\n41  \\n[  \\n]  \\n42  \\n[  \\n]  \\nGoogle Voice  \\n43  \\n[  \\n]  \\nTransformers  \\n44  \\n[  \\n]  \\n45  \\n[  \\n]  \\n46  \\n[  \\n]  \\n47  \\n[  \\n]  \\n48  \\n[  \\n]  \\n49  \\n[  \\n]  \\n50  \\n[  \\n]  \\nThe use of deep feedforward (non-recurrent) networks for   was introduced during the later part of 2009 by   and his students at the University of Toronto and by Li Deng  and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and the University of Toronto which was subsequently expanded to include IBM and Google (hence \"The shared views of four research groups\" subtitle in their 2012 review paper).  A Microsoft research executive called this innovation \"the most dramatic change in accuracy since 1979\".  In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%.  This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.  \\nacoustic modeling  \\nGeoffrey Hinton  \\n51  \\n[  \\n]  \\n52  \\n[  \\n]  \\n53  \\n[  \\n]  \\n54  \\n[  \\n]  \\n55  \\n[  \\n]  \\n55  \\n[  \\n]  \\nIn the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s. \\nBut these methods never won over the non-uniform internal-handcrafting  /  (GMM-HMM) technology based on generative models of speech trained discriminatively.  A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing  and weak temporal correlation structure in the neural predictive models.  All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009–2010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks for speech recognition.  \\n56  \\n[  \\n]  \\n57  \\n[  \\n]  \\n58  \\n[  \\n]  \\nGaussian mixture model  \\nhidden Markov model  \\n59  \\n[  \\n]  \\n60  \\n[  \\n]  \\n61  \\n[  \\n]  \\n62  \\n[  \\n]  \\n53  \\n[  \\n]  \\n54  \\n[  \\n]  \\n63  \\n[  \\n]  \\n64  \\n[  \\n]'),\n",
       " Document(metadata={'Header 4': '2010s'}, page_content='2010s'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nBy early 2010s   recognition, also called voice recognition  was clearly differentiated from   recognition, and speaker independence was considered a major breakthrough. Until then, systems required a \"training\" period.  A 1987 ad for a doll had carried the tagline \"Finally, the doll that understands you.\" – despite the fact that it was described as \"which children could train to respond to their voice\".  \\nspeech  \\n65  \\n[  \\n]  \\n66  \\n[  \\n]  \\n67  \\n[  \\n]  \\nspeaker  \\n12  \\n[  \\n]  \\nIn 2017, Microsoft researchers reached a historical human parity milestone of transcribing conversational telephony speech on the widely benchmarked Switchboard task. Multiple deep learning models were used to optimize speech recognition accuracy. The speech recognition word error rate was reported to be as low as 4 professional human transcribers working together on the same benchmark, which was funded by IBM Watson speech team on the same task.  \\n68  \\n[  \\n]'),\n",
       " Document(metadata={'Header 2': 'Models, methods, and algorithms'}, page_content='Models, methods, and algorithms'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nBoth   and   are important parts of modern statistically based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as   or  .  \\nacoustic modeling  \\nlanguage modeling  \\ndocument classification  \\nstatistical machine translation'),\n",
       " Document(metadata={'Header 3': 'Hidden Markov models'}, page_content='Hidden Markov models'),\n",
       " Document(metadata={'Header 3': 'Hidden Markov models'}, page_content='[  \\nedit  \\n]  \\nMain article:  \\nHidden Markov model  \\nModern general-purpose speech recognition systems are based on hidden Markov models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time scale (e.g., 10 milliseconds), speech can be approximated as a  . Speech can be thought of as a   for many stochastic purposes.  \\nstationary process  \\nMarkov model  \\nAnother reason why HMMs are popular is that they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of  -dimensional real-valued vectors (with   being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of   coefficients, which are obtained by taking a   of a short time window of speech and decorrelating the spectrum using a  , then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each  , will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.  \\nn  \\nn  \\ncepstral  \\nFourier transform  \\ncosine transform  \\nphoneme  \\nDescribed above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need   for the   (so that phonemes with different left and right context would have different realizations as HMM states); it would use   to normalize for a different speaker and recording conditions; for further speaker normalization, it might use vocal tract length normalization (VTLN) for male-female normalization and   (MLLR) for more general speaker adaptation. The features would have so-called   and   to capture speech dynamics and in addition, might use   (HLDA); or might skip the delta and delta-delta coefficients and use   and an  -based projection followed perhaps by   linear discriminant analysis or a   transform (also known as  , or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum   (MMI), minimum classification error (MCE), and minimum phone error (MPE).  \\ncontext dependency  \\nphonemes  \\ncepstral normalization  \\nmaximum likelihood linear regression  \\ndelta  \\ndelta-delta coefficients  \\nheteroscedastic linear discriminant analysis  \\nsplicing  \\nLDA  \\nheteroscedastic  \\nglobal semi-tied co variance  \\nmaximum likelihood linear transform  \\nmutual information  \\nDecoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the   to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information and combining it statically beforehand (the  , or FST, approach).  \\nViterbi algorithm  \\nfinite state transducer  \\nA possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function ( ) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the   approach) or as a subset of the models (a  ). Re scoring is usually done by trying to minimize the   (or an approximation thereof) Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the  , though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score   represented as weighted   with   represented themselves as a   verifying certain assumptions.  \\nre scoring  \\nN-best list  \\nlattice  \\nBayes risk  \\n69  \\n[  \\n]  \\nLevenshtein distance  \\nlattices  \\nfinite state transducers  \\nedit distances  \\nfinite state transducer  \\n70  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': 'Dynamic time warping (DTW)-based speech recognition'}, page_content='Dynamic time warping (DTW)-based speech recognition'),\n",
       " Document(metadata={'Header 3': 'Dynamic time warping (DTW)-based speech recognition'}, page_content='[  \\nedit  \\n]  \\nMain article:  \\nDynamic time warping  \\nDynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.  \\nDynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics\\xa0– indeed, any data that can be turned into a linear representation can be analyzed with DTW.  \\nA well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are \"warped\" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.'),\n",
       " Document(metadata={'Header 3': 'Neural networks'}, page_content='Neural networks'),\n",
       " Document(metadata={'Header 3': 'Neural networks'}, page_content=\"[  \\nedit  \\n]  \\nMain article:  \\nArtificial neural network  \\nNeural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification,  phoneme classification through multi-objective evolutionary algorithms,  isolated word recognition,   , audiovisual speaker recognition and speaker adaptation.  \\n71  \\n[  \\n]  \\n72  \\n[  \\n]  \\n73  \\n[  \\n]  \\naudiovisual speech recognition  \\nmake fewer explicit assumptions about feature statistical properties than HMMs and have several qualities making them more attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words,  early neural networks were rarely successful for continuous recognition tasks because of their limited ability to model temporal dependencies.  \\nNeural networks  \\n74  \\n[  \\n]  \\nOne approach to this limitation was to use neural networks as a pre-processing, feature transformation or dimensionality reduction,  step prior to HMM based recognition. However, more recently, LSTM and related recurrent neural networks (RNNs),  Time Delay Neural Networks(TDNN's),  and transformers  have demonstrated improved performance in this area.  \\n75  \\n[  \\n]  \\n39  \\n[  \\n]  \\n43  \\n[  \\n]  \\n76  \\n[  \\n]  \\n77  \\n[  \\n]  \\n78  \\n[  \\n]  \\n48  \\n[  \\n]  \\n49  \\n[  \\n]  \\n50  \\n[  \\n]\"),\n",
       " Document(metadata={'Header 3': 'Neural networks', 'Header 4': 'Deep feedforward and recurrent neural networks'}, page_content='Deep feedforward and recurrent neural networks'),\n",
       " Document(metadata={'Header 3': 'Neural networks', 'Header 4': 'Deep feedforward and recurrent neural networks'}, page_content='[  \\nedit  \\n]  \\nMain article:  \\nDeep learning  \\nDeep neural networks and denoising   are also under investigation. A deep feedforward neural network (DNN) is an   with multiple hidden layers of units between the input and output layers.  Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.  \\nautoencoders  \\n79  \\n[  \\n]  \\nartificial neural network  \\n53  \\n[  \\n]  \\n80  \\n[  \\n]  \\nA success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted. \\n  See comprehensive reviews of this development and of the state of the art as of October 2014 in the recent Springer book from Microsoft Research.  See also the related background of automatic speech recognition and the impact of various machine learning paradigms, notably including  , in\\nrecent overview articles.  \\n81  \\n[  \\n]  \\n82  \\n[  \\n]  \\n83  \\n[  \\n]  \\n84  \\n[  \\n]  \\ndeep learning  \\n85  \\n[  \\n]  \\n86  \\n[  \\n]  \\nOne fundamental principle of   is to do away with hand-crafted   and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features,  showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.\\nThe true \"raw\" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.  \\ndeep learning  \\nfeature engineering  \\n87  \\n[  \\n]  \\n88  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': 'End-to-end automatic speech recognition'}, page_content='End-to-end automatic speech recognition'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nSince 2014, there has been much research interest in \"end-to-end\" ASR. Traditional phonetic-based (i.e., all  -based model) approaches required separate components and training for the pronunciation, acoustic, and  . End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a   is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices.  Consequently, modern commercial ASR systems from   and   (as of 2017 ) are deployed on the cloud and require a network connection as opposed to the device locally.  \\nHMM  \\nlanguage model  \\nn-gram language model  \\n89  \\n[  \\n]  \\nGoogle  \\nApple  \\n[update]  \\nThe first attempt at end-to-end ASR was with   (CTC)-based systems introduced by   of   and Navdeep Jaitly of the   in 2014.  The model consisted of   and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to   assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later,   expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English.  In 2016,   presented  ,  the first end-to-end sentence-level lipreading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset.  A large-scale CNN-RNN-CTC architecture was presented in 2018 by   achieving 6 times better performance than human experts.  In 2019,   launched two CNN-CTC ASR models, Jasper and QuarzNet, with an overall performance WER of 3%.  Similar to other deep learning applications,   and   are important strategies for reusing and extending the capabilities of deep learning models, particularly due to the high costs of training models from scratch, and the small size of available corpus in many languages and/or specific domains.  \\nConnectionist Temporal Classification  \\nAlex Graves  \\nGoogle DeepMind  \\nUniversity of Toronto  \\n90  \\n[  \\n]  \\nrecurrent neural networks  \\nconditional independence  \\nBaidu  \\n91  \\n[  \\n]  \\nUniversity of Oxford  \\nLipNet  \\n92  \\n[  \\n]  \\n93  \\n[  \\n]  \\nGoogle DeepMind  \\n94  \\n[  \\n]  \\nNvidia  \\n95  \\n[  \\n]  \\n96  \\n[  \\n]  \\ntransfer learning  \\ndomain adaptation  \\n97  \\n[  \\n]  \\n98  \\n[  \\n]  \\n99  \\n[  \\n]  \\nAn alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of   and   and Bahdanau et al. of the   in 2016.  The model named \"Listen, Attend and Spell\" (LAS), literally \"listens\" to the acoustic signal, pays \"attention\" to different parts of the signal and \"spells\" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model).  Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by  ,   and   to directly emit sub-word units which are more natural than English characters;    and   extended LAS to \"Watch, Listen, Attend and Spell\" (WLAS) to handle lip reading surpassing human-level performance.  \\nCarnegie Mellon University  \\nGoogle Brain  \\nUniversity of Montreal  \\n100  \\n[  \\n]  \\n101  \\n[  \\n]  \\n102  \\n[  \\n]  \\nCarnegie Mellon University  \\nMIT  \\nGoogle Brain  \\n103  \\n[  \\n]  \\nUniversity of Oxford  \\nGoogle DeepMind  \\n104  \\n[  \\n]'),\n",
       " Document(metadata={'Header 2': 'Applications'}, page_content='Applications'),\n",
       " Document(metadata={'Header 2': 'Applications'}, page_content='[  \\nedit  \\n]'),\n",
       " Document(metadata={'Header 2': 'Applications', 'Header 3': 'In-car systems'}, page_content='In-car systems'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nTypically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signaled to the driver by an audio prompt. Following the audio prompt, the system has a \"listening window\" during which it may accept a speech input for recognition.  \\n[ ]  \\ncitation needed  \\nSimple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent  car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.  \\n[ ]  \\nwhen?  \\n[ ]  \\ncitation needed'),\n",
       " Document(metadata={'Header 3': 'Education'}, page_content='Education'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nNewPP limit report\\nParsed by mw‐api‐int.codfw.main‐54c7cf74fd‐rzqjw\\nCached time: 20250315175016\\nCache expiry: 2592000\\nReduced expiry: false\\nComplications: [vary‐revision‐sha1, show‐toc]\\nCPU time usage: 1.433 seconds\\nReal time usage: 1.652 seconds\\nPreprocessor visited node count: 10259/1000000\\nPost‐expand include size: 394987/2097152 bytes\\nTemplate argument size: 6516/2097152 bytes\\nHighest expansion depth: 17/100\\nExpensive parser function count: 15/500\\nUnstrip recursion depth: 1/20\\nUnstrip post‐expand size: 552642/5000000 bytes\\nLua time usage: 0.918/10.000 seconds\\nLua memory usage: 6941909/52428800 bytes\\nNumber of Wikibase entities loaded: 0/400\\n \\n \\nTransclusion expansion time report (%,ms,calls,template)\\n100.00% 1314.653      1 -total\\n 67.95%  893.356      1 Template:Reflist\\n 19.29%  253.628     46 Template:Cite_web\\n 16.07%  211.292     35 Template:Cite_journal\\n 11.30%  148.593     24 Template:Cite_book\\n  6.78%   89.094      1 Template:Short_description\\n  6.61%   86.851      5 Template:Navbox\\n  5.60%   73.581     13 Template:Cite_arXiv\\n  5.28%   69.370      1 Template:Natural_Language_Processing\\n  4.83%   63.553      7 Template:Fix\\n \\n  Saved in parser cache with key enwiki:pcache:29468:|#|:idhash:canonical and timestamp 20250315175016 and revision id 1279817574. Rendering was triggered because: api-parse  \\nMain article:  \\nPronunciation assessment  \\nAutomatic   assessment is the use of speech recognition to verify the correctness of pronounced speech,  as distinguished from manual assessment by an instructor or proctor.  Also called speech verification, pronunciation evaluation, and pronunciation scoring, the main application of this technology is computer-aided pronunciation teaching (CAPT) when combined with   for   (CALL), speech  , or  . Pronunciation assessment does not determine unknown speech (as in   or  ) but instead, knowing the expected word(s) in advance, it attempts to verify the correctness of the learner\\'s pronunciation and ideally their   to listeners,  sometimes along with often inconsequential   such as  ,  ,  ,  , and  .  Pronunciation assessment is also used in  , for example in products such as   and from Amira Learning.  Automatic pronunciation assessment can also be used to help diagnose and treat   such as  .  \\npronunciation  \\n105  \\n[  \\n]  \\n106  \\n[  \\n]  \\ncomputer-aided instruction  \\ncomputer-assisted language learning  \\nremediation  \\naccent reduction  \\ndictation  \\nautomatic transcription  \\nintelligibility  \\n107  \\n[  \\n]  \\n108  \\n[  \\n]  \\nprosody  \\nintonation  \\npitch  \\ntempo  \\nrhythm  \\nstress  \\n109  \\n[  \\n]  \\nreading tutoring  \\nMicrosoft Teams  \\n110  \\n[  \\n]  \\n111  \\n[  \\n]  \\nspeech disorders  \\napraxia  \\n112  \\n[  \\n]  \\nAssessing authentic listener intelligibility is essential for avoiding inaccuracies from   bias, especially in high-stakes assessments;  from words with multiple correct pronunciations;  and from phoneme coding errors in machine-readable pronunciation dictionaries.  In 2022, researchers found that some newer speech to text systems, based on   to map audio signals directly into words, produce word and phrase confidence scores very closely correlated with genuine listener intelligibility.  In the   (CEFR) assessment criteria for \"overall phonological control\", intelligibility outweighs formally correct pronunciation at all levels.  \\naccent  \\n113  \\n[  \\n]  \\n114  \\n[  \\n]  \\n115  \\n[  \\n]  \\n116  \\n[  \\n]  \\n117  \\n[  \\n]  \\nend-to-end reinforcement learning  \\n118  \\n[  \\n]  \\nCommon European Framework of Reference for Languages  \\n119  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': 'Health care'}, page_content='Health care'),\n",
       " Document(metadata={'Header 3': 'Health care'}, page_content='[  \\nedit  \\n]'),\n",
       " Document(metadata={'Header 3': 'Health care', 'Header 4': 'Medical documentation'}, page_content='Medical documentation'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nIn the   sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a   system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.  \\nhealth care  \\ndigital dictation  \\nOne of the major issues relating to the use of speech recognition in healthcare is that the   ( ) provides for substantial financial benefits to physicians who utilize an EMR according to \"Meaningful Use\" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an   or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a  ) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.  \\nAmerican Recovery and Reinvestment Act of 2009  \\nARRA  \\nElectronic Health Record  \\ncontrolled vocabulary  \\nA more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician\\'s interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice \"macros\", where the use of certain phrases – e.g., \"normal report\", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam – e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.'),\n",
       " Document(metadata={'Header 4': 'Therapeutic use'}, page_content='Therapeutic use'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nProlonged use of speech recognition software in conjunction with   has shown benefits to short-term-memory restrengthening in   patients who have been treated with  . Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.  \\nword processors  \\nbrain AVM  \\nresection  \\n[ ]  \\ncitation needed'),\n",
       " Document(metadata={'Header 3': 'Military'}, page_content='Military'),\n",
       " Document(metadata={'Header 3': 'Military'}, page_content='[  \\nedit  \\n]'),\n",
       " Document(metadata={'Header 3': 'Military', 'Header 4': 'High-performance fighter aircraft'}, page_content='High-performance fighter aircraft'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nSubstantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in  . Of particular note have been the US program in speech recognition for the  /  aircraft ( ), the program in France for   aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.  \\nfighter aircraft  \\nAdvanced Fighter Technology Integration (AFTI)  \\nF-16  \\nF-16 VISTA  \\nMirage  \\nWorking with Swedish pilots flying in the   Gripen cockpit, Englund (2004) found recognition deteriorated with increasing  . The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.  \\nJAS-39  \\ng-loads  \\n120  \\n[  \\n]  \\nThe  , currently in service with the UK  , employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot  ,  and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.  \\nEurofighter Typhoon  \\nRAF  \\nworkload  \\n121  \\n[  \\n]  \\n122  \\n[  \\n]  \\nSpeaker-independent systems are also being developed and are under test for the   (JSF) and the   lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.  \\nF-35 Lightning II  \\nAlenia Aermacchi M-346 Master  \\n123  \\n[  \\n]'),\n",
       " Document(metadata={'Header 4': 'Helicopters'}, page_content='Helicopters'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nThe problems of achieving high recognition accuracy under stress and noise are particularly relevant in the   environment as well as in the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a  , which would reduce acoustic noise in the  . Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the   Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment ( ) in the UK. Work in France has included speech recognition in the  . There has also been much useful work in  . Results have been encouraging, and voice applications have included: control of communication radios, setting of   systems, and control of an automated target handover system.  \\nhelicopter  \\nfacemask  \\nmicrophone  \\nU.S. Army  \\nRAE  \\nPuma helicopter  \\nCanada  \\nnavigation  \\nAs in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall   in order to consistently achieve performance improvements in operational settings.  \\nspeech technology'),\n",
       " Document(metadata={'Header 4': 'Training air traffic controllers'}, page_content='Training air traffic controllers'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nTraining for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a \"pseudo-pilot\", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation. Speech recognition and   techniques offer the potential to eliminate the need for a person to act as a pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.  \\nsynthesis  \\nThe USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.  \\n[ ]  \\ncitation needed'),\n",
       " Document(metadata={'Header 3': 'Telephony and other domains'}, page_content='Telephony and other domains'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nASR is now commonplace in the field of   and is becoming more widespread in the field of   and simulation. In telephony systems, ASR is now being predominantly used in contact centers by integrating it with   systems. Despite the high level of integration with word processing in general personal computing, in the field of document production, ASR has not seen the expected increases in use.  \\ntelephony  \\ncomputer gaming  \\nIVR  \\nThe improvement of mobile processor speeds has made speech recognition practical in  . Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands.  \\nsmartphones'),\n",
       " Document(metadata={'Header 3': 'People with disabilities'}, page_content='People with disabilities'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nPeople with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.  \\n124  \\n[  \\n]  \\nStudents who are blind (see  ) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.  \\nBlindness and education  \\n125  \\n[  \\n]  \\nStudents who are physically disabled have a  /other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.  \\nRepetitive strain injury  \\n125  \\n[  \\n]  \\nSpeech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing.  Also, see  .  \\n126  \\n[  \\n]  \\nLearning disability  \\nThe use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term memory capacity, in stroke and craniotomy individuals.  \\nSpeech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed   became an urgent early market for speech recognition.  Speech recognition is used in    , such as voicemail to text,  , and  . Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof.  Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.  \\nRSI  \\n127  \\n[  \\n]  \\n128  \\n[  \\n]  \\ndeaf  \\ntelephony  \\nrelay services  \\ncaptioned telephone  \\n129  \\n[  \\n]  \\n130  \\n[  \\n]  \\nThis type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it from being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.  \\n131  \\n[  \\n]\"),\n",
       " Document(metadata={'Header 3': 'Further applications'}, page_content='Further applications'),\n",
       " Document(metadata={'Header 3': 'Further applications'}, page_content=\"[  \\nedit  \\n]  \\n(e.g.  ,  , etc.) NASA's   used speech recognition technology from   in the Mars Microphone on the Lander  \\nAerospace  \\nspace exploration  \\nspacecraft  \\nMars Polar Lander  \\nSensory, Inc.  \\n132  \\n[  \\n]  \\nAutomatic   with speech recognition  \\nsubtitling  \\nAutomatic  \\nemotion recognition  \\n133  \\n[  \\n]  \\nAutomatic   listing in audiovisual production  \\nshot  \\nAutomatic translation  \\n(Legal discovery)  \\neDiscovery  \\n: Speech recognition computer  \\nHands-free computing  \\nuser interface  \\nHome automation  \\nInteractive voice response  \\n, including mobile email  \\nMobile telephony  \\nMultimodal interaction  \\n64  \\n[  \\n]  \\nReal Time  \\nCaptioning  \\n134  \\n[  \\n]  \\nRobotics  \\nSecurity, including usage with other biometric scanners for  \\nmulti-factor authentication  \\n135  \\n[  \\n]  \\nSpeech to text (transcription of speech into text, real time video  , Court reporting )  \\ncaptioning  \\n(e.g. vehicle Navigation Systems)  \\nTelematics  \\n(digital speech-to-text)  \\nTranscription  \\n, with   and   as working examples  \\nVideo games  \\nTom Clancy's EndWar  \\nLifeline  \\n(e.g.  )  \\nVirtual assistant  \\nApple's Siri\"),\n",
       " Document(metadata={'Header 2': 'Performance'}, page_content='Performance'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nThe performance of speech recognition systems is usually evaluated in terms of accuracy and speed.  Accuracy is usually rated with   (WER), whereas speed is measured with the  . Other measures of accuracy include   (SWER) and   (CSR).  \\n136  \\n[  \\n]  \\n137  \\n[  \\n]  \\nword error rate  \\nreal time factor  \\nSingle Word Error Rate  \\nCommand Success Rate  \\nSpeech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:  \\n138  \\n[  \\n]  \\n[ ]  \\ncitation needed  \\nVocabulary size and confusability  \\nSpeaker dependence versus independence  \\nIsolated, discontinuous or continuous speech  \\nTask and language constraints  \\nRead versus spontaneous speech  \\nAdverse conditions'),\n",
       " Document(metadata={'Header 3': 'Accuracy'}, page_content='Accuracy'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nAs mentioned earlier in this article, the accuracy of speech recognition may vary depending on the following factors:  \\nError rates increase as the vocabulary size grows:  \\ne.g. the 10 digits \"zero\" to \"nine\" can be recognized essentially perfectly, but vocabulary sizes of 200, 5000 or 100000 may have error rates of 3%, 7%, or 45% respectively.  \\nVocabulary is hard to recognize if it contains confusing letters:  \\ne.g. the 26 letters of the English alphabet are difficult to discriminate because they are confusing words (most notoriously, the E-set: \"B, C, D, E, G, P, T, V, Z — when \"Z\" is pronounced \"zee\" rather than \"zed\" depending on the English region); an 8% error rate is considered good for this vocabulary.  \\n139  \\n[  \\n]  \\nSpeaker dependence vs. independence:  \\nA speaker-dependent system is intended for use by a single speaker.  \\nA speaker-independent system is intended for use by any speaker (more difficult).  \\nIsolated, Discontinuous or continuous speech  \\nWith isolated speech, single words are used, therefore it becomes easier to recognize the speech.  \\nWith discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech.  \\nWith continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.  \\nTask and language constraints  \\ne.g. Querying application may dismiss the hypothesis \"The apple is red.\"  \\ne.g. Constraints may be semantic; rejecting \"The apple is angry.\"  \\ne.g. Syntactic; rejecting \"Red is apple the.\"  \\nConstraints are often represented by grammar.  \\nRead vs. Spontaneous Speech – When a person reads it\\'s usually in a context that has been previously prepared, but when a person uses spontaneous speech, it is difficult to recognize the speech because of the disfluencies (like \"uh\" and \"um\", false starts, incomplete sentences, stuttering, coughing, and laughter) and limited vocabulary.  \\nAdverse conditions – Environmental noise (e.g. Noise in a car or a factory). Acoustical distortions (e.g. echoes, room acoustics)  \\nSpeech recognition is a multi-leveled pattern recognition task.  \\nAcoustical signals are structured into a hierarchy of units, e.g.  , Words, Phrases, and Sentences;  \\nPhonemes  \\nEach level provides additional constraints;  \\ne.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at a lower level;  \\nThis hierarchy of constraints is exploited. By combining decisions probabilistically at all lower levels, and making more deterministic decisions only at the highest level, speech recognition by a machine is a process broken into several phases. Computationally, it is a problem in which a sound pattern has to be recognized or classified into a category that represents a meaning to a human. Every acoustic signal can be broken into smaller more basic sub-signals. As the more complex sound signal is broken into the smaller sub-sounds, different levels are created, where at the top level we have complex sounds, which are made of simpler sounds on the lower level, and going to lower levels, even more, we create more basic and shorter and simpler sounds. At the lowest level, where the sounds are the most fundamental, a machine would check for simple and more probabilistic rules of what sound should represent. Once these sounds are put together into more complex sounds on upper level, a new set of more deterministic rules should predict what the new complex sound should represent. The most upper level of a deterministic rule should figure out the meaning of complex expressions. In order to expand our knowledge about speech recognition, we need to take into consideration neural networks. There are four steps of neural network approaches:  \\nDigitize the speech that we want to recognize  \\nFor telephone speech the sampling rate is 8000 samples per second;  \\nCompute features of spectral-domain of the speech (with Fourier transform);  \\ncomputed every 10\\xa0ms, with one 10\\xa0ms section called a frame;  \\nAnalysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has two descriptions:   (how strong is it), and   (how often it vibrates per second).\\nAccuracy can be computed with the help of word error rate (WER). Word error rate can be calculated by aligning the recognized word and referenced word using dynamic string alignment. The problem may occur while computing the word error rate due to the difference between the sequence lengths of the recognized word and referenced word.  \\namplitude  \\nfrequency  \\nThe formula to compute the word error rate (WER) is:  \\nW  \\nE  \\nR  \\n=  \\n(  \\ns  \\n+  \\nd  \\n+  \\ni  \\n)  \\nn  \\n{\\\\displaystyle WER={(s+d+i) \\\\over n}}  \\nwhere   is the number of substitutions,   is the number of deletions,   is the number of insertions, and   is the number of word references.  \\ns  \\nd  \\ni  \\nn  \\nWhile computing, the word recognition rate (WRR) is used. The formula is:  \\nW  \\nR  \\nR  \\n=  \\n1  \\n−  −  \\nW  \\nE  \\nR  \\n=  \\n(  \\nn  \\n−  −  \\ns  \\n−  −  \\nd  \\n−  −  \\ni  \\n)  \\nn  \\n=  \\nh  \\n−  −  \\ni  \\nn  \\n{\\\\displaystyle WRR=1-WER={(n-s-d-i) \\\\over n}={h-i \\\\over n}}  \\nwhere   is the number of correctly recognized words:  \\nh  \\nh  \\n=  \\nn  \\n−  −  \\n(  \\ns  \\n+  \\nd  \\n)  \\n.  \\n{\\\\displaystyle h=n-(s+d).}'),\n",
       " Document(metadata={'Header 3': 'Security concerns'}, page_content='Security concerns'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nSpeech recognition can become a means of attack, theft, or accidental operation. For example, activation words like \"Alexa\" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action.  Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.  \\n140  \\n[  \\n]  \\nTwo attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing.  The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.  \\n141  \\n[  \\n]  \\n142  \\n[  \\n]'),\n",
       " Document(metadata={'Header 2': 'Further information'}, page_content='Further information'),\n",
       " Document(metadata={'Header 2': 'Further information'}, page_content='[  \\nedit  \\n]'),\n",
       " Document(metadata={'Header 2': 'Further information', 'Header 3': 'Conferences and journals'}, page_content='Conferences and journals'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nPopular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe,  ,  /Eurospeech, and the IEEE ASRU. Conferences in the field of  , such as  ,  , EMNLP, and HLT, are beginning to include papers on  . Important journals include the   Transactions on Speech and Audio Processing (later renamed   Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed  /ACM Transactions on Audio, Speech and Language Processing—after merging with an ACM publication), Computer Speech and Language, and Speech Communication.  \\nICASSP  \\nInterspeech  \\nnatural language processing  \\nACL  \\nNAACL  \\nspeech processing  \\nIEEE  \\nIEEE  \\nIEEE'),\n",
       " Document(metadata={'Header 3': 'Books'}, page_content='Books'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nBooks like \"Fundamentals of Speech Recognition\" by   can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be \"Statistical Methods for Speech Recognition\" by   and \"Spoken Language Processing (2001)\" by   etc., \"Computer Speech\", by  , second edition published in 2004, and \"Speech Processing: A Dynamic and Optimization-Oriented Approach\" published in 2003 by Li Deng and Doug O\\'Shaughnessey. The updated textbook   (2008) by   and Martin presents the basics and the state of the art for ASR.   also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A comprehensive textbook, \"Fundamentals of Speaker Recognition\" is an in depth source for up to date details on the theory and practice.  A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by   (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).  \\nLawrence Rabiner  \\nFrederick Jelinek  \\nXuedong Huang  \\nManfred R. Schroeder  \\nSpeech and Language Processing  \\nJurafsky  \\nSpeaker recognition  \\n143  \\n[  \\n]  \\nDARPA  \\nA good and accessible introduction to speech recognition technology and its history is provided by the general audience book \"The Voice in the Machine. Building Computers That Understand Speech\" by   (2012).  \\nRoberto Pieraccini  \\nThe most recent book on speech recognition is   (Publisher: Springer) written by Microsoft researchers D. Yu and L. Deng and published near the end of 2014, with highly mathematically oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods.  A related book, published earlier in 2014, \"Deep Learning: Methods and Applications\" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009–2014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.  \\nAutomatic Speech Recognition: A Deep Learning Approach  \\n84  \\n[  \\n]  \\n80  \\n[  \\n]'),\n",
       " Document(metadata={'Header 3': 'Software'}, page_content='Software'),\n",
       " Document(metadata={}, page_content=\"[  \\nedit  \\n]  \\nIn terms of freely available resources,  's   toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the   book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques,   toolkit can be used.  In 2017   launched the open source project called   to gather big database of voices that would help build free speech recognition project DeepSpeech (available free at  ),  using Google's open source platform  .  When Mozilla redirected funding away from the project in 2020, it was forked by its original developers as Coqui STT  using the same open-source license.  \\nCarnegie Mellon University  \\nSphinx  \\nHTK  \\nKaldi  \\n144  \\n[  \\n]  \\nMozilla  \\nCommon Voice  \\n145  \\n[  \\n]  \\nGitHub  \\n146  \\n[  \\n]  \\nTensorFlow  \\n147  \\n[  \\n]  \\n148  \\n[  \\n]  \\n149  \\n[  \\n]  \\n150  \\n[  \\n]  \\nGoogle   supports speech recognition on all   applications. It can be activated through the    .  \\nGboard  \\nAndroid  \\nmicrophone  \\nicon  \\n151  \\n[  \\n]  \\nThe commercial cloud based speech recognition APIs are broadly available.  \\nFor more software resources, see  .  \\nList of speech recognition software\"),\n",
       " Document(metadata={'Header 2': 'See also'}, page_content='See also'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n.mw-parser-output .div-col{margin-top:0.3em;column-width:30em}.mw-parser-output .div-col-small{font-size:90%}.mw-parser-output .div-col-rules{column-rule:1px solid #aaa}.mw-parser-output .div-col dl,.mw-parser-output .div-col ol,.mw-parser-output .div-col ul{margin-top:0}.mw-parser-output .div-col li,.mw-parser-output .div-col dd{page-break-inside:avoid;break-inside:avoid-column}  \\nAI effect  \\nALPAC  \\nApplication Language Tags for speech recognition  \\nArticulatory speech recognition  \\nAudio mining  \\nAudio-visual speech recognition  \\nAutomatic Language Translator  \\nAutomotive head unit  \\nBraina  \\nCache language model  \\nDragon NaturallySpeaking  \\nFluency Voice Technology  \\nGoogle Voice Search  \\nIBM ViaVoice  \\nKeyword spotting  \\nKinect  \\nMondegreen  \\nMultimedia information retrieval  \\nOrigin of speech  \\nPhonetic search technology  \\nSpeaker diarisation  \\nSpeaker recognition  \\nSpeech analytics  \\nSpeech interface guideline  \\nSpeech recognition software for Linux  \\nSpeech synthesis  \\nSpeech verification  \\nSubtitle (captioning)  \\nVoiceXML  \\nVoxForge  \\nWindows Speech Recognition  \\nLists  \\nList of speech recognition software  \\nList of emerging technologies  \\nOutline of artificial intelligence  \\nTimeline of speech and voice recognition'),\n",
       " Document(metadata={'Header 2': 'References'}, page_content='References'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\n.mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}  \\n^  \\n.mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:\"\\\\\"\"\"\\\\\"\"\"\\'\"\"\\'\"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url(\"//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url(\"//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url(\"//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg\")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url(\"//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg\")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}  \\n. Fifthgen.com.   from the original on 11 November 2013 .  \\n\"Speaker Independent Connected Speech Recognition- Fifth Generation Computer Corporation\"  \\nArchived  \\n. Retrieved   2013  \\n15 June  \\n^  \\nP. Nguyen (2010). \"Automatic classification of speaker characteristics\".  . pp.\\xa0 152.  : .  \\xa0 .  \\xa0 .  \\nInternational Conference on Communications and Electronics 2010  \\n147–  \\ndoi  \\n10.1109/ICCE.2010.5670700  \\nISBN  \\n978-1-4244-7055-6  \\nS2CID  \\n13482115  \\n^  \\n. Macmillan Publishers Limited.   from the original on 16 September 2011 .  \\n\"British English definition of voice recognition\"  \\nArchived  \\n. Retrieved   2012  \\n21 February  \\n^  \\n. WebFinance, Inc.   from the original on 3 December 2011 .  \\n\"voice recognition, definition of\"  \\nArchived  \\n. Retrieved   2012  \\n21 February  \\n^  \\n. Linuxgazette.net.   from the original on 19 February 2013 .  \\n\"The Mailbag LG #114\"  \\nArchived  \\n. Retrieved   2013  \\n15 June  \\n^  \\nSarangi, Susanta; Sahidullah, Md; Saha, Goutam (September 2020). \"Optimization of data-driven filterbank for automatic speaker verification\".  .  : 102795.  : .  : .  : .  \\xa0 .  \\nDigital Signal Processing  \\n104  \\narXiv  \\n2007.10729  \\nBibcode  \\n2020DSP...10402795S  \\ndoi  \\n10.1016/j.dsp.2020.102795  \\nS2CID  \\n220665533  \\n^  \\nReynolds, Douglas; Rose, Richard (January 1995).    .  .   (1):  83.  : .  \\xa0 .  \\xa0 .  \\xa0 .     from the original on 8 March 2014 .  \\n\"Robust text-independent speaker identification using Gaussian mixture speaker models\"  \\n(PDF)  \\nIEEE Transactions on Speech and Audio Processing  \\n3  \\n72–  \\ndoi  \\n10.1109/89.365379  \\nISSN  \\n1063-6676  \\nOCLC  \\n26108901  \\nS2CID  \\n7319345  \\nArchived  \\n(PDF)  \\n. Retrieved   2014  \\n21 February  \\n^  \\n.  . Microsoft.   from the original on 25 February 2014 .  \\n\"Speaker Identification (WhisperID)\"  \\nMicrosoft Research  \\nArchived  \\n. Retrieved   2014  \\n21 February  \\nWhen you speak to someone, they don\\'t just recognize what you say: they recognize who you are. WhisperID will let computers do that, too, figuring out who you are by the way you sound.  \\n^  \\n.  . 22 July 2012.   from the original on 4 April 2019 .  \\n\"Obituaries: Stephen Balashek\"  \\nThe Star-Ledger  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\n. androidauthority.net.   from the original on 9 August 2018 .  \\n\"IBM-Shoebox-front.jpg\"  \\nArchived  \\n. Retrieved   2019  \\n4 April  \\n^  \\nJuang, B. H.; Rabiner, Lawrence R.    . p.\\xa06.     from the original on 17 August 2014 .  \\n\"Automatic speech recognition–a brief history of the technology development\"  \\n(PDF)  \\nArchived  \\n(PDF)  \\n. Retrieved   2015  \\n17 January  \\n^  \\na  \\nb  \\nMelanie Pinola (2 November 2011).  .  .   from the original on 3 November 2018 .  \\n\"Speech Recognition Through the Decades: How We Ended Up With Siri\"  \\nPC World  \\nArchived  \\n. Retrieved   2018  \\n22 October  \\n^  \\nGray, Robert M. (2010).    .  .   (4):  303.  : .  \\xa0 .     from the original on 9 October 2022 .  \\n\"A History of Realtime Digital Speech on Packet Networks: Part II of Linear Predictive Coding and the Internet Protocol\"  \\n(PDF)  \\nFound. Trends Signal Process  \\n3  \\n203–  \\ndoi  \\n10.1561/2000000036  \\nISSN  \\n1932-8346  \\nArchived  \\n(PDF)  \\n. Retrieved   2024  \\n9 September  \\n^  \\n(1969). \"Whither speech recognition?\".  .   (48):  1051.  : .  : .  \\nJohn R. Pierce  \\nJournal of the Acoustical Society of America  \\n46  \\n1049–  \\nBibcode  \\n1969ASAJ...46.1049P  \\ndoi  \\n10.1121/1.1911801  \\n^  \\nBenesty, Jacob; Sondhi, M. M.; Huang, Yiteng (2008).  . Springer Science & Business Media.  \\xa0 .  \\nSpringer Handbook of Speech Processing  \\nISBN  \\n978-3540491255  \\n^  \\nJohn Makhoul.  .   from the original on 24 January 2018 .  \\n\"ISCA Medalist: For leadership and extensive contributions to speech and language processing\"  \\nArchived  \\n. Retrieved   2018  \\n23 January  \\n^  \\nBlechman, R. O.; Blechman, Nicholas (23 June 2008).  .  .   from the original on 20 January 2015 .  \\n\"Hello, Hal\"  \\nThe New Yorker  \\nArchived  \\n. Retrieved   2015  \\n17 January  \\n^  \\nKlatt, Dennis H. (1977). \"Review of the ARPA speech understanding project\".  .   (6):  1366.  : .  : .  \\nThe Journal of the Acoustical Society of America  \\n62  \\n1345–  \\nBibcode  \\n1977ASAJ...62.1345K  \\ndoi  \\n10.1121/1.381666  \\n^  \\nRabiner (1984).    .     from the original on 9 August 2017 .  \\n\"The Acoustics, Speech, and Signal Processing Society. A Historical Perspective\"  \\n(PDF)  \\nArchived  \\n(PDF)  \\n. Retrieved   2018  \\n23 January  \\n^  \\n.  . 12 January 2015.   from the original on 3 April 2018 .  \\n\"First-Hand:The Hidden Markov Model – Engineering and Technology History Wiki\"  \\nethw.org  \\nArchived  \\n. Retrieved   2018  \\n1 May  \\n^  \\na  \\nb  \\n.   from the original on 28 August 2017 .  \\n\"James Baker interview\"  \\nArchived  \\n. Retrieved   2017  \\n9 February  \\n^  \\n. 7 March 2012. Archived from   on 19 February 2015 .  \\n\"Pioneering Speech Recognition\"  \\nthe original  \\n. Retrieved   2015  \\n18 January  \\n^  \\nHuang, Xuedong; Baker, James; Reddy, Raj (January 2014).  .  .   (1):  103.  : .  \\xa0 .  \\xa0 . Archived from   on 8 December 2023.  \\n\"A historical perspective of speech recognition\"  \\nCommunications of the ACM  \\n57  \\n94–  \\ndoi  \\n10.1145/2500887  \\nISSN  \\n0001-0782  \\nS2CID  \\n6175701  \\nthe original  \\n^  \\nJuang, B. H.; Rabiner, Lawrence R.     (Report). p.\\xa010.     from the original on 17 August 2014 .  \\nAutomatic speech recognition–a brief history of the technology development  \\n(PDF)  \\nArchived  \\n(PDF)  \\n. Retrieved   2015  \\n17 January  \\n^  \\nLi, Xiaochang (1 July 2023).  .  .  :  182.  : .  \\xa0 .  \\xa0 .  \\n\" \"There\\'s No Data Like More Data\": Automatic Speech Recognition and the Making of Algorithmic Culture\"  \\nOsiris  \\n38  \\n165–  \\ndoi  \\n10.1086/725132  \\nISSN  \\n0369-7827  \\nS2CID  \\n259502346  \\n^  \\n.  . Archived from   on 13 August 2015 .  \\n\"History of Speech Recognition\"  \\nDragon Medical Transcription  \\nthe original  \\n. Retrieved   2015  \\n17 January  \\n^  \\nBilli, Roberto; Canavesio, Franco; Ciaramella, Alberto; Nebbia, Luciano (1 November 1995).  .  .   (3):  271.  : .  \\n\"Interactive voice technology at work: The CSELT experience\"  \\nSpeech Communication  \\n17  \\n263–  \\ndoi  \\n10.1016/0167-6393(95)00030-R  \\n^  \\na  \\nb  \\nXuedong Huang; James Baker; Raj Reddy (January 2014).  . Communications of the ACM.   from the original on 20 January 2015 .  \\n\"A Historical Perspective of Speech Recognition\"  \\nArchived  \\n. Retrieved   2015  \\n20 January  \\n^  \\nKevin McKean (8 April 1980).  . Sarasota Journal. AP .  \\n\"When Cole talks, computers listen\"  \\n. Retrieved   2015  \\n23 November  \\n^  \\n.  .   from the original on 21 December 2016 .  \\n\"ACT/Apricot - Apricot history\"  \\nactapricot.org  \\nArchived  \\n. Retrieved   2016  \\n2 February  \\n^  \\nMelanie Pinola (2 November 2011).  .  .   from the original on 13 January 2017 .  \\n\"Speech Recognition Through the Decades: How We Ended Up With Siri\"  \\nPC World  \\nArchived  \\n. Retrieved   2017  \\n28 July  \\n^  \\n. KurzweilAINetwork.   from the original on 5 February 2014 .  \\n\"Ray Kurzweil biography\"  \\nArchived  \\n. Retrieved   2014  \\n25 September  \\n^  \\nJuang, B.H.; Rabiner, Lawrence.     (Report).     from the original on 9 August 2017 .  \\nAutomatic Speech Recognition – A Brief History of the Technology Development  \\n(PDF)  \\nArchived  \\n(PDF)  \\n. Retrieved   2017  \\n28 July  \\n^  \\n. Tech.pinions. 10 October 2011.   from the original on 19 November 2011 .  \\n\"Nuance Exec on iPhone 4S, Siri, and the Future of Speech\"  \\nArchived  \\n. Retrieved   2011  \\n23 November  \\n^  \\n.   from the original on 11 July 2017 .  \\n\"Switchboard-1 Release 2\"  \\nArchived  \\n. Retrieved   2017  \\n26 July  \\n^  \\nJason Kincaid (13 February 2011).  .  .   from the original on 21 July 2015 .  \\n\"The Power of Voice: A Conversation With The Head Of Google\\'s Speech Technology\"  \\nTech Crunch  \\nArchived  \\n. Retrieved   2015  \\n21 July  \\n^  \\nFroomkin, Dan (5 May 2015).  .  .   from the original on 27 June 2015 .  \\n\"THE COMPUTERS ARE LISTENING\"  \\nThe Intercept  \\nArchived  \\n. Retrieved   2015  \\n20 June  \\n^  \\nHerve Bourlard and  , Connectionist Speech Recognition: A Hybrid Approach, The Kluwer International Series in Engineering and Computer Science; v. 247, Boston: Kluwer Academic Publishers, 1994.  \\nNelson Morgan  \\n^  \\na  \\nb  \\n;   (1997). \"Long Short-Term Memory\".  .   (8):  1780.  : .  \\xa0 .  \\xa0 .  \\nSepp Hochreiter  \\nJ. Schmidhuber  \\nNeural Computation  \\n9  \\n1735–  \\ndoi  \\n10.1162/neco.1997.9.8.1735  \\nPMID  \\n9377276  \\nS2CID  \\n1915014  \\n^  \\n(2015). \"Deep learning in neural networks: An overview\".  .  :  117.  : .  : .  \\xa0 .  \\xa0 .  \\nSchmidhuber, Jürgen  \\nNeural Networks  \\n61  \\n85–  \\narXiv  \\n1404.7828  \\ndoi  \\n10.1016/j.neunet.2014.09.003  \\nPMID  \\n25462637  \\nS2CID  \\n11715509  \\n^  \\nAlex Graves, Santiago Fernandez, Faustino Gomez, and   (2006).     9 September 2024 at the  . Proceedings of ICML\\'06, pp. 369–376.  \\nJürgen Schmidhuber  \\nConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural nets  \\nArchived  \\nWayback Machine  \\n^  \\nSantiago Fernandez, Alex Graves, and Jürgen Schmidhuber (2007).  . Proceedings of ICANN (2), pp. 220–229.  \\nAn application of recurrent neural networks to discriminative keyword spotting  \\n[ ]  \\npermanent dead link  \\n\\u200d  \\n^  \\na  \\nb  \\nHaşim Sak, Andrew Senior, Kanishka Rao, Françoise Beaufays and Johan Schalkwyk (September 2015): \" .\"  \\n. Archived from   on 9 March 2016 .  \\n\"Google voice search: faster and more accurate\"  \\nthe original  \\n. Retrieved   2016  \\n5 April  \\n^  \\nDosovitskiy, Alexey; Beyer, Lucas; Kolesnikov, Alexander; Weissenborn, Dirk; Zhai, Xiaohua; Unterthiner, Thomas; Dehghani, Mostafa; Minderer, Matthias; Heigold, Georg; Gelly, Sylvain; Uszkoreit, Jakob; Houlsby, Neil (3 June 2021). \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\".  :  [ ].  \\narXiv  \\n2010.11929  \\ncs.CV  \\n^  \\nWu, Haiping; Xiao, Bin; Codella, Noel; Liu, Mengchen; Dai, Xiyang; Yuan, Lu; Zhang, Lei (29 March 2021). \"CvT: Introducing Convolutions to Vision Transformers\".  :  [ ].  \\narXiv  \\n2103.15808  \\ncs.CV  \\n^  \\nVaswani, Ashish; Shazeer, Noam; Parmar, Niki; Uszkoreit, Jakob; Jones, Llion; Gomez, Aidan N; Kaiser, Łukasz; Polosukhin, Illia (2017).  .  .  . Curran Associates.   from the original on 9 September 2024 .  \\n\"Attention is All you Need\"  \\nAdvances in Neural Information Processing Systems  \\n30  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\nDevlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina (24 May 2019). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\".  :  [ ].  \\narXiv  \\n1810.04805  \\ncs.CL  \\n^  \\na  \\nb  \\nGong, Yuan; Chung, Yu-An; Glass, James (8 July 2021). \"AST: Audio Spectrogram Transformer\".  :  [ ].  \\narXiv  \\n2104.01778  \\ncs.SD  \\n^  \\na  \\nb  \\nRistea, Nicolae-Catalin; Ionescu, Radu Tudor; Khan, Fahad Shahbaz (20 June 2022). \"SepTr: Separable Transformer for Audio Spectrogram Processing\".  :  [ ].  \\narXiv  \\n2203.09581  \\ncs.CV  \\n^  \\na  \\nb  \\nLohrenz, Timo; Li, Zhengyang; Fingscheidt, Tim (14 July 2021). \"Multi-Encoder Learning and Stream Fusion for Transformer-Based End-to-End Automatic Speech Recognition\".  :  [ ].  \\narXiv  \\n2104.00120  \\neess.AS  \\n^  \\n. Li Deng Site.   from the original on 9 September 2024 .  \\n\"Li Deng\"  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\nNIPS Workshop: Deep Learning for Speech Recognition and Related Applications, Whistler, BC, Canada, Dec. 2009 (Organizers: Li Deng, Geoff Hinton, D. Yu).  \\n^  \\na  \\nb  \\nc  \\nHinton, Geoffrey; Deng, Li; Yu, Dong; Dahl, George; Mohamed, Abdel-Rahman; Jaitly, Navdeep; Senior, Andrew; Vanhoucke, Vincent; Nguyen, Patrick;  ; Kingsbury, Brian (2012). \"Deep Neural Networks for Acoustic Modeling in Speech Recognition: The shared views of four research groups\".  .   (6):  97.  : .  : .  \\xa0 .  \\nSainath, Tara  \\nIEEE Signal Processing Magazine  \\n29  \\n82–  \\nBibcode  \\n2012ISPM...29...82H  \\ndoi  \\n10.1109/MSP.2012.2205597  \\nS2CID  \\n206485943  \\n^  \\na  \\nb  \\nDeng, L.; Hinton, G.; Kingsbury, B. (2013). \"New types of deep neural network learning for speech recognition and related applications: An overview\".  . p.\\xa08599.  : .  \\xa0 .  \\xa0 .  \\n2013 IEEE International Conference on Acoustics, Speech and Signal Processing: New types of deep neural network learning for speech recognition and related applications: An overview  \\ndoi  \\n10.1109/ICASSP.2013.6639344  \\nISBN  \\n978-1-4799-0356-6  \\nS2CID  \\n13953660  \\n^  \\na  \\nb  \\nMarkoff, John (23 November 2012).  .  .   from the original on 30 November 2012 .  \\n\"Scientists See Promise in Deep-Learning Programs\"  \\nNew York Times  \\nArchived  \\n. Retrieved   2015  \\n20 January  \\n^  \\nMorgan, Bourlard, Renals, Cohen, Franco (1993) \"Hybrid neural network/hidden Markov model systems for continuous speech recognition. ICASSP/IJPRAI\"  \\n^  \\n(1992).  .  . pp.\\xa0617–620 vol.1.  : .  \\xa0 .  \\xa0 .  \\nT. Robinson  \\n\"A real-time recurrent error propagation network word recognition system\"  \\n[Proceedings] ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing  \\ndoi  \\n10.1109/ICASSP.1992.225833  \\nISBN  \\n0-7803-0532-9  \\nS2CID  \\n62446313  \\n^  \\n, Hanazawa, Hinton, Shikano, Lang. (1989) \"    25 February 2021 at the  . IEEE Transactions on Acoustics, Speech, and Signal Processing.\"  \\nWaibel  \\nPhoneme recognition using time-delay neural networks  \\nArchived  \\nWayback Machine  \\n^  \\nBaker, J.; Li Deng; Glass, J.; Khudanpur, S.;  ; Morgan, N.; O\\'Shaughnessy, D. (2009). \"Developments and Directions in Speech Recognition and Understanding, Part 1\".  .   (3):  80.  : .  : .  : .  \\xa0 .  \\nChin-Hui Lee  \\nIEEE Signal Processing Magazine  \\n26  \\n75–  \\nBibcode  \\n2009ISPM...26...75B  \\ndoi  \\n10.1109/MSP.2009.932166  \\nhdl  \\n1721.1/51891  \\nS2CID  \\n357467  \\n^  \\n(1991),     6 March 2015 at the  , Diploma thesis. Institut f. Informatik, Technische Univ. Munich. Advisor: J. Schmidhuber.  \\nSepp Hochreiter  \\nUntersuchungen zu dynamischen neuronalen Netzen  \\nArchived  \\nWayback Machine  \\n^  \\nBengio, Y. (1991).   (Ph.D. thesis). McGill University.  \\nArtificial Neural Networks and their Application to Speech/Sequence Recognition  \\n^  \\nDeng, L.; Hassanein, K.; Elmasry, M. (1994). \"Analysis of the correlation structure for a neural predictive model with application to speech recognition\".  .   (2):  339.  : .  \\nNeural Networks  \\n7  \\n331–  \\ndoi  \\n10.1016/0893-6080(94)90027-2  \\n^  \\nKeynote talk: Recent Developments in Deep Neural Networks. ICASSP, 2013 (by Geoff Hinton).  \\n^  \\na  \\nb  \\nKeynote talk: \"    5 March 2021 at the  ,\" Interspeech, September 2014 (by  ).  \\nAchievements and Challenges of Deep Learning: From Speech Analysis and Recognition To Language and Multimodal Processing  \\nArchived  \\nWayback Machine  \\nLi Deng  \\n^  \\n.  . 27 August 2002. Archived from   on 23 October 2018 .  \\n\"Improvements in voice recognition software increase\"  \\nTechRepublic.com  \\nthe original  \\n. Retrieved   2018  \\n22 October  \\nManers said IBM has worked on advancing speech recognition ... or on the floor of a noisy trade show.  \\n^  \\n.  . 3 March 1997.   from the original on 9 September 2024 .  \\n\"Voice Recognition To Ease Travel Bookings: Business Travel News\"  \\nBusinessTravelNews.com  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\nThe earliest applications of speech recognition software were dictation ... Four months ago, IBM introduced a \\'continual dictation product\\' designed to ... debuted at the National Business Travel Association trade show in 1994.  \\n^  \\nEllis Booker (14 March 1994). \"Voice recognition enters the mainstream\".  . p.\\xa045.  \\nComputerworld  \\nJust a few years ago, speech recognition was limited to ...  \\n^  \\n.  . 21 August 2017.   from the original on 9 September 2024 .  \\n\"Microsoft researchers achieve new conversational speech recognition milestone\"  \\nMicrosoft  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\nGoel, Vaibhava; Byrne, William J. (2000).  .  .   (2):  135.  : .  \\xa0 .   from the original on 25 July 2011 .  \\n\"Minimum Bayes-risk automatic speech recognition\"  \\nComputer Speech & Language  \\n14  \\n115–  \\ndoi  \\n10.1006/csla.2000.0138  \\nS2CID  \\n206561058  \\nArchived  \\n. Retrieved   2011  \\n28 March  \\n^  \\nMohri, M. (2002).    .  .   (6):  982.  : .     from the original on 18 March 2012 .  \\n\"Edit-Distance of Weighted Automata: General Definitions and Algorithms\"  \\n(PDF)  \\nInternational Journal of Foundations of Computer Science  \\n14  \\n957–  \\ndoi  \\n10.1142/S0129054103002114  \\nArchived  \\n(PDF)  \\n. Retrieved   2011  \\n28 March  \\n^  \\nWaibel, A.; Hanazawa, T.; Hinton, G.; Shikano, K.; Lang, K. J. (1989). \"Phoneme recognition using time-delay neural networks\".  .   (3):  339.  : .  : .  \\xa0 .  \\nIEEE Transactions on Acoustics, Speech, and Signal Processing  \\n37  \\n328–  \\ndoi  \\n10.1109/29.21701  \\nhdl  \\n10338.dmlcz/135496  \\nS2CID  \\n9563026  \\n^  \\nBird, Jordan J.; Wanner, Elizabeth; Ekárt, Anikó; Faria, Diego R. (2020).    .  .  . Elsevier BV: 113402.  : .  \\xa0 .  \\xa0 .     from the original on 9 September 2024 .  \\n\"Optimisation of phonetic aware speech recognition through multi-objective evolutionary algorithms\"  \\n(PDF)  \\nExpert Systems with Applications  \\n153  \\ndoi  \\n10.1016/j.eswa.2020.113402  \\nISSN  \\n0957-4174  \\nS2CID  \\n216472225  \\nArchived  \\n(PDF)  \\n. Retrieved   2024  \\n9 September  \\n^  \\nWu, J.; Chan, C. (1993). \"Isolated Word Recognition by Neural Network Models with Cross-Correlation Coefficients for Speech Dynamics\".  .   (11):  1185.  : .  \\nIEEE Transactions on Pattern Analysis and Machine Intelligence  \\n15  \\n1174–  \\ndoi  \\n10.1109/34.244678  \\n^  \\nS. A. Zahorian, A. M. Zimmer, and F. Meng, (2002) \" ,\" in ICSLP 2002  \\nVowel Classification for Computer based Visual Feedback for Speech Training for the Hearing Impaired  \\n^  \\nHu, Hongbing; Zahorian, Stephen A. (2010).    .  .     from the original on 6 July 2012.  \\n\"Dimensionality Reduction Methods for HMM Phonetic Recognition\"  \\n(PDF)  \\nICASSP 2010  \\nArchived  \\n(PDF)  \\n^  \\nFernandez, Santiago; Graves, Alex;   (2007).    .  .     from the original on 15 August 2017.  \\nSchmidhuber, Jürgen  \\n\"Sequence labelling in structured domains with hierarchical recurrent neural networks\"  \\n(PDF)  \\nProceedings of IJCAI  \\nArchived  \\n(PDF)  \\n^  \\nICASSP 2013.  \\nGraves, Alex; Mohamed, Abdel-rahman; Hinton, Geoffrey (2013). \"Speech recognition with deep recurrent neural networks\".  :  [ ].  \\narXiv  \\n1303.5778  \\ncs.NE  \\n^  \\nWaibel, Alex (1989).    .  .   (1):  46.  : .  \\xa0 .     from the original on 29 June 2016.  \\n\"Modular Construction of Time-Delay Neural Networks for Speech Recognition\"  \\n(PDF)  \\nNeural Computation  \\n1  \\n39–  \\ndoi  \\n10.1162/neco.1989.1.1.39  \\nS2CID  \\n236321  \\nArchived  \\n(PDF)  \\n^  \\nMaas, Andrew L.; Le, Quoc V.; O\\'Neil, Tyler M.; Vinyals, Oriol; Nguyen, Patrick;   (2012). \"Recurrent Neural Networks for Noise Reduction in Robust ASR\".  .  \\nNg, Andrew Y.  \\nProceedings of Interspeech 2012  \\n^  \\na  \\nb  \\nDeng, Li; Yu, Dong (2014).    .  .   ( 4):  387.  \\xa0 .  : .     from the original on 22 October 2014.  \\n\"Deep Learning: Methods and Applications\"  \\n(PDF)  \\nFoundations and Trends in Signal Processing  \\n7  \\n3–  \\n197–  \\nCiteSeerX  \\n10.1.1.691.3679  \\ndoi  \\n10.1561/2000000039  \\nArchived  \\n(PDF)  \\n^  \\nYu, D.; Deng, L.; Dahl, G. (2010).    .  .  \\n\"Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition\"  \\n(PDF)  \\nNIPS Workshop on Deep Learning and Unsupervised Feature Learning  \\n^  \\nDahl, George E.; Yu, Dong; Deng, Li; Acero, Alex (2012). \"Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition\".  .   (1):  42.  : .  \\xa0 .  \\nIEEE Transactions on Audio, Speech, and Language Processing  \\n20  \\n30–  \\ndoi  \\n10.1109/TASL.2011.2134090  \\nS2CID  \\n14862572  \\n^  \\nDeng L., Li, J., Huang, J., Yao, K., Yu, D., Seide, F. et al.     9 September 2024 at the  . ICASSP, 2013.  \\nRecent Advances in Deep Learning for Speech Research at Microsoft  \\nArchived  \\nWayback Machine  \\n^  \\na  \\nb  \\nYu, D.; Deng, L. (2014). \"Automatic Speech Recognition: A Deep Learning Approach (Publisher: Springer)\".  \\n:  \\n{{ }}  \\ncite journal  \\nCite journal requires   ( )  \\n|journal=  \\nhelp  \\n^  \\nDeng, L.; Li, Xiao (2013).    .  .   (5):  1089.  : .  \\xa0 .     from the original on 9 September 2024 .  \\n\"Machine Learning Paradigms for Speech Recognition: An Overview\"  \\n(PDF)  \\nIEEE Transactions on Audio, Speech, and Language Processing  \\n21  \\n1060–  \\ndoi  \\n10.1109/TASL.2013.2244083  \\nS2CID  \\n16585863  \\nArchived  \\n(PDF)  \\n. Retrieved   2024  \\n9 September  \\n^  \\n(2015).  .  .   (11): 32832.  : .  : .  \\nSchmidhuber, Jürgen  \\n\"Deep Learning\"  \\nScholarpedia  \\n10  \\nBibcode  \\n2015SchpJ..1032832S  \\ndoi  \\n10.4249/scholarpedia.32832  \\n^  \\nL. Deng, M. Seltzer, D. Yu, A. Acero, A. Mohamed, and G. Hinton (2010)  . Interspeech.  \\nBinary Coding of Speech Spectrograms Using a Deep Auto-encoder  \\n^  \\nTüske, Zoltán; Golik, Pavel; Schlüter, Ralf; Ney, Hermann (2014).    .  .     from the original on 21 December 2016.  \\n\"Acoustic Modeling with Deep Neural Networks Using Raw Time Signal for LVCSR\"  \\n(PDF)  \\nInterspeech 2014  \\nArchived  \\n(PDF)  \\n^  \\nJurafsky, Daniel (2016).  .  \\nSpeech and Language Processing  \\n^  \\nGraves, Alex (2014).    .  . Archived from     on 10 January 2017 .  \\n\"Towards End-to-End Speech Recognition with Recurrent Neural Networks\"  \\n(PDF)  \\nICML  \\nthe original  \\n(PDF)  \\n. Retrieved   2019  \\n22 July  \\n^  \\nAmodei, Dario (2016). \"Deep Speech 2: End-to-End Speech Recognition in English and Mandarin\".  :  [ ].  \\narXiv  \\n1512.02595  \\ncs.CL  \\n^  \\n.  . 4 November 2016.   from the original on 27 April 2017 .  \\n\"LipNet: How easy do you think lipreading is?\"  \\nYouTube  \\nArchived  \\n. Retrieved   2017  \\n5 May  \\n^  \\nAssael, Yannis; Shillingford, Brendan; Whiteson, Shimon; de Freitas, Nando (5 November 2016). \"LipNet: End-to-End Sentence-level Lipreading\".  :  [ ].  \\narXiv  \\n1611.01599  \\ncs.CV  \\n^  \\nShillingford, Brendan; Assael, Yannis; Hoffman, Matthew W.; Paine, Thomas; Hughes, Cían; Prabhu, Utsav; Liao, Hank; Sak, Hasim; Rao, Kanishka (13 July 2018). \"Large-Scale Visual Speech Recognition\".  :  [ ].  \\narXiv  \\n1807.05162  \\ncs.CV  \\n^  \\nLi, Jason; Lavrukhin, Vitaly; Ginsburg, Boris; Leary, Ryan; Kuchaiev, Oleksii; Cohen, Jonathan M.; Nguyen, Huyen; Gadde, Ravi Teja (2019).  .  . pp.\\xa0 75.  : .  : .  \\n\"Jasper: An End-to-End Convolutional Neural Acoustic Model\"  \\nInterspeech 2019  \\n71–  \\narXiv  \\n1904.03288  \\ndoi  \\n10.21437/Interspeech.2019-1819  \\n^  \\nKriman, Samuel; Beliaev, Stanislav; Ginsburg, Boris; Huang, Jocelyn; Kuchaiev, Oleksii; Lavrukhin, Vitaly; Leary, Ryan; Li, Jason; Zhang, Yang (22 October 2019),  ,  :  \\nQuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions  \\narXiv  \\n1910.10261  \\n^  \\nMedeiros, Eduardo; Corado, Leonel; Rato, Luís; Quaresma, Paulo; Salgueiro, Pedro (May 2023).  .  .   (5): 159.  : .  \\xa0 .  \\n\"Domain Adaptation Speech-to-Text for Low-Resource European Portuguese Using Deep Learning\"  \\nFuture Internet  \\n15  \\ndoi  \\n10.3390/fi15050159  \\nISSN  \\n1999-5903  \\n^  \\nJoshi, Raviraj; Singh, Anupam (May 2022). Malmasi, Shervin; Rokhlenko, Oleg; Ueffing, Nicola; Guy, Ido; Agichtein, Eugene; Kallumadi, Surya (eds.).  .  . Dublin, Ireland: Association for Computational Linguistics:  249.  : .  : .  \\n\"A Simple Baseline for Domain Adaptation in End to End ASR Systems Using Synthetic Data\"  \\nProceedings of the Fifth Workshop on E-Commerce and NLP (ECNLP 5)  \\n244–  \\narXiv  \\n2206.13240  \\ndoi  \\n10.18653/v1/2022.ecnlp-1.28  \\n^  \\nSukhadia, Vrunda N.; Umesh, S. (9 January 2023).  .  . IEEE. pp.\\xa0 301.  : .  : .  \\xa0 .  \\n\"Domain Adaptation of Low-Resource Target-Domain Models Using Well-Trained ASR Conformer Models\"  \\n2022 IEEE Spoken Language Technology Workshop (SLT)  \\n295–  \\narXiv  \\n2202.09167  \\ndoi  \\n10.1109/SLT54892.2023.10023233  \\nISBN  \\n979-8-3503-9690-4  \\n^  \\nChan, William; Jaitly, Navdeep; Le, Quoc; Vinyals, Oriol (2016).    .  .     from the original on 9 September 2024 .  \\n\"Listen, Attend and Spell: A Neural Network for Large Vocabulary Conversational Speech Recognition\"  \\n(PDF)  \\nICASSP  \\nArchived  \\n(PDF)  \\n. Retrieved   2024  \\n9 September  \\n^  \\nBahdanau, Dzmitry (2016). \"End-to-End Attention-based Large Vocabulary Speech Recognition\".  :  [ ].  \\narXiv  \\n1508.04395  \\ncs.CL  \\n^  \\nChorowski, Jan; Jaitly, Navdeep (8 December 2016). \"Towards better decoding and language model integration in sequence to sequence models\".  :  [ ].  \\narXiv  \\n1612.02695  \\ncs.NE  \\n^  \\nChan, William; Zhang, Yu; Le, Quoc; Jaitly, Navdeep (10 October 2016). \"Latent Sequence Decompositions\".  :  [ ].  \\narXiv  \\n1610.03035  \\nstat.ML  \\n^  \\nChung, Joon Son; Senior, Andrew; Vinyals, Oriol; Zisserman, Andrew (16 November 2016). \"Lip Reading Sentences in the Wild\".  . pp.\\xa0 3453.  : .  : .  \\xa0 .  \\xa0 .  \\n2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)  \\n3444–  \\narXiv  \\n1611.05358  \\ndoi  \\n10.1109/CVPR.2017.367  \\nISBN  \\n978-1-5386-0457-1  \\nS2CID  \\n1662180  \\n^  \\nEl Kheir, Yassine; et\\xa0al. (21 October 2023),  , Conference on Empirical Methods in Natural Language Processing,  : ,  \\nAutomatic Pronunciation Assessment — A Review  \\narXiv  \\n2310.13974  \\nS2CID  \\n264426545  \\n^  \\nIsaacs, Talia; Harding, Luke (July 2017).  .  .   (3):  366.  : .  \\xa0 .  \\xa0 .  \\n\"Pronunciation assessment\"  \\nLanguage Teaching  \\n50  \\n347–  \\ndoi  \\n10.1017/S0261444817000118  \\nISSN  \\n0261-4448  \\nS2CID  \\n209353525  \\n^  \\nLoukina, Anastassia; et\\xa0al. (6 September 2015),    ,  , Dresden, Germany:  , pp.\\xa0 1921,     from the original on 9 September 2024 ,  \\n\"Pronunciation accuracy and intelligibility of non-native speech\"  \\n(PDF)  \\nINTERSPEECH 2015  \\nInternational Speech Communication Association  \\n1917–  \\narchived  \\n(PDF)  \\n, retrieved   2024  \\n9 September  \\nonly 16% of the variability in word-level intelligibility can be explained by the presence of obvious mispronunciations.  \\n^  \\nO’Brien, Mary Grantham; et\\xa0al. (31 December 2018).  .  .   (2):  207.  : .  : .  \\xa0 .  \\xa0 .  \\n\"Directions for the future of technology in pronunciation research and teaching\"  \\nJournal of Second Language Pronunciation  \\n4  \\n182–  \\ndoi  \\n10.1075/jslp.17001.obr  \\nhdl  \\n2066/199273  \\nISSN  \\n2215-1931  \\nS2CID  \\n86440885  \\npronunciation researchers are primarily interested in improving L2 learners\\' intelligibility and comprehensibility, but they have not yet collected sufficient amounts of representative and reliable data (speech recordings with corresponding annotations and judgments) indicating which errors affect these speech dimensions and which do not. These data are essential to train ASR algorithms to assess L2 learners\\' intelligibility.  \\n^  \\nEskenazi, Maxine (January 1999).  .  .   (2):  76.   from the original on 9 September 2024 .  \\n\"Using automatic speech processing for foreign language pronunciation tutoring: Some issues and a prototype\"  \\nLanguage Learning & Technology  \\n2  \\n62–  \\nArchived  \\n. Retrieved   2023  \\n11 February  \\n^  \\nTholfsen, Mike (9 February 2023).  .  . Microsoft.   from the original on 9 September 2024 .  \\n\"Reading Coach in Immersive Reader plus new features coming to Reading Progress in Microsoft Teams\"  \\nTechcommunity Education Blog  \\nArchived  \\n. Retrieved   2023  \\n12 February  \\n^  \\nBanerji, Olina (7 March 2023).  .  .   from the original on 9 September 2024 .  \\n\"Schools Are Using Voice Technology to Teach Reading. Is It Helping?\"  \\nEdSurge News  \\nArchived  \\n. Retrieved   2023  \\n7 March  \\n^  \\nHair, Adam; et\\xa0al. (19 June 2018). \"Apraxia world: A speech therapy game for children with speech sound disorders\".    . pp.\\xa0 131.  : .  \\xa0 .  \\xa0 .     from the original on 9 September 2024 .  \\nProceedings of the 17th ACM Conference on Interaction Design and Children  \\n(PDF)  \\n119–  \\ndoi  \\n10.1145/3202185.3202733  \\nISBN  \\n9781450351522  \\nS2CID  \\n13790002  \\nArchived  \\n(PDF)  \\n. Retrieved   2024  \\n9 September  \\n^  \\n.  . Australian Associated Press. 8 August 2017.   from the original on 9 September 2024 .  \\n\"Computer says no: Irish vet fails oral English test needed to stay in Australia\"  \\nThe Guardian  \\nArchived  \\n. Retrieved   2023  \\n12 February  \\n^  \\nFerrier, Tracey (9 August 2017).  .  .   from the original on 9 September 2024 .  \\n\"Australian ex-news reader with English degree fails robot\\'s English test\"  \\nThe Sydney Morning Herald  \\nArchived  \\n. Retrieved   2023  \\n12 February  \\n^  \\nMain, Ed; Watson, Richard (9 February 2022).  .  .   from the original on 9 September 2024 .  \\n\"The English test that ruined thousands of lives\"  \\nBBC News  \\nArchived  \\n. Retrieved   2023  \\n12 February  \\n^  \\nJoyce, Katy Spratte (24 January 2023).  . Reader\\'s Digest.   from the original on 9 September 2024 .  \\n\"13 Words That Can Be Pronounced Two Ways\"  \\nArchived  \\n. Retrieved   2023  \\n23 February  \\n^  \\nE.g.,  ,   Compare \"four\" given as \"F AO R\" with the vowel AO as in \"caught,\" to \"row\" given as \"R OW\" with the vowel OW as in \"oat.\"  \\nCMUDICT  \\n.  .   from the original on 15 August 2010 .  \\n\"The CMU Pronouncing Dictionary\"  \\nwww.speech.cs.cmu.edu  \\nArchived  \\n. Retrieved   2023  \\n15 February  \\n^  \\nTu, Zehai; Ma, Ning; Barker, Jon (2022).    .  . INTERSPEECH 2022. ISCA. pp.\\xa0 3497.  : .     from the original on 9 September 2024 .  \\n\"Unsupervised Uncertainty Measures of Automatic Speech Recognition for Non-intrusive Speech Intelligibility Prediction\"  \\n(PDF)  \\nProc. Interspeech 2022  \\n3493–  \\ndoi  \\n10.21437/Interspeech.2022-10408  \\nArchived  \\n(PDF)  \\n. Retrieved   2023  \\n17 December  \\n^  \\n. Language Policy Programme, Education Policy Division, Education Department,  . February 2018. p.\\xa0136.  \\xa0 .   from the original on 9 September 2024 .  \\nCommon European framework of reference for languages learning, teaching, assessment: Companion volume with new descriptors  \\nCouncil of Europe  \\nOCLC  \\n1090351600  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\nEnglund, Christine (2004).     (Masters thesis thesis).  .     from the original on 2 October 2008.  \\nSpeech recognition in the JAS\\xa039 Gripen aircraft: Adaptation to speech at different G-loads  \\n(PDF)  \\nStockholm Royal Institute of Technology  \\nArchived  \\n(PDF)  \\n^  \\n.  .   from the original on 1 March 2017.  \\n\"The Cockpit\"  \\nEurofighter Typhoon  \\nArchived  \\n^  \\n.  .   from the original on 11 May 2013 .  \\n\"Eurofighter Typhoon – The world\\'s most advanced fighter aircraft\"  \\nwww.eurofighter.com  \\nArchived  \\n. Retrieved   2018  \\n1 May  \\n^  \\nSchutte, John (15 October 2007).  . United States Air Force. Archived from   on 20 October 2007.  \\n\"Researchers fine-tune F-35 pilot-aircraft speech system\"  \\nthe original  \\n^  \\n. MassMATCH. 18 March 2010. Archived from the original on 25 July 2013 .  \\n\"Overcoming Communication Barriers in the Classroom\"  \\n. Retrieved   2013  \\n15 June  \\n^  \\na  \\nb  \\n. National Center for Technology Innovation. 2010.   from the original on 13 April 2014 .  \\n\"Speech Recognition for Learning\"  \\nArchived  \\n. Retrieved   2014  \\n26 March  \\n^  \\nFollensbee, Bob; McCloskey-Dale, Susan (2000).  .  .   from the original on 21 August 2006 .  \\n\"Speech recognition in schools: An update from the field\"  \\nTechnology And Persons With Disabilities Conference 2000  \\nArchived  \\n. Retrieved   2014  \\n26 March  \\n^  \\n. Archived from   on 4 April 2008.  \\n\"Speech recognition for disabled people\"  \\nthe original  \\n^  \\nFriends International Support Group  \\n^  \\nGarrett, Jennifer Tumlin; et\\xa0al. (2011).  .  .   (1):  41.  : .  \\xa0 .   from the original on 9 September 2024 .  \\n\"Using Speech Recognition Software to Increase Writing Fluency for Individuals with Physical Disabilities\"  \\nJournal of Special Education Technology  \\n26  \\n25–  \\ndoi  \\n10.1177/016264341102600104  \\nS2CID  \\n142730664  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\nForgrave, Karen E. \"Assistive Technology: Empowering Students with Disabilities.\" Clearing House 75.3 (2002): 122–6. Web.  \\n^  \\nTang, K. W.; Kamoua, Ridha; Sutan, Victor (2004). \"Speech Recognition Technology for Disabilities Education\".  .   (2):  84.  \\xa0 .  : .  \\xa0 .  \\nJournal of Educational Technology Systems  \\n33  \\n173–  \\nCiteSeerX  \\n10.1.1.631.3736  \\ndoi  \\n10.2190/K6K8-78K2-59Y7-R9R2  \\nS2CID  \\n143159997  \\n^  \\n. The Planetary Society. Archived from   on 27 January 2012.  \\n\"Projects: Planetary Microphones\"  \\nthe original  \\n^  \\nCaridakis, George; Castellano, Ginevra; Kessous, Loic; Raouzaiou, Amaryllis; Malatesta, Lori; Asteriadis, Stelios; Karpouzis, Kostas (19 September 2007). \"Multimodal emotion recognition from expressive faces, body gestures and speech\".  . IFIP the International Federation for Information Processing. Vol.\\xa0247. Springer US. pp.\\xa0 388.  : .  \\xa0 .  \\nArtificial Intelligence and Innovations 2007: From Theory to Applications  \\n375–  \\ndoi  \\n10.1007/978-0-387-74161-1_41  \\nISBN  \\n978-0-387-74160-4  \\n^  \\n.  .   from the original on 9 September 2024 .  \\n\"What is real-time captioning? | DO-IT\"  \\nwww.washington.edu  \\nArchived  \\n. Retrieved   2021  \\n11 April  \\n^  \\nZheng, Thomas Fang; Li, Lantian (2017).  . SpringerBriefs in Electrical and Computer Engineering. Singapore: Springer Singapore.  : .  \\xa0 .   from the original on 9 September 2024 .  \\nRobustness-Related Issues in Speaker Recognition  \\ndoi  \\n10.1007/978-981-10-3238-7  \\nISBN  \\n978-981-10-3237-0  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\nCiaramella, Alberto. \"A prototype performance evaluation report.\" Sundial workpackage 8000 (1993).  \\n^  \\nGerbino, E.; Baggia, P.; Ciaramella, A.; Rullent, C. (1993). \"Test and evaluation of a spoken dialogue system\".  . pp.\\xa0135–138 vol.2.  : .  \\xa0 .  \\xa0 .  \\nIEEE International Conference on Acoustics Speech and Signal Processing  \\ndoi  \\n10.1109/ICASSP.1993.319250  \\nISBN  \\n0-7803-0946-4  \\nS2CID  \\n57374050  \\n^  \\nNational Institute of Standards and Technology. \"    8 October 2013 at the  \".  \\nThe History of Automatic Speech Recognition Evaluation at NIST  \\nArchived  \\nWayback Machine  \\n^  \\n.  .   from the original on 9 September 2024 .  \\n\"Letter Names Can Cause Confusion and Other Things to Know About Letter–Sound Relationships\"  \\nNAEYC  \\nArchived  \\n. Retrieved   2023  \\n27 October  \\n^  \\n.  . 6 March 2016.   from the original on 23 July 2017.  \\n\"Listen Up: Your AI Assistant Goes Crazy For NPR Too\"  \\nNPR  \\nArchived  \\n^  \\nClaburn, Thomas (25 August 2017).  .  .   from the original on 2 September 2017.  \\n\"Is it possible to control Amazon Alexa, Google Now using inaudible commands? Absolutely\"  \\nThe Register  \\nArchived  \\n^  \\n.  . 31 January 2018.   from the original on 3 March 2018 .  \\n\"Attack Targets Automatic Speech Recognition Systems\"  \\nvice.com  \\nArchived  \\n. Retrieved   2018  \\n1 May  \\n^  \\nBeigi, Homayoon (2011).  . New York: Springer.  \\xa0 .   from the original on 31 January 2018.  \\nFundamentals of Speaker Recognition  \\nISBN  \\n978-0-387-77591-3  \\nArchived  \\n^  \\nPovey, D., Ghoshal, A., Boulianne, G., Burget, L., Glembek, O., Goel, N., ... & Vesely, K. (2011). The Kaldi speech recognition toolkit. In IEEE 2011 workshop on automatic speech recognition and understanding (No. CONF). IEEE Signal Processing Society.  \\n^  \\n.  . Archived from   on 27 February 2020 .  \\n\"Common Voice by Mozilla\"  \\nvoice.mozilla.org  \\nthe original  \\n. Retrieved   2019  \\n9 November  \\n^  \\n. 9 November 2019.   from the original on 9 September 2024  – via GitHub.  \\n\"A TensorFlow implementation of Baidu\\'s DeepSpeech architecture: mozilla/DeepSpeech\"  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\n. 9 November 2019.   from the original on 9 September 2024  – via GitHub.  \\n\"GitHub - tensorflow/docs: TensorFlow documentation\"  \\nArchived  \\n. Retrieved   2024  \\n9 September  \\n^  \\n.  .   from the original on 9 September 2024 .  \\n\"Coqui, a startup providing open speech tech for everyone\"  \\nGitHub  \\nArchived  \\n. Retrieved   2022  \\n7 March  \\n^  \\nCoffey, Donavyn (28 April 2021).  .  .  \\xa0 .   from the original on 9 September 2024 .  \\n\"Māori are trying to save their language from Big Tech\"  \\nWired UK  \\nISSN  \\n1357-0978  \\nArchived  \\n. Retrieved   2021  \\n16 October  \\n^  \\n.  . 7 July 2021 .  \\n\"Why you should move from DeepSpeech to coqui.ai\"  \\nMozilla Discourse  \\n. Retrieved   2021  \\n16 October  \\n^  \\n.   from the original on 9 September 2024 .  \\n\"Type with your voice\"  \\nArchived  \\n. Retrieved   2024  \\n9 September'),\n",
       " Document(metadata={'Header 2': 'Further reading'}, page_content='Further reading'),\n",
       " Document(metadata={}, page_content='[  \\nedit  \\n]  \\nCole, Ronald;  ; Uszkoreit, Hans; Varile, Giovanni Battista; Zaenen, Annie; Zampolli; Zue, Victor, eds. (1997).  . Cambridge Studies in Natural Language Processing. Vol.\\xa0 XIII. Cambridge University Press.  \\xa0 .  \\nMariani, Joseph  \\nSurvey of the state of the art in human language technology  \\nXII–  \\nISBN  \\n978-0-521-59277-2  \\nJunqua, J.-C.; Haton, J.-P. (1995).  . Kluwer Academic Publishers.  \\xa0 .  \\nRobustness in Automatic Speech Recognition: Fundamentals and Applications  \\nISBN  \\n978-0-7923-9646-8  \\nKarat, Clare-Marie; Vergo, John; Nahamoo, David (2007). \"Conversational Interface Technologies\". In  ; Jacko, Julie A. (eds.).  . Lawrence Erlbaum Associates Inc.  \\xa0 .  \\nSears, Andrew  \\nThe Human-Computer Interaction Handbook: Fundamentals, Evolving Technologies, and Emerging Applications (Human Factors and Ergonomics)  \\nISBN  \\n978-0-8058-5870-9  \\nPieraccini, Roberto (2012).  . The MIT Press.  \\xa0 .  \\nThe Voice in the Machine. Building Computers That Understand Speech  \\nISBN  \\n978-0262016858  \\nPirani, Giancarlo, ed. (2013).  . Springer Science & Business Media.  \\xa0 .  \\nAdvanced algorithms and architectures for speech understanding  \\nISBN  \\n978-3-642-84341-9  \\nSigner, Beat; Hoste, Lode (December 2013).  .  . 15th International Conference on Multimodal Interaction. Sydney, Australia.  \\n\"SpeeG2: A Speech- and Gesture-based Interface for Efficient Controller-free Text Entry\"  \\nProceedings of ICMI 2013  \\nWoelfel, Matthias; McDonough, John (26 May 2009).  . Wiley.  \\xa0 .  \\nDistant Speech Recognition  \\nISBN  \\n978-0470517048  \\n.mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:\": \"}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:\" · \";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:\" (\";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:\")\";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:\" \"counter(listitem)\"\\\\a0 \"}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:\" (\"counter(listitem)\"\\\\a0 \"}  \\n.mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}  \\n.mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:\"[ \"}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:\" ]\"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}  \\nv  \\nt  \\ne  \\nNatural language processing  \\nGeneral terms  \\nAI-complete  \\nBag-of-words  \\nn-gram  \\nBigram  \\nTrigram  \\nComputational linguistics  \\nNatural language understanding  \\nStop words  \\nText processing  \\nText analysis  \\nArgument mining  \\nCollocation extraction  \\nConcept mining  \\nCoreference resolution  \\nDeep linguistic processing  \\nDistant reading  \\nInformation extraction  \\nNamed-entity recognition  \\nOntology learning  \\nParsing  \\nSemantic parsing  \\nSyntactic parsing  \\nPart-of-speech tagging  \\nSemantic analysis  \\nSemantic role labeling  \\nSemantic decomposition  \\nSemantic similarity  \\nSentiment analysis  \\nTerminology extraction  \\nText mining  \\nTextual entailment  \\nTruecasing  \\nWord-sense disambiguation  \\nWord-sense induction  \\nText segmentation  \\nCompound-term processing  \\nLemmatisation  \\nLexical analysis  \\nText chunking  \\nStemming  \\nSentence segmentation  \\nWord segmentation  \\nAutomatic summarization  \\nMulti-document summarization  \\nSentence extraction  \\nText simplification  \\nMachine translation  \\nComputer-assisted  \\nExample-based  \\nRule-based  \\nStatistical  \\nTransfer-based  \\nNeural  \\nmodels  \\nDistributional semantics  \\nBERT  \\nDocument-term matrix  \\nExplicit semantic analysis  \\nfastText  \\nGloVe  \\n( )  \\nLanguage model  \\nlarge  \\nLatent semantic analysis  \\nSeq2seq  \\nWord embedding  \\nWord2vec  \\n, datasets and corpora  \\nLanguage resources  \\nTypes and standards  \\nCorpus linguistics  \\nLexical resource  \\nLinguistic Linked Open Data  \\nMachine-readable dictionary  \\nParallel text  \\nPropBank  \\nSemantic network  \\nSimple Knowledge Organization System  \\nSpeech corpus  \\nText corpus  \\nThesaurus (information retrieval)  \\nTreebank  \\nUniversal Dependencies  \\nData  \\nBabelNet  \\nBank of English  \\nDBpedia  \\nFrameNet  \\nGoogle Ngram Viewer  \\nUBY  \\nWordNet  \\nWikidata  \\nAutomatic identification and data capture  \\nSpeech recognition  \\nSpeech segmentation  \\nSpeech synthesis  \\nNatural language generation  \\nOptical character recognition  \\nTopic model  \\nDocument classification  \\nLatent Dirichlet allocation  \\nPachinko allocation  \\nComputer-assisted reviewing  \\nAutomated essay scoring  \\nConcordancer  \\nGrammar checker  \\nPredictive text  \\nPronunciation assessment  \\nSpell checker  \\nNatural language user interface  \\nChatbot  \\n(c.f.  )  \\nInteractive fiction  \\nSyntax guessing  \\nQuestion answering  \\nVirtual assistant  \\nVoice user interface  \\nRelated  \\nFormal semantics  \\nHallucination  \\nNatural Language Toolkit  \\nspaCy  \\nv  \\nt  \\ne  \\n(AI)  \\nArtificial intelligence  \\n( )  \\nHistory  \\ntimeline  \\nConcepts  \\nParameter  \\nHyperparameter  \\nLoss functions  \\nRegression  \\nBias–variance tradeoff  \\nDouble descent  \\nOverfitting  \\nClustering  \\nGradient descent  \\nSGD  \\nQuasi-Newton method  \\nConjugate gradient method  \\nBackpropagation  \\nAttention  \\nConvolution  \\nNormalization  \\nBatchnorm  \\nActivation  \\nSoftmax  \\nSigmoid  \\nRectifier  \\nGating  \\nWeight initialization  \\nRegularization  \\nDatasets  \\nAugmentation  \\nPrompt engineering  \\nReinforcement learning  \\nQ-learning  \\nSARSA  \\nImitation  \\nPolicy gradient  \\nDiffusion  \\nLatent diffusion model  \\nAutoregression  \\nAdversary  \\nRAG  \\nUncanny valley  \\nRLHF  \\nSelf-supervised learning  \\nRecursive self-improvement  \\nWord embedding  \\nHallucination  \\nApplications  \\nMachine learning  \\nIn-context learning  \\nArtificial neural network  \\nDeep learning  \\nLanguage model  \\nLarge language model  \\nNMT  \\nArtificial general intelligence  \\nImplementations  \\nAudio–visual  \\nAlexNet  \\nWaveNet  \\nHuman image synthesis  \\nHWR  \\nOCR  \\nSpeech synthesis  \\n15.ai  \\nElevenLabs  \\nSpeech recognition  \\nWhisper  \\nFacial recognition  \\nAlphaFold  \\nText-to-image models  \\nAurora  \\nDALL-E  \\nFirefly  \\nFlux  \\nIdeogram  \\nImagen  \\nMidjourney  \\nStable Diffusion  \\nText-to-video models  \\nDream Machine  \\nGen-3 Alpha  \\nHailuo AI  \\nKling  \\nSora  \\nVeo  \\nMusic generation  \\nSuno AI  \\nUdio  \\nText  \\nWord2vec  \\nSeq2seq  \\nGloVe  \\nBERT  \\nT5  \\nLlama  \\nChinchilla AI  \\nPaLM  \\nGPT  \\n1  \\n2  \\n3  \\nJ  \\nChatGPT  \\n4  \\n4o  \\n4.5  \\no1  \\no3  \\nClaude  \\nGemini  \\nchatbot  \\nGrok  \\nLaMDA  \\nBLOOM  \\nProject Debater  \\nIBM Watson  \\nIBM Watsonx  \\nGranite  \\nPanGu-Σ  \\nDeepSeek  \\nQwen  \\nDecisional  \\nAlphaGo  \\nAlphaZero  \\nOpenAI Five  \\nSelf-driving car  \\nMuZero  \\nAction selection  \\nAutoGPT  \\nRobot control  \\nPeople  \\nAlan Turing  \\nWarren Sturgis McCulloch  \\nWalter Pitts  \\nJohn von Neumann  \\nClaude Shannon  \\nMarvin Minsky  \\nJohn McCarthy  \\nNathaniel Rochester  \\nAllen Newell  \\nCliff Shaw  \\nHerbert A. Simon  \\nOliver Selfridge  \\nFrank Rosenblatt  \\nBernard Widrow  \\nJoseph Weizenbaum  \\nSeymour Papert  \\nSeppo Linnainmaa  \\nPaul Werbos  \\nJürgen Schmidhuber  \\nYann LeCun  \\nGeoffrey Hinton  \\nJohn Hopfield  \\nYoshua Bengio  \\nLotfi A. Zadeh  \\nStephen Grossberg  \\nAlex Graves  \\nAndrew Ng  \\nFei-Fei Li  \\nAlex Krizhevsky  \\nIlya Sutskever  \\nDemis Hassabis  \\nDavid Silver  \\nIan Goodfellow  \\nAndrej Karpathy  \\nArchitectures  \\nNeural Turing machine  \\nDifferentiable neural computer  \\nTransformer  \\nVision transformer (ViT)  \\nRecurrent neural network (RNN)  \\nLong short-term memory (LSTM)  \\nGated recurrent unit (GRU)  \\nEcho state network  \\nMultilayer perceptron (MLP)  \\nConvolutional neural network (CNN)  \\nResidual neural network (RNN)  \\nHighway network  \\nMamba  \\nAutoencoder  \\nVariational autoencoder (VAE)  \\nGenerative adversarial network (GAN)  \\nGraph neural network (GNN)  \\nPortals  \\nTechnology  \\nCategory  \\nArtificial neural networks  \\nMachine learning  \\nList  \\nCompanies  \\nProjects  \\nRetrieved from \" \"  \\nhttps://en.wikipedia.org/w/index.php?title=Speech_recognition&oldid=1279817574  \\n:  \\nCategories  \\nSpeech recognition  \\nAutomatic identification and data capture  \\nComputational linguistics  \\nUser interface techniques  \\nHistory of human–computer interaction  \\nComputer accessibility  \\nMachine learning task  \\nHidden categories:  \\nWebarchive template wayback links  \\nAll articles with dead external links  \\nArticles with dead external links from March 2023  \\nArticles with permanently dead external links  \\nCS1 errors: missing periodical  \\nCS1: unfit URL  \\nArticles with short description  \\nShort description matches Wikidata  \\nUse dmy dates from February 2017  \\nArticles containing potentially dated statements from 2017  \\nAll articles containing potentially dated statements  \\nAll articles with unsourced statements  \\nArticles with unsourced statements from March 2014  \\nAll articles with vague or ambiguous time  \\nVague or ambiguous time from April 2014  \\nArticles with unsourced statements from November 2016  \\nArticles with unsourced statements from December 2012  \\nArticles with unsourced statements from May 2013  \\nCS1: long volume value  \\nThis page was last edited on 10 March 2025, at 19:22 .  \\n(UTC)  \\nText is available under the  ;\\nadditional terms may apply. By using this site, you agree to the   and  . Wikipedia® is a registered trademark of the  , a non-profit organization.  \\nCreative Commons Attribution-ShareAlike 4.0 License  \\nTerms of Use  \\nPrivacy Policy  \\nWikimedia Foundation, Inc.  \\nPrivacy policy  \\nAbout Wikipedia  \\nDisclaimers  \\nContact Wikipedia  \\nCode of Conduct  \\nDevelopers  \\nStatistics  \\nCookie statement  \\nMobile view  \\nSearch  \\nSearch  \\nToggle the table of contents  \\nSpeech recognition  \\n49 languages  \\nAdd topic  \\n(RLQ=window.RLQ||[]).push(function(){mw.config.set({\"wgHostname\":\"mw-web.codfw.main-5886496d-6r6hx\",\"wgBackendResponseTime\":245,\"wgPageParseReport\":{\"limitreport\":{\"cputime\":\"1.433\",\"walltime\":\"1.652\",\"ppvisitednodes\":{\"value\":10259,\"limit\":1000000},\"postexpandincludesize\":{\"value\":394987,\"limit\":2097152},\"templateargumentsize\":{\"value\":6516,\"limit\":2097152},\"expansiondepth\":{\"value\":17,\"limit\":100},\"expensivefunctioncount\":{\"value\":15,\"limit\":500},\"unstrip-depth\":{\"value\":1,\"limit\":20},\"unstrip-size\":{\"value\":552642,\"limit\":5000000},\"entityaccesscount\":{\"value\":0,\"limit\":400},\"timingprofile\":[\"100.00% 1314.653      1 -total\",\" 67.95%  893.356      1 Template:Reflist\",\" 19.29%  253.628     46 Template:Cite_web\",\" 16.07%  211.292     35 Template:Cite_journal\",\" 11.30%  148.593     24 Template:Cite_book\",\"  6.78%   89.094      1 Template:Short_description\",\"  6.61%   86.851      5 Template:Navbox\",\"  5.60%   73.581     13 Template:Cite_arXiv\",\"  5.28%   69.370      1 Template:Natural_Language_Processing\",\"  4.83%   63.553      7 Template:Fix\"]},\"scribunto\":{\"limitreport-timeusage\":{\"value\":\"0.918\",\"limit\":\"10.000\"},\"limitreport-memusage\":{\"value\":6941909,\"limit\":52428800}},\"cachereport\":{\"origin\":\"mw-api-int.codfw.main-54c7cf74fd-rzqjw\",\"timestamp\":\"20250315175016\",\"ttl\":2592000,\"transientcontent\":false}}});});  \\n{\"@context\":\"https:\\\\/\\\\/schema.org\",\"@type\":\"Article\",\"name\":\"Speech recognition\",\"url\":\"https:\\\\/\\\\/en.wikipedia.org\\\\/wiki\\\\/Speech_recognition\",\"sameAs\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q189436\",\"mainEntity\":\"http:\\\\/\\\\/www.wikidata.org\\\\/entity\\\\/Q189436\",\"author\":{\"@type\":\"Organization\",\"name\":\"Contributors to Wikimedia projects\"},\"publisher\":{\"@type\":\"Organization\",\"name\":\"Wikimedia Foundation, Inc.\",\"logo\":{\"@type\":\"ImageObject\",\"url\":\"https:\\\\/\\\\/www.wikimedia.org\\\\/static\\\\/images\\\\/wmf-hor-googpub.png\"}},\"datePublished\":\"2002-01-08T05:06:41Z\",\"dateModified\":\"2025-03-10T19:22:33Z\",\"headline\":\"automatic conversion of spoken language into text\"}')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/Speech_recognition\"\n",
    "\n",
    "headers_to_split_on = [('h1','Header 1'),\n",
    "                       ('h2','Header 2'),\n",
    "                       ('h3','Header 3'),\n",
    "                       ('h4','Header 4')]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on)\n",
    "html_split = html_splitter.split_text_from_url(url)\n",
    "html_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
