{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x20475d16cb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Text Loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content=\"Speech is the use of the human voice as a medium for language. \\nSpoken language combines vowel and consonant sounds to form units of meaning like words, which belong to a language's lexicon. \\nThere are many different intentional speech acts, such as informing, declaring, asking, persuading, directing; acts may vary in various \\naspects like enunciation, intonation, loudness, and tempo to convey meaning. \\nIndividuals may also unintentionally communicate aspects of their social position through speech, such as sex, age, place of origin,\\n physiological and mental condition, education, and experiences.\\n\\nWhile normally used to facilitate communication with others, people may also use speech without the intent to communicate. \\nSpeech may nevertheless express emotions or desires; people talk to themselves sometimes in acts that are a development of what\\nsome psychologists (e.g., Lev Vygotsky) have maintained is the use of silent speech in an interior monologue to vivify and \\norganize cognition, sometimes in the momentary adoption of a dual persona as self addressing self as though addressing another person. \\nSolo speech can be used to memorize or to test one's memorization of things, and in prayer or in meditation.\")]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 0, 'page_label': '1'}, page_content='arXiv:2501.09223v1  [cs.CL]  16 Jan 2025\\nFoundations of\\nLarge Language Models\\nT ong Xiao and Jingbo Zhu\\nJanuary 17, 2025\\nNLP Lab, Northeastern University & NiuTrans Research'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 1, 'page_label': 'i'}, page_content='Copyright © 2021-2025 T ong Xiao and Jingbo Zhu\\nNATU R A L LA N G UAG E PRO C ESSIN G LA B, N O RTH EA STER N UN IV ER SITY\\n&\\nNIU TR A N S RESEA R C H\\nLicensed under the Creative Commons Attribution-NonComme rcial 4.0 Unported License (the\\n“License”). Y ou may not use this ﬁle except in compliance wit h the License. Y ou may ob-\\ntain a copy of the License at\\nhttp://creativecommons.org/licenses/by-nc/4.0. Unless\\nrequired by applicable law or agreed to in writing, software distributed under the License is dis-\\ntributed on an “ A S IS ” BA SIS , W ITH O U T W A R R A N TIES O R C O N D ITIO N S O F A N Y K IN D , either\\nexpress or implied. See the License for the speciﬁc language governing permissions and limita-\\ntions under the License.\\nJanuary 17, 2025'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 2, 'page_label': 'ii'}, page_content='Preface\\nLarge language models originated from natural language pro cessing, but they have undoubtedly\\nbecome one of the most revolutionary technological advance ments in the ﬁeld of artiﬁcial intelli-\\ngence in recent years. An important insight brought by large language models is that knowledge\\nof the world and languages can be acquired through large-sca le language modeling tasks, and\\nin this way , we can create a universal model that handles dive rse problems. This discovery has\\nprofoundly impacted the research methodologies in natural language processing and many related\\ndisciplines. W e have shifted from training specialized sys tems from scratch using a large amount\\nof labeled data to a new paradigm of using large-scale pre-tr aining to obtain foundation models,\\nwhich are then ﬁne-tuned, aligned, and prompted.\\nThis book aims to outline the basic concepts of large languag e models and introduce the\\nrelated techniques. As the title suggests, the book focuses more on the foundational aspects of\\nlarge language models rather than providing comprehensive coverage of all cutting-edge methods.\\nThe book consists of four chapters:\\n• Chapter 1 introduces the basics of pre-training. This is th e foundation of large language\\nmodels, and common pre-training methods and model architec tures will be discussed here.\\n• Chapter 2 introduces generative models, which are the larg e language models we commonly\\nrefer to today . After presenting the basic process of buildi ng these models, we will also\\nexplore how to scale up model training and handle long texts.\\n• Chapter 3 introduces prompting methods for large language models. W e will discuss var-\\nious prompting strategies, along with more advanced method s such as chain-of-thought\\nreasoning and automatic prompt design.\\n• Chapter 4 introduces alignment methods for large language models. This chapter focuses\\non instruction ﬁne-tuning and alignment based on human feed back.\\nIf readers have some background in machine learning and natu ral language processing, along\\nwith a certain understanding of neural networks like Transf ormers, reading this book will be quite\\neasy . However, even without this prior knowledge, it is stil l perfectly ﬁne, as we have made the\\ncontent of each chapter as self-contained as possible, ensu ring that readers will not be burdened\\nwith too much reading difﬁculty .\\nIn writing this book, we have gradually realized that it is mo re like a compilation of \"notes\" we\\nhave taken while learning about large language models. Thro ugh this note-taking writing style, we\\nhope to offer readers a ﬂexible learning path. Whether they w ish to dive deep into a speciﬁc area\\nor gain a comprehensive understanding of large language mod els, they will ﬁnd the knowledge\\nand insights they need within these \"notes\".\\nW e would like to thank the students in our laboratory and all o ur friends who have shared\\nwith us their views on large language models and helped with c orrections of errors in writing. In\\nparticular, we wish to thank W eiqiao Shan, Y ongyu Mu, Chengl ong W ang, Kaiyan Chang, Y uchun\\nFan, Hang Zhou, Xinyu Liu, Huiwen Bao, T ong Zheng, Junhao Rua n, and Qing Y ang.\\nii'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 3, 'page_label': 'iii'}, page_content='Notation\\na variable\\na row vector or matrix\\nf(a) function of a\\nmax f(a) maximum value of f(a)\\narg max a f(a) value of athat maximizes f(a)\\nx input token sequence to a model\\nxj input token at position j\\ny output token sequence produced by a model\\nyi output token at position i\\nθ model parameters\\nPr(a) probability of a\\nPr(a|b) conditional probability of agiven b\\nPr(·|b) probability distribution of a variable given b\\nPrθ (a) probability of aas parameterized by θ\\nht hidden state at time step tin sequential models\\nH matrix of all hidden states over time in a sequence\\nQ, K, V query , key , and value matrices in attention mechanisms\\nSoftmax(A) Softmax function that normalizes the input vector or matrix A\\nL loss function\\nD dataset used for training or ﬁne-tuning a model\\n∂ L\\n∂θ gradient of the loss function L with respect to the parameters θ\\nKL(p|| q) KL divergence between distributions pand q\\niii'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 4, 'page_label': 'iv'}, page_content='Contents\\n1 Pre-training 1\\n1.1 Pre-training NLP Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-t raining . . . . . . . . 2\\n1.1.2 Adapting Pre-trained Models . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Self-supervised Pre-training T asks . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.2.1 Decoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n1.2.2 Encoder-only Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . 8\\n1.2.3 Encoder-Decoder Pre-training . . . . . . . . . . . . . . . . . . . . . . . 15\\n1.2.4 Comparison of Pre-training T asks . . . . . . . . . . . . . . . . . . . . . 20\\n1.3 Example: BERT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n1.3.1 The Standard Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n1.3.2 More Training and Larger Models . . . . . . . . . . . . . . . . . . . . . 27\\n1.3.3 More Efﬁcient Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\\n1.3.4 Multi-lingual Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n1.4 Applying BERT Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n1.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n2 Generative Models 36\\n2.1 A Brief Introduction to LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\\n2.1.1 Decoder-only Transformers . . . . . . . . . . . . . . . . . . . . . . . . 38\\n2.1.2 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\\n2.1.3 Fine-tuning LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\\n2.1.4 Aligning LLMs with the W orld . . . . . . . . . . . . . . . . . . . . . . 46\\n2.1.5 Prompting LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\n2.2 Training at Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n2.2.1 Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\\n2.2.2 Model Modiﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\n2.2.3 Distributed Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\\n2.2.4 Scaling Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\\n2.3 Long Sequence Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\\n2.3.1 Optimization from HPC Perspectives . . . . . . . . . . . . . . . . . . . 67\\n2.3.2 Efﬁcient Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . 68\\n2.3.3 Cache and Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\\n2.3.4 Sharing across Heads and Layers . . . . . . . . . . . . . . . . . . . . . 80\\niv'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 5, 'page_label': 'v'}, page_content='v\\n2.3.5 Position Extrapolation and Interpolation . . . . . . . . . . . . . . . . . . 82\\n2.3.6 Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\\n2.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\\n3 Prompting 96\\n3.1 General Prompt Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\\n3.1.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\\n3.1.2 In-context Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\\n3.1.3 Prompt Engineering Strategies . . . . . . . . . . . . . . . . . . . . . . . 101\\n3.1.4 More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\\n3.2 Advanced Prompting Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n3.2.1 Chain of Thought . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\\n3.2.2 Problem Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . 117\\n3.2.3 Self-reﬁnement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124\\n3.2.4 Ensembling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130\\n3.2.5 RAG and T ool Use . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134\\n3.3 Learning to Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\\n3.3.1 Prompt Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\\n3.3.2 Soft Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\\n3.3.3 Prompt Length Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 152\\n3.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\\n4 Alignment 155\\n4.1 An Overview of LLM Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 155\\n4.2 Instruction Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n4.2.1 Supervised Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . 157\\n4.2.2 Fine-tuning Data Acquisition . . . . . . . . . . . . . . . . . . . . . . . . 161\\n4.2.3 Fine-tuning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . . 166\\n4.2.4 Instruction Generalization . . . . . . . . . . . . . . . . . . . . . . . . . 167\\n4.2.5 Using W eak Models to Improve Strong Models . . . . . . . . . . . . . . 169\\n4.3 Human Preference Alignment: RLHF . . . . . . . . . . . . . . . . . . . . . . . 172\\n4.3.1 Basics of Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 173\\n4.3.2 Training Reward Models . . . . . . . . . . . . . . . . . . . . . . . . . . 179\\n4.3.3 Training LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\\n4.4 Improved Human Preference Alignment . . . . . . . . . . . . . . . . . . . . . . 187\\n4.4.1 Better Reward Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 187'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 6, 'page_label': 'vi'}, page_content='vi Notation\\n4.4.2 Direct Preference Optimization . . . . . . . . . . . . . . . . . . . . . . 193\\n4.4.3 Automatic Preference Data Generation . . . . . . . . . . . . . . . . . . 196\\n4.4.4 Step-by-step Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . 198\\n4.4.5 Inference-time Alignment . . . . . . . . . . . . . . . . . . . . . . . . . 200\\n4.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\\nBibliography 203'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 7, 'page_label': '1'}, page_content='CH A P TER 1\\nPre-training\\nThe development of neural sequence models, such as T ransformers [V aswani et al. , 2017], along\\nwith the improvements in large-scale self-supervised lear ning, has opened the door to universal\\nlanguage understanding and generation. This achievement i s largely motivated by pre-training:\\nwe separate common components from many neural network-bas ed systems, and then train them\\non huge amounts of unlabeled data using self-supervision. T hese pre-trained models serve as\\nfoundation models that can be easily adapted to different ta sks via ﬁne-tuning or prompting. As a\\nresult, the paradigm of NLP has been enormously changed. In m any cases, large-scale supervised\\nlearning for speciﬁc tasks is no longer required, and instea d, we only need to adapt pre-trained\\nfoundation models.\\nWhile pre-training has gained popularity in recent NLP rese arch, this concept dates back\\ndecades to the early days of deep learning. For example, earl y attempts to pre-train deep learning\\nsystems include unsupervised learning for RNNs, deep feedf orward networks, autoencoders, and\\nothers [ Schmidhuber, 2015]. In the modern era of deep learning, we experienced a resurg ence of\\npre-training, caused in part by the large-scale unsupervis ed learning of various word embedding\\nmodels [ Mikolov et al. , 2013b; Pennington et al. , 2014]. During the same period, pre-training also\\nattracted signiﬁcant interest in computer vision, where th e backbone models were trained on rel-\\natively large labeled datasets such as ImageNet, and then ap plied to different downstream tasks\\n[He et al. , 2019; Zoph et al. , 2020]. Large-scale research on pre-training in NLP began with th e\\ndevelopment of language models using self-supervised lear ning. This family of models covers\\nseveral well-known examples like BERT [Devlin et al. , 2019] and GPT [Brown et al. , 2020], all\\nwith a similar idea that general language understanding and generation can be achieved by train-\\ning the models to predict masked words in a huge amount of text . Despite the simple nature of\\nthis approach, the resulting models show remarkable capabi lity in modeling linguistic structure,\\nthough they are not explicitly trained to achieve this. The g enerality of the pre-training tasks\\nleads to systems that exhibit strong performance in a large v ariety of NLP problems, even outper-\\nforming previously well-developed supervised systems. Mo re recently , pre-trained large language\\nmodels have achieved a greater success, showing the excitin g prospects for more general artiﬁcial\\nintelligence [ Bubeck et al. , 2023].\\nThis chapter discusses the concept of pre-training in the co ntext of NLP . It begins with a gen-\\neral introduction to pre-training methods and their applic ations. BERT is then used as an example\\nto illustrate how a sequence model is trained via a self-supe rvised task, called masked language\\nmodeling. This is followed by a discussion of methods for adapting pre -trained sequence mod-\\nels for various NLP tasks. Note that in this chapter, we will f ocus primarily on the pre-training\\nparadigm in NLP , and therefore, we do not intend to cover deta ils about generative large language\\nmodels. A detailed discussion of these models will be left to subsequent chapters.\\n1.1 Pre-training NLP Models\\nThe discussion of pre-training issues in NLP typically involves two types of problems: sequence\\nmodeling (or sequence encoding) and sequence generation. W hile these problems have different\\n1'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 8, 'page_label': '2'}, page_content='2 Pre-training\\nforms, for simplicity , we describe them using a single model deﬁned as follows:\\no = g(x0 ,x1 ,...,xm ; θ)\\n= gθ (x0 ,x1 ,...,xm ) (1.1)\\nwhere {x0 ,x1,...,xm } denotes a sequence of input tokens 1 , x0 denotes a special symbol ( ⟨ s⟩ or\\n[CLS]) attached to the beginning of a sequence, g(·; θ) (also written as gθ (·)) denotes a neural\\nnetwork with parameters θ, and o denotes the output of the neural network. Different problem s\\ncan vary based on the form of the output o. For example, in token prediction problems (as in\\nlanguage modeling), o is a distribution over a vocabulary; in sequence encoding pr oblems, o is a\\nrepresentation of the input sequence, often expressed as a r eal-valued vector sequence.\\nThere are two fundamental issues here.\\n• Optimizing θon a pre-training task. Unlike standard learning problems i n NLP , pre-training\\ndoes not assume speciﬁc downstream tasks to which the model w ill be applied. Instead, the\\ngoal is to train a model that can generalize across various ta sks.\\n• Applying the pre-trained model gˆθ (·) to downstream tasks. T o adapt the model to these\\ntasks, we need to adjust the parameters ˆθ slightly using labeled data or prompt the model\\nwith task descriptions.\\nIn this section, we discuss the basic ideas in addressing the se issues.\\n1.1.1 Unsupervised, Supervised and Self-supervised Pre-t raining\\nIn deep learning, pre-training refers to the process of opti mizing a neural network before it is\\nfurther trained/tuned and applied to the tasks of interest. This approach is based on an assumption\\nthat a model pre-trained on one task can be adapted to perform another task. As a result, we do\\nnot need to train a deep, complex neural network from scratch on tasks with limited labeled data.\\nInstead, we can make use of tasks where supervision signals a re easier to obtain. This reduces the\\nreliance on task-speciﬁc labeled data, enabling the develo pment of more general models that are\\nnot conﬁned to particular problems.\\nDuring the resurgence of neural networks through deep learn ing, many early attempts to\\nachieve pre-training were focused on unsupervised learning . In these methods, the parame-\\nters of a neural network are optimized using a criterion that is not directly related to speciﬁc tasks.\\nFor example, we can minimize the reconstruction cross-entr opy of the input vector for each layer\\n[Bengio et al. , 2006]. Unsupervised pre-training is commonly employed as a prel iminary step\\nbefore supervised learning, offering several advantages, such as aiding in the discovery of better\\nlocal minima and adding a regularization effect to the train ing process [ Erhan et al. , 2010]. These\\nbeneﬁts make the subsequent supervised learning phase easi er and more stable.\\nA second approach to pre-training is to pre-train a neural ne twork on supervised learning\\ntasks. For example, consider a sequence model designed to en code input sequences into some\\n1 Here we assume that tokens are basic units of text that are sep arated through tokenization. Sometimes, we will use\\nthe terms token and word interchangeably , though they have closely related but slig htly different meanings in NLP .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 9, 'page_label': '3'}, page_content='1.1 Pre-training NLP Models 3\\nrepresentations. In pre-training, this model is combined w ith a classiﬁcation layer to form a clas-\\nsiﬁcation system. This system is then trained on a pre-train ing task, such as classifying sentences\\nbased on sentiment (e.g., determining if a sentence conveys a positive or negative sentiment).\\nThen, we adapt the sequence model to a downstream task. W e bui ld a new classiﬁcation system\\nbased on this pre-trained sequence model and a new classiﬁca tion layer (e.g., determining if a\\nsequence is subjective or objective). T ypically , we need to ﬁne-tune the parameters of the new\\nmodel using task-speciﬁc labeled data, ensuring the model i s optimally adjusted to perform well\\non this new type of data. The ﬁne-tuned model is then employed to classify new sequences for\\nthis task. An advantage of supervised pre-training is that t he training process, either in the pre-\\ntraining or ﬁne-tuning phase, is straightforward, as it fol lows the well-studied general paradigm\\nof supervised learning in machine learning. However, as the complexity of the neural network\\nincreases, the demand for more labeled data also grows. This , in turn, makes the pre-training task\\nmore difﬁcult, especially when large-scale labeled data is not available.\\nA third approach to pre-training is self-supervised learning . In this approach, a neural net-\\nwork is trained using the supervision signals generated by i tself, rather than those provided by\\nhumans. This is generally done by constructing its own train ing tasks directly from unlabeled\\ndata, such as having the system create pseudo labels. While s elf-supervised learning has recently\\nemerged as a very popular method in NLP , it is not a new concept . In machine learning, a related\\nconcept is self-training where a model is iteratively improved by learning from the ps eudo labels\\nassigned to a dataset. T o do this, we need some seed data to bui ld an initial model. This model\\nthen generates pseudo labels for unlabeled data, and these p seudo labels are subsequently used to\\niteratively reﬁne and bootstrap the model itself. Such a met hod has been successfully used in sev-\\neral NLP areas, such as word sense disambiguation [ Y arowsky, 1995] and document classiﬁcation\\n[Blum and Mitchell , 1998]. Unlike the standard self-training method, self-supervi sed pre-training\\nin NLP does not rely on an initial model for annotating the dat a. Instead, all the supervision sig-\\nnals are created from the text, and the entire model is traine d from scratch. A well-known example\\nof this is training sequence models by successively predict ing a masked word given its preceding\\nor surrounding words in a text. This enables large-scale sel f-supervised learning for deep neural\\nnetworks, leading to the success of pre-training in many und erstanding, writing, and reasoning\\ntasks.\\nFigure 1.1 shows a comparison of the above three pre-training approach es. Self-supervised\\npre-training is so successful that most current state-of-t he-art NLP models are based on this\\nparadigm. Therefore, in this chapter and throughout this bo ok, we will focus on self-supervised\\npre-training. W e will show how sequence models are pre-trai ned via self-supervision and how the\\npre-trained models are applied.\\n1.1.2 Adapting Pre-trained Models\\nAs mentioned above, two major types of models are widely usedin NLP pre-training.\\n• Sequence Encoding Models . Given a sequence of words or tokens, a sequence encoding\\nmodel represents this sequence as either a real-valued vect or or a sequence of vectors, and\\nobtains a representation of the sequence. This representat ion is typically used as input to\\nanother model, such as a sentence classiﬁcation system.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 10, 'page_label': '4'}, page_content='4 Pre-training\\nUnsupervised Supervised\\nPre-training Training\\nUnlabeled\\nData\\nLabeled\\nData\\n(a) Unsupervised Pre-training\\nSupervised Supervised\\nPre-training Tuning\\nLabeled\\nData\\nT ask 1\\nLabeled\\nData\\nT ask 2\\n(b) Supervised Pre-training\\nSelf-\\nSupervised\\nSupervised\\nZero/Few\\nShot Learning\\nPre-training Tuning\\nPrompting\\nUnlabeled\\nData\\nLabeled\\nData\\n(c) Self-supervised Pre-training\\nFig. 1.1: Illustration of unsupervised, supervised, and self-super vised pre-training. In unsupervised pre-training, the\\npre-training is performed on large-scale unlabeled data. I t can be viewed as a preliminary step to have a good starting\\npoint for the subsequent optimization process, though cons iderable effort is still required to further train the model\\nwith labeled data after pre-training. In supervised pre-tr aining, the underlying assumption is that different (super vised)\\nlearning tasks are related. So we can ﬁrst train the model on o ne task, and transfer the resulting model to another task\\nwith some training or tuning effort. In self-supervised pre -training, a model is pre-trained on large-scale unlabeled data\\nvia self-supervision. The model can be well trained in this w ay , and we can efﬁciently adapt it to new tasks through\\nﬁne-tuning or prompting.\\n• Sequence Generation Models . In NLP , sequence generation generally refers to the prob-\\nlem of generating a sequence of tokens based on a given contex t. The term context has\\ndifferent meanings across applications. For example, it re fers to the preceding tokens in\\nlanguage modeling, and refers to the source-language seque nce in machine translation\\n2 .\\nW e need different techniques for applying these models to do wnstream tasks after pre-training.\\nHere we are interested in the following two methods.\\n1.1.2.1 Fine-tuning of Pre-trained Models\\nFor sequence encoding pre-training, a common method of adapting pre-trained models is ﬁne-\\ntuning. Let Encodeθ (·) denote an encoder with parameters θ, for example, Encodeθ (·) can be a\\nstandard Transformer encoder. Provided we have pre-traine d this model in some way and obtained\\nthe optimal parameters ˆθ, we can employ it to model any sequence and generate the corre sponding\\nrepresentation, like this\\nH = Encode ˆθ (x) (1.2)\\nwhere x is the input sequence {x0 ,x1 ,...,xm }, and H is the output representation which is a\\nsequence of real-valued vectors {h0 ,h1,...,hm }. Because the encoder does not work as a stan-\\ndalone NLP system, it is often integrated as a component into a bigger system. Consider, for\\nexample, a text classiﬁcation problem in which we identify t he polarity (i.e., positive, negative,\\n2 More precisely , in auto-regressive decoding of machine tra nslation, each target-language token is generated based\\non both its preceding tokens and source-language sequence.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 11, 'page_label': '5'}, page_content='1.1 Pre-training NLP Models 5\\nand neutral) of a given text. W e can build a text classiﬁcatio n system by stacking a classiﬁer\\non top of the encoder. Let Classifyω (·) be a neural network with parameters ω. Then, the text\\nclassiﬁcation model can be expressed in the form\\nPrω, ˆθ (·|x) = Classify ω (H)\\n= Classify ω (Encode ˆθ (x)) (1.3)\\nHere Prω, ˆθ (·|x) is a probability distribution over the label set {positive,negative,neutral}, and\\nthe label with the highest probability in this distribution is selected as output. T o keep the notation\\nuncluttered, we will use Fω, ˆθ (·) to denote Classifyω (Encode ˆθ (·)).\\nBecause the model parameters ωand ˆθare not optimized for the classiﬁcation task, we cannot\\ndirectly use this model. Instead, we must use a modiﬁed versi on of the model that is adapted to\\nthe task. A typical way is to ﬁne-tune the model by giving expl icit labeling in downstream tasks.\\nW e can train Fω, ˆθ (·) on a labeled dataset, treating it as a common supervised lear ning task. The\\noutcome of the ﬁne-tuning is the parameters ˜ω and ˜θ that are further optimized. Alternatively ,\\nwe can freeze the encoder parameters ˆθ to maintain their pre-trained state, and focus solely on\\noptimizing ω. This allows the classiﬁer to be efﬁciently adapted to work i n tandem with the\\npre-trained encoder.\\nOnce we have obtained a ﬁne-tuned model, we can use it to class ify a new text. For example,\\nsuppose we have a comment posted on a travel website:\\nI love the food here. It’s amazing!\\nW e ﬁrst tokenize this text into tokens 3 , and then feed the token sequence xnew into the ﬁne-tuned\\nmodel F˜ω, ˜θ (·). The model generates a distribution over classes by\\nF˜ω, ˜θ (xnew ) =\\n[\\nPr(positive|xnew) Pr(negative |xnew) Pr(neutral |xnew)\\n]\\n(1.4)\\nAnd we select the label of the entry with the maximum value as o utput. In this example it is\\npositive.\\nIn general, the amount of labeled data used in ﬁne-tuning is s mall compared to that of the\\npre-training data, and so ﬁne-tuning is less computational ly expensive. This makes the adaption\\nof pre-trained models very efﬁcient in practice: given a pre -trained model and a downstream task,\\nwe just need to collect some labeled data, and slightly adjus t the model parameters on this data. A\\nmore detailed discussion of ﬁne-tuning can be found in Secti on 1.4.\\n1.1.2.2 Prompting of Pre-trained Models\\nUnlike sequence encoding models, sequence generation models are often employed independently\\nto address language generation problems, such as question a nswering and machine translation,\\nwithout the need for additional modules. It is therefore str aightforward to ﬁne-tune these models\\n3 The text can be tokenized in many different ways. One of the si mplest is to segment the text into English words\\nand punctuations {I, love, the, food, here, ., It, ’s, amazing, !}'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 12, 'page_label': '6'}, page_content='6 Pre-training\\nas complete systems on downstream tasks. For example, we can ﬁne-tune a pre-trained encoder-\\ndecoder multilingual model on some bilingual data to improv e its performance on a speciﬁc trans-\\nlation task.\\nAmong various sequence generation models, a notable exampl e is the large language models\\ntrained on very large amounts of data. These language models are trained to simply predict the next\\ntoken given its preceding tokens. Although token predictio n is such a simple task that it has long\\nbeen restricted to “language modeling” only , it has been fou nd to enable the learning of the general\\nknowledge of languages by repeating the task a large number o f times. The result is that the\\npre-trained large language models exhibit remarkably good abilities in token prediction, making\\nit possible to transform numerous NLP problems into simple t ext generation problems through\\nprompting the large language models. For example, we can fra me the above text classiﬁcation\\nproblem as a text generation task\\nI love the food here. It’s amazing! I’m\\nHere indicates the word or phrase we want to predict (call it the completion). If the predicted\\nword is happy, or glad, or satisﬁed or a related positive word, we can classify the text as positive.\\nThis example shows a simple prompting method in which we conc atenate the input text with I’m\\nto form a prompt. Then, the completion helps decide which lab el is assigned to the original text.\\nGiven the strong performance of language understanding and generation of large language\\nmodels, a prompt can instruct the models to perform more comp lex tasks. Here is a prompt where\\nwe prompt the LLM to perform polarity classiﬁcation with an i nstruction.\\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\\nneutral}. Identify the polarity of the input.\\nInput: I love the food here. It’s amazing!\\nPolarity:\\nThe ﬁrst two sentences are a description of the task. Input and Polarity are indicators of the input\\nand output, respectively . W e expect the model to complete th e text and at the same time give the\\ncorrect polarity label. By using instruction-based prompt s, we can adapt large language models to\\nsolve NLP problems without the need for additional training .\\nThis example also demonstrates the zero-shot learning capa bility of large language models,\\nwhich can perform tasks that were not observed during the tra ining phase. Another method for\\nenabling new capabilities in a neural network is few-shot le arning. This is typically achieved\\nthrough in-context learning (ICT). More speciﬁcally , we add some samples that demonstrate ho w\\nan input corresponds to an output. These samples, known as demonstrations, are used to teach\\nlarge language models how to perform the task. Below is an exa mple involving demonstrations'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 13, 'page_label': '7'}, page_content='1.2 Self-supervised Pre-training T asks 7\\nAssume that the polarity of a text is a label chosen from {posi tive, negative,\\nneutral}. Identify the polarity of the input.\\nInput: The trafﬁc is terrible during rush hours, making it difﬁcul t to reach the\\nairport on time.\\nPolarity: Negative\\nInput: The weather here is wonderful.\\nPolarity: Positive\\nInput: I love the food here. It’s amazing!\\nPolarity:\\nPrompting and in-context learning play important roles in t he recent rise of large language\\nmodels. W e will discuss these issues more deeply in Chapter 3 . However, it is worth noting\\nthat while prompting is a powerful way to adapt large languag e models, some tuning efforts are\\nstill needed to ensure the models can follow instructions ac curately . Additionally , the ﬁne-tuning\\nprocess is crucial for aligning the values of these models wi th human values. More detailed\\ndiscussions of ﬁne-tuning can be found in Chapter 4.\\n1.2 Self-supervised Pre-training T asks\\nIn this section, we consider self-supervised pre-trainingapproaches for different neural architec-\\ntures, including decoder-only , encoder-only , and encoder -decoder architectures. W e restrict our\\ndiscussion to Transformers since they form the basis of most pre-trained models in NLP . How-\\never, pre-training is a broad concept, and so we just give a br ief introduction to basic approaches\\nin order to make this section concise.\\n1.2.1 Decoder-only Pre-training\\nThe decoder-only architecture has been widely used in developing language models [\\nRadford et al. ,\\n2018]. For example, we can use a Transformer decoder as a language model by simply removing\\ncross-attention sub-layers from it. Such a model predicts t he distribution of tokens at a position\\ngiven its preceding tokens, and the output is the token with t he maximum probability . The stan-\\ndard way to train this model, as in the language modeling prob lem, is to minimize a loss function\\nover a collection of token sequences. Let Decoderθ (·) denote a decoder with parameters θ. At\\neach position i, the decoder generates a distribution of the next tokens bas ed on its preceding\\ntokens {x0 ,...,xi }, denoted by Prθ (·|x0 ,...,xi ) (or pθ\\ni+1 for short). Suppose we have the gold-\\nstandard distribution at the same position, denoted by pgold\\ni+1 . For language modeling, we can think\\nof pgold\\ni+1 as a one-hot representation of the correct predicted word. W e then deﬁne a loss function\\nL(pθ\\ni+1,pgold\\ni+1 ) to measure the difference between the model prediction and t he true prediction. In\\nNLP , the log-scale cross-entropy loss is typically used.\\nGiven a sequence of mtokens {x0 ,...,xm }, the loss on this sequence is the sum of the loss'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 14, 'page_label': '8'}, page_content='8 Pre-training\\nover the positions {0,...,m − 1}, given by\\nLossθ (x0 ,...,xm ) =\\nm−1∑\\ni=0\\nL(pθ\\ni+1 ,pgold\\ni+1 )\\n=\\nm−1∑\\ni=0\\nLogCrossEntropy(pθ\\ni+1 ,pgold\\ni+1 ) (1.5)\\nwhere LogCrossEntropy(·) is the log-scale cross-entropy , and pgold\\ni+1 is the one-hot representation\\nof xi+1.\\nThis loss function can be extended to a set of sequences D. In this case, the objective of\\npre-training is to ﬁnd the best parameters that minimize the loss on D\\nˆθ = arg min\\nθ\\n∑\\nx∈D\\nLossθ (x) (1.6)\\nNote that this objective is mathematically equivalent to ma ximum likelihood estimation, and can\\nbe re-expressed as\\nˆθ = arg max\\nθ\\n∑\\nx∈D\\nlog Pr θ (x)\\n= arg max\\nθ\\n∑\\nx∈D\\ni−1∑\\ni=0\\nlog Pr θ (xi+1|x0 ,...,xi ) (1.7)\\nWith these optimized parameters ˆθ, we can use the pre-trained language model Decoder ˆθ (·)\\nto compute the probability Prˆθ (xi+1|x0 ,...,xi ) at each position of a given sequence.\\n1.2.2 Encoder-only Pre-training\\nAs deﬁned in Section\\n1.1.2.1, an encoder Encoderθ (·) is a function that reads a sequence of\\ntokens x = x0 ...xm and produces a sequence of vectors H = h0...hm 4. Training this model is\\nnot straightforward, as we do not have gold-standard data fo r measuring how good the output of\\nthe real-valued function is. A typical approach to encoder p re-training is to combine the encoder\\nwith some output layers to receive supervision signals that are easier to obtain. Figure 1.2 shows\\na common architecture for pre-training Transformer encode rs, where we add a Softmax layer on\\ntop of the Transformer encoder. Clearly , this architecture is the same as that of the decoder-based\\nlanguage model, and the output is a sequence of probability d istributions\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\npW,θ\\n1\\n.\\n.\\n.\\npW,θ\\nm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb= Softmax W(Encoderθ (x)) (1.9)\\n4 If we view hi as a row vector, H can be written as\\nH =\\n\\uf8ee\\n\\uf8ef\\n\\uf8f0\\nh0\\n.\\n.\\n.\\nhm\\n\\uf8f9\\n\\uf8fa\\n\\uf8fb (1.8)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 15, 'page_label': '9'}, page_content='1.2 Self-supervised Pre-training T asks 9\\nx0 x1 x2 x3 x4\\n(masked)\\ne0 e1 e2 e3 e4\\nEncoder\\nSoftmax\\nmodel reconstructs the masked token\\nE.g., evaluate how well the\\nSelf-supervision\\n(a) Pre-training\\nx0 x1 x2 x3 x4\\ne0 e1 e2 e3 e4\\nPre-trained Encoder\\nPrediction Network\\nOutput for Downstream T asks\\n(b) Applying the Pre-trained Encoder\\nFig. 1.2: Pre-training a Transformer encoder (left) and then applyin g the pre-trained encoder (right). In the pre-training\\nphase, the encoder, together with a Softmax layer, is traine d via self-supervision. In the application phase, the Softm ax\\nlayer is removed, and the pre-trained encoder is combined wi th a prediction network to address speciﬁc problems. In\\ngeneral, for better adaptation to these tasks, the system is ﬁne-tuned using labeled data.\\nHere pW,θ\\ni is the output distribution Pr(·|x) at position i. W e use SoftmaxW(·) to denote that\\nthe Softmax layer is parameterized by W, that is, SoftmaxW(H) = Softmax( H · W). For\\nnotation simplicity , we will sometimes drop the superscrip ts W and θafﬁxed to each probability\\ndistribution.\\nThe difference between this model and standard language mod els is that the output pi has\\ndifferent meanings in encoder pre-training and language mo deling. In language modeling, pi is\\nthe probability distribution of predicting the next word. T his follows an auto-regressive decoding\\nprocess: a language model only observes the words up to posit ion i and predicts the next. By\\ncontrast, in encoder pre-training, the entire sequence can be observed at once, and so it makes no\\nsense to predict any of the tokens in this sequence.\\n1.2.2.1 Masked Language Modeling\\nOne of the most popular methods of encoder pre-training ismasked language modeling , which\\nforms the basis of the well-known BERT model [\\nDevlin et al. , 2019]. The idea of masked lan-\\nguage modeling is to create prediction challenges by maskin g out some of the tokens in the input\\nsequence and training a model to predict the masked tokens. I n this sense, the conventional lan-\\nguage modeling problem, which is sometimes called causal language modeling , is a special case\\nof masked language modeling: at each position, we mask the to kens in the right-context, and pre-\\ndict the token at this position using its left context. Howev er, in causal language modeling we\\nonly make use of the left-context in word prediction, while t he prediction may depend on tokens\\nin the right-context. By contrast, in masked language model ing, all the unmasked tokens are used\\nfor word prediction, leading to a bidirectional model that m akes predictions based on both left and\\nright-contexts.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 16, 'page_label': '10'}, page_content='10 Pre-training\\nMore formally , for an input sequence x = x0...xm , suppose that we mask the tokens at po-\\nsitions A(x) = {i1 ,...,iu }. Hence we obtain a masked token sequence ¯x where the token at\\neach position in A(x) is replaced with a special symbol [MASK]. For example, for the following\\nsequence\\nThe early bird catches the worm\\nwe may have a masked token sequence like this\\nThe [MASK] bird catches the [MASK]\\nwhere we mask the tokens early and worm (i.e., i1 = 2 and i2 = 6 ).\\nNow we have two sequences x and ¯x. The model is then optimized so that we can correctly\\npredict x based on ¯x. This can be thought of as an autoencoding-like process, and the train-\\ning objective is to maximize the reconstruction probabilit y Pr(x|¯x). Note that there is a simple\\nposition-wise alignment between x and ¯x. Because an unmasked token in ¯x is the same as the to-\\nken in x at the same position, there is no need to consider the predict ion for this unmasked token.\\nThis leads to a simpliﬁed training objective which only maxi mizes the probabilities for masked\\ntokens. W e can express this objective in a maximum likelihoo d estimation fashion\\n( ˆW,ˆθ) = arg max\\nW,θ\\n∑\\nx∈D\\n∑\\ni∈A(x)\\nlog Pr W,θ\\ni (xi |¯x) (1.10)\\nor alternatively express it using the cross-entropy loss\\n( ˆW,ˆθ) = arg min\\nW,θ\\n∑\\nx∈D\\n∑\\ni∈A(x)\\nLogCrossEntropy(pW,θ\\ni ,pgold\\ni) (1.11)\\nwhere PrW,θ\\nk (xk |¯x) is the probability of the true token xk at position kgiven the corrupted input\\nx, and pW,θ\\nk is the probability distribution at position kgiven the corrupted input x. T o illustrate,\\nconsider the above example where two tokens of the sequence “ the early bird catches the worm ”\\nare masked. For this example, the objective is to maximize th e sum of log-scale probabilities\\nLoss = log Pr( x2 = early|¯x= [CLS] The [MASK]\\n\\ued19\\n\\ued18\\ued17 \\ued1a\\n¯x2\\nbird catches the [MASK]\\n\\ued19\\n\\ued18\\ued17 \\ued1a\\n¯x6\\n) +\\nlog Pr( x6 = worm|¯x= [CLS] The [MASK]\\n\\ued19\\n\\ued18\\ued17 \\ued1a\\n¯x2\\nbird catches the [MASK]\\n\\ued19\\n\\ued18\\ued17 \\ued1a\\n¯x6\\n) (1.12)\\nOnce we obtain the optimized parameters ˆW and ˆθ, we can drop ˆW. Then, we can further\\nﬁne-tune the pre-trained encoder Encoder ˆθ (·) or directly apply it to downstream tasks.\\n1.2.2.2 Permuted Language Modeling\\nWhile masked language modeling is simple and widely applied, it introduces new issues. One\\ndrawback is the use of a special token, [MASK], which is employed only during training but not'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 17, 'page_label': '11'}, page_content='1.2 Self-supervised Pre-training T asks 11\\nat test time. This leads to a discrepancy between training an d inference. Moreover, the auto-\\nencoding process overlooks the dependencies between maske d tokens. For example, in the above\\nexample, the prediction of x2 (i.e., the ﬁrst masked token) is made independently of x6 (i.e., the\\nsecond masked token), though x6 should be considered in the context of x2 .\\nThese issues can be addressed using the permuted language modeling approach to pre-\\ntraining [ Y ang et al. , 2019]. Similar to causal language modeling, permuted language m odeling\\ninvolves making sequential predictions of tokens. However , unlike causal modeling where predic-\\ntions follow the natural sequence of the text (like left-to- right or right-to-left), permuted language\\nmodeling allows for predictions in any order. The approach i s straightforward: we determine an\\norder for token predictions and then train the model in a stan dard language modeling manner, as\\ndescribed in Section 1.2.1. Note that in this approach, the actual order of tokens in the text remains\\nunchanged, and only the order in which we predict these token s differs from standard language\\nmodeling. For example, consider a sequence of 5 tokens x0x1 x2x3x4 . Let ei represent the em-\\nbedding of xi (i.e., combination of the token embedding and positional em bedding). In standard\\nlanguage modeling, we would generate this sequence in the or der of x0 → x1 → x2 → x3 → x4.\\nThe probability of the sequence can be modeled via a generati on process.\\nPr(x) = Pr( x0) · Pr(x1 |x0) · Pr(x2|x0 ,x1 ) · Pr(x3 |x0 ,x1 ,x2) ·\\nPr(x4|x0 ,x1 ,x2,x3 )\\n= Pr( x0) · Pr(x1 |e0) · Pr(x2|e0 ,e1 ) · Pr(x3 |e0 ,e1,e2 ) ·\\nPr(x4|e0 ,e1 ,e2,e3 ) (1.13)\\nNow , let us consider a different order for token prediction: x0 → x4 → x2 → x1 → x3 . The\\nsequence generation process can then be expressed as follow s:\\nPr(x) = Pr( x0) · Pr(x4 |e0) · Pr(x2|e0 ,e4 ) · Pr(x1 |e0 ,e4,e2 ) ·\\nPr(x3|e0 ,e4 ,e2,e1 ) (1.14)\\nThis new prediction order allows for the generation of some t okens to be conditioned on a\\nbroader context, rather than being limited to just the prece ding tokens as in standard language\\nmodels. For example, in generating x3, the model considers both its left-context (i.e., e0,e1 ,e2 )\\nand right-context (i.e., e4). The embeddings e0,e1 ,e2 ,e4 incorporate the positional information\\nof x0,x1 ,x2 ,x4 , preserving the original order of the tokens. As a result, th is approach is somewhat\\nakin to masked language modeling: we mask out x3 and use its surrounding tokens x0 ,x1 ,x2,x4\\nto predict this token.\\nThe implementation of permuted language models is relative ly easy for Transformers. Be-\\ncause the self-attention model is insensitive to the order o f inputs, we do not need to explicitly\\nreorder the sequence to have a factorization like Eq. ( 1.14). Instead, permutation can be done\\nby setting appropriate masks for self-attention. For examp le, consider the case of computing\\nPr(x1 |e0,e4 ,e2 ). W e can place x0,x1 ,x2 ,x3,x4 in order and block the attention from x3 to x1\\nin self-attention, as illustrated below'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 18, 'page_label': '12'}, page_content='12 Pre-training\\nx0 x1 x2 x3 x4\\nMasks for Self-attention:\\nBlue box = valid attention\\nGray box = blocked attention\\nFor a more illustrative example, we compare the self-attent ion masking results of causal language\\nmodeling, masked language modeling and permuted language m odeling in Figure\\n1.3.\\n1.2.2.3 Pre-training Encoders as Classiﬁers\\nAnother commonly-used idea to train an encoder is to consider classiﬁcation tasks. In self-\\nsupervised learning, this is typically done by creating new classiﬁcation challenges from the unla-\\nbeled text. There are many different ways to design the class iﬁcation tasks. Here we present two\\npopular tasks.\\nA simple method, called next sentence prediction (NSP), is presented in BERT’s original\\npaper [\\nDevlin et al. , 2019]. The assumption of NSP is that a good text encoder should cap ture\\nthe relationship between two sentences. T o model such a rela tionship, in NSP we can use the\\noutput of encoding two consecutive sentences SentA and SentB to determine whether SentB is\\nthe next sentence following SentA. For example, suppose SentA = ’It is raining . ’ and SentB =\\n’I need an umbrella . ’ . The input sequence of the encoder could be\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nwhere [CLS] is the start symbol (i.e., x0) which is commonly used in encoder pre-training, and\\n[SEP] is a separator that separates the two sentences. The process ing of this sequence follows a\\nstandard procedure of Transformer encoding: we ﬁrst repres ent each token xi as its corresponding\\nembedding ei, and then feed the embedding sequence {e0 ,...,em } into the encoder to obtain the\\noutput sequence {h0 ,...,hm }. Since h0 is generally considered as the representation of the entire\\nsequence, we add a Softmax layer on top of it to construct a bin ary classiﬁcation system. This\\nprocess is illustrated as follows\\ntoken: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nembedding: e0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11\\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\\nEncoder\\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\\nencoding: h0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11\\n↓\\nSoftmax\\n↓\\nIs Next or Not?'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 19, 'page_label': '13'}, page_content='1.2 Self-supervised Pre-training T asks 13\\nx0\\nx0\\nx1\\nx1\\nx2\\nx2\\nx3\\nx3\\nx4\\nx4\\nPr(x0) = 1\\nPr(x1|e0 )\\nPr(x2|e0 ,e1 )\\nPr(x3|e0 ,e1 ,e2 )\\nPr(x4|e0 ,e1 ,e2 ,e3)\\n(a) Causal Language Modeling (order: x0 → x1 → x2 → x3 → x4 )\\nx0\\nx0\\nx1\\nx1\\nx2\\nx2\\nx3\\nx3\\nx4\\nx4\\nmasked masked\\nmasked\\nmasked\\n1\\nPr(x1|e0 ,emask ,e2 ,emask ,e4 )\\n1\\nPr(x3|e0 ,emask ,e2 ,emask ,e4 )\\n1\\n(b) Masked Language Modeling (order: x0,[MASK],x2 ,[MASK],x4 → x1 ,x3)\\nx0\\nx0\\nx1\\nx1\\nx2\\nx2\\nx3\\nx3\\nx4\\nx4\\nPr(x0) = 1\\nPr(x1|e0 ,e4 ,e2 )\\nPr(x2|e0 ,e4 )\\nPr(x3|e0 ,e4 ,e2 ,e1)\\nPr(x4|e0 )\\n(c) Permuted Language Modeling (order: x0 → x4 → x2 → x1 → x3 )\\nFig. 1.3: Comparison of self-attention masking results of causal lan guage modeling, masked language modeling and\\npermuted language modeling. The gray cell denotes the token at position j does not attend to the token at position i.\\nThe blue cell (i,j ) denotes that the token at position jattends to the token at position i. emask represents the embedding\\nof the symbol [MASK], which is a combination of the token embedding and the positi onal embedding.\\nIn order to generate training samples, we need two sentences each time, one for SentA and\\nthe other for SentB . A simple way to do this is to utilize the natural sequence of t wo consecu-\\ntive sentences in the text. For example, we obtain a positive sample by using actual consecutive\\nsentences, and a negative sample by using randomly sampled s entences. Consequently , training\\nthis model is the same as training a classiﬁer. T ypically , NS P is used as an additional training loss'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 20, 'page_label': '14'}, page_content='14 Pre-training\\nfunction for pre-training based on masked language modelin g.\\nA second example of training Transformer encoders as classi ﬁers is to apply classiﬁcation-\\nbased supervision signals to each output of an encoder. For e xample, Clark et al. [2019] in their\\nELECTRA model, propose training a Transformer encoder to id entify whether each input token\\nis identical to the original input or has been altered in some manner. The ﬁrst step of this method\\nis to generate a new sequence from a given sequence of tokens, where some of the tokens are\\naltered. T o do this, a small masked language model (call it th e generator) is applied: we randomly\\nmask some of the tokens, and train this model to predict the ma sked tokens. For each training\\nsample, this masked language model outputs a token at each ma sked position, which might be\\ndifferent from the original token. At the same time, we train another Transformer encoder (call it\\nthe discriminator) to determine whether each predicted tok en is the same as the original token or\\naltered. More speciﬁcally , we use the generator to generate a sequence where some of the tokens\\nare replaced. Below is an illustration.\\noriginal: [CLS] The boy spent hours working on toys .\\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\\nmasked: [CLS] The boy spent [MASK] working on [MASK] .\\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\\nGenerator (small masked language model)\\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\\nreplaced: [CLS] The boy spent decades working on toys .\\nThen, we use the discriminator to label each of these tokens a s orginal or replaced, as follows\\nreplaced: [CLS] The boy spent decades working on toys .\\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\\nDiscriminator (the model we want)\\n↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓ ↓\\nlabel: original original original original replaced original original original original\\nFor training, the generator is optimized as a masked languag e model with maximum likelihood\\nestimation, and the discriminator is optimized as a classiﬁ er using a classiﬁcation-based loss. In\\nELECTRA, the maximum likelihood-based loss and the classiﬁ cation-based loss are combined for\\njointly training both the generator and discriminator. An a lternative approach is to use generative\\nadversarial networks (GANs), that is, the generator is trai ned to fool the discriminator, and the dis-\\ncriminator is trained to distinguish the output of the gener ator from the true distribution. However,\\nGAN-style training complicates the training task and is mor e difﬁcult to scale up. Nevertheless,\\nonce training is complete, the generator is discarded, and t he encoding part of the discriminator is\\napplied as the pre-trained model for downstream tasks.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 21, 'page_label': '15'}, page_content='1.2 Self-supervised Pre-training T asks 15\\n1.2.3 Encoder-Decoder Pre-training\\nIn NLP , encoder-decoder architectures are often used to model sequence-to-sequence problems,\\nsuch as machine translation and question answering. In addi tion to these typical sequence-to-\\nsequence problems in NLP , encoder-decoder models can be ext ended to deal with many other\\nproblems. A simple idea is to consider text as both the input a nd output of a problem, and so\\nwe can directly apply encoder-decoder models. For example, given a text, we can ask a model to\\noutput a text describing the sentiment of the input text, suc h as positive, negative, and neutral.\\nSuch an idea allows us to develop a single text-to-text syste m to address any NLP problem.\\nW e can formulate different problems into the same text-to-t ext format. W e ﬁrst train an encoder-\\ndecoder model to gain general-purpose knowledge of languag e via self-supervision. This model\\nis then ﬁne-tuned for speciﬁc downstream tasks using target ed text-to-text data.\\n1.2.3.1 Masked Encoder-Decoder Pre-training\\nIn\\nRaffel et al. [2020]’s T5 model, many different tasks are framed as the same text-to-t ext task.\\nEach sample in T5 follows the format\\nSource T ext → T arget T ext\\nHere → separates the source text, which consists of a task descript ion or instruction and the input\\ngiven to the system, from the target text, which is the respon se to the input task. As an example,\\nconsider a task of translating from Chinese to English. A tra ining sample can be expressed as\\n[CLS] T ranslate from Chinese to English: 你好！ → ⟨ s⟩ Hello!\\nwhere [CLS] and ⟨ s⟩ are the start symbols on the source and target sides, respect ively5 .\\nLikewise, we can express other tasks in the same way . For exam ple\\n[CLS] Answer: when was Albert Einstein born?\\n→ ⟨ s⟩ He was born on March 14, 1879.\\n[CLS] Simplify: the professor, who has has published numerous pap ers in his ﬁeld,\\nwill be giving a lecture on the topic next week.\\n→ ⟨ s⟩ The experienced professor will give a lecture next week.\\n[CLS] Score the translation from English to Chinese. English: whe n in Rome, do as\\nthe Romans do. Chinese: 人 在 罗马 就 像 罗马 人 一样 做事 。\\n→ ⟨ s⟩ 0.81\\nwhere instructions are highlighted in gray . An interesting case is that in the last example we\\n5 W e could use the same start symbol for different sequences. H ere we use different symbols to distinguish the\\nsequences on the encoder and decoder-sides.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 22, 'page_label': '16'}, page_content='16 Pre-training\\nreframe the scoring problem as the text generation problem. Our goal is to generate a text repre-\\nsenting the number 0.81, rather than outputting it as a numerical value.\\nThe approach described above provides a new framework of uni versal language understanding\\nand generation. Both the task instructions and the problem i nputs are provided to the system\\nin text form. The system then follows the instructions to com plete the task. This method puts\\ndifferent problems together, with the beneﬁt of training a s ingle model that can perform many\\ntasks simultaneously .\\nIn general, ﬁne-tuning is necessary for adapting the pre-tr ained model to a speciﬁc downstream\\ntask. In this process, one can use different ways to instruct the model for the task, such as using a\\nshort name of the task as the preﬁx to the actual input sequenc e or providing a detailed description\\nof the task. Since the task instructions are expressed in tex t form and involved as part of the input,\\nthe general knowledge of instruction can be gained through l earning the language understanding\\nmodels in the pre-training phase. This may help enable zero- shot learning. For example, pre-\\ntrained models can generalize to address new problems where the task instructions have never\\nbeen encountered.\\nThere have been several powerful methods of self-supervise d learning for either Transformer\\nencoders or decoders. Applying these methods to pre-train e ncoder-decoder models is relatively\\nstraightforward. One common choice is to train encoder-dec oder models as language models. For\\nexample, the encoder receives a sequence preﬁx, while the de coder generates the remaining se-\\nquence. However, this differs from standard causal languag e modeling, where the entire sequence\\nis autoregressively generated from the ﬁrst token. In our ca se, the encoder processes the preﬁx at\\nonce, and then the decoder predicts subsequent tokens in the manner of causal language modeling.\\nPut more precisely , this is a preﬁx language modeling problem: a language model predicts the\\nsubsequent sequence given a preﬁx, which serves as the conte xt for prediction.\\nConsider the following example\\n[CLS] The puppies are frolicking\\n\\ued19\\n\\ued18\\ued17 \\ued1a\\nPreﬁx\\n→ ⟨ s⟩ outside the house .\\ued19 \\ued18\\ued17 \\ued1a\\nSubsequent Sequence\\nW e can directly train an encoder-decoder model using exampl es like this. Then, the encoder learns\\nto understand the preﬁx, and the decoder learns to continue w riting based on this understanding.\\nFor large-scale pre-training, it is easy to create a large nu mber of training examples from unlabeled\\ntext.\\nIt is worth noting that for pre-trained encoder-decoder mod els to be effective in multi-lingual\\nand cross-lingual tasks, such as machine translation, they should be trained with multi-lingual\\ndata. This typically requires that the vocabulary includes tokens from all the languages. By\\ndoing so, the models can learn shared representations acros s different languages, thereby enabling\\ncapabilities in both language understanding and generatio n in a multi-lingual and cross-lingual\\ncontext.\\nA second approach to pre-training encoder-decoder models i s masked language modeling. In\\nthis approach, as discussed in Section 1.2.2, tokens in a sequence are randomly replaced with a\\nmask symbol, and the model is then trained to predict these ma sked tokens based on the entire\\nmasked sequence.\\nAs an illustration, consider the task of masking and reconst ructing the sentence'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 23, 'page_label': '17'}, page_content='1.2 Self-supervised Pre-training T asks 17\\nThe puppies are frolicking outside the house .\\nBy masking two tokens (say , frolicking and the), we have the BERT -style input and output of the\\nmodel, as follows\\n[CLS] The puppies are [MASK] outside [MASK] house .\\n→ ⟨ s⟩ frolicking the\\nHere denotes the masked position at which we do not make token pred ictions. By varying the\\npercentage of the tokens in the text, this approach can be gen eralized towards either BERT -style\\ntraining or language modeling-style training [ Song et al. , 2019]. For example, if we mask out all\\nthe tokens, then the model is trained to generate the entire s equence\\n[CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MAS K]\\n→ ⟨ s⟩ The puppies are frolicking outside the house .\\nIn this case, we train the decoder as a language model.\\nNote that, in the context of the encoder-decoder architectu re, we can use the encoder to read\\nthe masked sequence, and use the decoder to predict the origi nal sequence. With this objective,\\nwe essentially have a denoising autoencoder: the encoder tr ansforms a corrupted input into some\\nhidden representation, and the decoder reconstructs the un corrupted input from this hidden repre-\\nsentation. Here is an example of input and output for denoisi ng training.\\n[CLS] The puppies are [MASK] outside [MASK] house .\\n→ ⟨ s⟩ The puppies are frolicking outside the house .\\nBy learning to map from this corrupted sequence to its uncorr upted counterpart, the model gains\\nthe ability to understand on the encoder side and to generate on the decoder side. See Figure 1.4\\nfor an illustration of how an encoder-decoder model is train ed with BERT -style and denoising\\nautoencoding objectives.\\nAs we randomly select tokens for masking, we can certainly ma sk consecutive tokens [ Joshi et al. ,\\n2020]. Here is an example.\\n[CLS] The puppies are [MASK] outside [MASK] [MASK] .\\n→ ⟨ s⟩ The puppies are frolicking outside the house .\\nAnother way to consider consecutive masked tokens is to repr esent them as spans. Here we\\nfollow Raffel et al. [2020]’s work, and use [X], [Y] and [Z] to denote sentinel tokens that cover\\none or more consecutive masked tokens. Using this notation, we can re-express the above training\\nexample as\\n[CLS] The puppies are [X] outside [Y] .\\n→ ⟨ s⟩ [X] frolicking [Y] the house [Z]'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 24, 'page_label': '18'}, page_content='18 Pre-training\\n[CLS] The puppies are [M] in [M] house .\\nEncoder Decoder\\nfrolicking[M] [M] [M]⟨ s⟩ [M] the [M]\\n[M] [M] [M] frolicking [M] the [M] [M]\\nLoss\\n(a) Training an encoder-decoder model with BER T -style mask ed language modeling\\n[CLS] The puppies are [M] in [M] house .\\nEncoder Decoder\\nfrolickingarepuppiesThe⟨ s⟩ in the house\\nThe puppies are frolicking in the house .\\n(b) Training an encoder-decoder model with denoising autoe ncoding\\nLoss over the sequence\\nFig. 1.4: Training an encoder-decoder model using BERT -style and den oising autoencoding methods. In both methods,\\nthe input to the encoder is a corrupted token sequence where s ome tokens are masked and replaced with [MASK] (or\\n[M] for short). The decoder predicts these masked tokens, but in different ways. In BERT -style training, the decoder\\nonly needs to compute the loss for the masked tokens, while th e remaining tokens in the sequence can be simply treated\\nas [MASK] tokens. In denoising autoencoding, the decoder predicts th e sequence of all tokens in an autoregressive\\nmanner. As a result, the loss is obtained by accumulating the losses of all these tokens, as in standard language\\nmodeling.\\nThe idea is that we represent the corrupted sequence as a sequ ence containing placeholder\\nslots. The training task is to ﬁll these slots with the correc t tokens using the surrounding context.\\nAn advantage of this approach is that the sequences used in tr aining would be shorter, making the\\ntraining more efﬁcient. Note that masked language modeling provides a very general framework\\nfor training encoder-decoder models. V arious settings can be adjusted to have different training\\nversions, such as altering the percentage of tokens masked a nd the maximum length of the masked\\nspans.\\n1.2.3.2 Denoising T raining\\nIf we view the problem of training encoder-decoder models asa problem of training denoising\\nautoencoders, there will typically be many different metho ds for introducing input corruption and\\nreconstructing the input. For instance, beyond randomly ma sking tokens, we can also alter some\\nof them or rearrange their order.\\nSuppose we have an encoder-decoder model that can map an inpu t sequence x to an output'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 25, 'page_label': '19'}, page_content='1.2 Self-supervised Pre-training T asks 19\\nsequence y\\ny = Decode ω (Encodeθ (x))\\n= Model θ,ω (x) (1.15)\\nwhere θ and ω are the parameters of the encoder and the decoder, respectiv ely . In denoising\\nautoencoding problems, we add some noise to x to obtain a noisy , corrupted input xnoise. By\\nfeeding xnoise into the encoder, we wish the decoder to output the original i nput. The training\\nobjective can be deﬁned as\\n( ˆθ,ˆω) = arg max\\nθ,ω\\nLoss(Modelθ,ω (xnoise ),x) (1.16)\\nHere the loss function Loss(Modelθ,ω (xnoise ),x) evaluates how well the model Modelθ,ω (xnoise )\\nreconstructs the original input x. W e can choose the cross-entropy loss as usual.\\nAs the model architecture and the training approach have bee n developed, the remaining issue\\nis the corruption of the input. Lewis et al. [2020], in their BART model, propose corrupting the\\ninput sequence in several different ways.\\n• T oken Masking. This is the same masking method that we used in masked langua ge mod-\\neling. The tokens in the input sequence are randomly selecte d and masked.\\n• T oken Deletion. This method is similar to token masking. However, rather th an replacing\\nthe selected tokens with a special symbol [MASK], these tokens are removed from the\\nsequence. See the following example for a comparison of the t oken masking and token\\ndeletion methods.\\nOriginal ( x): The puppies are frolicking outside the house .\\nT oken Masking ( xnoise ): The puppies are [MASK] outside [MASK] house .\\nT oken Deletion ( xnoise ): The puppies are frolicking outside the house .\\nwhere the underlined tokens in the original sequence are mas ked or deleted.\\n• Span Masking . Non-overlapping spans are randomly sampled over the seque nce. Each\\nspan is masked by [MASK]. W e also consider spans of length 0, and, in such cases, [MASK]\\nis simply inserted at a position in the sequence. For example , we can use span masking to\\ncorrupt the above sequence as\\nOriginal ( x): The 0 puppies are frolicking outside the house .\\nSpan Masking ( xnoise ): The [MASK] puppies are [MASK] house .\\nHere the span frolicking outside the is replaced with a single [MASK]. 0 indicates a length-\\n0 span, and so we insert an [MASK] between The and puppies. Span masking introduces\\nnew prediction challenges in which the model needs to know ho w many tokens are gener-\\nated from a span. This problem is very similar to fertility mo deling in machine translation\\n[Brown et al. , 1993].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 26, 'page_label': '20'}, page_content='20 Pre-training\\nIf we consider a sequence consisting of multiple sentences, additional methods of corruption\\ncan be applied. In the BART model, there are two such methods.\\n• Sentence Reordering . This method randomly permutes the sentences so that the mod el can\\nlearn to reorder sentences in a document. Consider, for exam ple, two consecutive sentences\\nHard work leads to success . Success brings happiness .\\nW e can reorder the two sentences to have a corrupted input seq uence\\nSuccess brings happiness . Hard work leads to success .\\n• Document Rotation . The goal of this task is to identify the start token of the seq uence.\\nFirst, a token is randomly selected from the sequence. Then, the sequence is rotated so that\\nthe selected token is the ﬁrst token. For example, suppose we select the token leads from\\nthe above sequence. The rotated sequence is\\nleads to success . Success brings happiness .Hard work Hard work\\nselected\\nwhere the subsequence Hard work before leads is appended to the end of the sequence.\\nFor pre-training, we can apply multiple corruption methods to learn robust models, for ex-\\nample, we randomly choose one of them for each training sampl e. In practice, the outcome of\\nencoder-decoder pre-training depends heavily on the input corruption methods used, and so we\\ntypically need to choose appropriate training objectives t hrough careful experimentation.\\n1.2.4 Comparison of Pre-training T asks\\nSo far, we have discussed a number of pre-training tasks. Since the same training objective can\\napply to different architectures (e.g., using masked langu age modeling for both encoder-only and\\nencoder-decoder pre-training), categorizing pre-traini ng tasks based solely on model architecture\\ndoes not seem ideal. Instead, we summarize these tasks based on the training objectives.\\n• Language Modeling . T ypically , this approach refers to an auto-regressive gen eration pro-\\ncedure of sequences. At one time, it predicts the next token b ased on its previous context.\\n• Masked Language Modeling . Masked Language Modeling belongs to a general mask-\\npredict framework. It randomly masks tokens in a sequence an d predicts these tokens using\\nthe entire masked sequence.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 27, 'page_label': '21'}, page_content='1.3 Example: BERT 21\\n• Permuted Language Modeling . Permuted language modeling follows a similar idea to\\nmasked language modeling, but considers the order of (maske d) token prediction. It reorders\\nthe input sequence and predicts the tokens sequentially . Ea ch prediction is based on some\\ncontext tokens that are randomly selected.\\n• Discriminative T raining . In discriminative training, supervision signals are crea ted from\\nclassiﬁcation tasks. Models for pre-training are integrat ed into classiﬁers and trained to-\\ngether with the remaining parts of the classiﬁers to enhance their classiﬁcation performance.\\n• Denoising Autoencoding . This approach is applied to the pre-training of encoder-de coder\\nmodels. The input is a corrupted sequence and the encoder-de coder models are trained to\\nreconstruct the original sequence.\\nT able 1.1 illustrates these methods and their variants using example s. The use of these ex-\\namples does not distinguish between models, but we mark the m odel architectures where the\\npre-training tasks can be applied. In each example, the inpu t consists of a token sequence, and the\\noutput is either a token sequence or some probabilities. For generation tasks, such as language\\nmodeling, superscripts are used to indicate the generation order on the target side. If the super-\\nscripts are omitted, it indicates that the output sequence c an be generated either autoregressively\\nor simultaneously . On the source side, we assume that the seq uence undergoes a standard Trans-\\nformer encoding process, meaning that each token can see the entire sequence in self-attention.\\nThe only exception is in permuted language modeling, where a n autoregressive generation pro-\\ncess is implemented by setting attention masks on the encode r side. T o simplify the discussion,\\nwe remove the token ⟨ s⟩ from the target-side of each example.\\nWhile these pre-training tasks are different, it is possibl e to compare them in the same frame-\\nwork and experimental setup [ Dong et al. , 2019; Raffel et al. , 2020; Lewis et al. , 2020]. Note that\\nwe cannot list all the pre-training tasks here as there are ma ny of them. For more discussions on\\npre-training tasks, the interested reader may refer to some surveys on this topic [ Qiu et al. , 2020;\\nHan et al. , 2021].\\n1.3 Example: BERT\\nIn this section, we introduce BERT models, which are among the most popular and widely used\\npre-trained sequence encoding models in NLP .\\n1.3.1 The Standard Model\\nThe standard BERT model, which is proposed in\\nDevlin et al. [2019]’s work, is a Transformer\\nencoder trained using both masked language modeling and nex t sentence prediction tasks. The\\nloss used in training this model is a sum of the loss of the two t asks.\\nLossBER T = Loss MLM + Loss NSP (1.17)\\nAs is regular in training deep neural networks, we optimize t he model parameters by minimizing\\nthis loss. T o do this, a number of training samples are collec ted. During training, a batch of'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 28, 'page_label': '22'}, page_content='22 Pre-training\\nMethod Enc Dec E-D Input Output\\nCausal LM • • The1 kitten2 is3 chasing4 the5 ball6 .7\\nPreﬁx LM • • [C] The kitten is chasing1 the2 ball3 .4\\nMasked LM • • [C] The kitten [M] chasing the [M] . is ball\\nMASS-style • • [C] The kitten [M] [M] [M] ball . is chasing the\\nBER T -style • • [C] The kitten [M] playing the [M] . kitten is chasing ball\\nPermuted LM • [C] The kitten is chasing the ball . The5 kitten7 is6 chasing1 the4 ball2 .3\\nNext Sentence • [C] The kitten is chasing the ball . Pr(IsNext | representation-of-[C])\\nPrediction Birds eat worms .\\nSentence • Encode a sentence as ha and Score(ha,hb )\\nComparison another sentence as hb\\nT oken Classiﬁcation • [C] The kitten is chasing the ball . Pr(·|The) Pr( ·|kitten) ... Pr(·|.)\\nT oken Reordering • [C] . kitten the chasing The is ball The1 kitten2 is3 chasing4 the5 ball6 .7\\nT oken Deletion • [C] The kitten is chasing the ball . The1 kitten2 is3 chasing4 the5 ball6 .7\\nSpan Masking • [C] The kitten [M] is [M] . The1 kitten2 is3 chasing4 the5 ball6 .7\\nSentinel Masking • [C] The kitten [X] the [Y] [X]1 is2 chasing3 [Y]4 ball5 .6\\nSentence • [C] The ball rolls away swiftly . The The1 kitten2 is3 chasing4 the5 ball6 .7\\nReordering kitten is chasing the ball . The8 ball9 rolls10 away11 swiftly12 .13\\nDocument • [C] chasing the ball . The ball rolls The1 kitten2 is3 chasing4 the5 ball6 .7\\nRotation away swiftly . The kitten is The8 ball9 rolls10 away11 swiftly12 .13\\nT able 1.1: Comparison of pre-training tasks, including language modeling , masked language modeling , permuted\\nlanguage modeling , discriminative training , and denoising autoencoding . [C] = [CLS] , [M] = [MASK] , [X], [Y] =\\nsentinel tokens . Enc, Dec and E-D indicate whether the approach can be applie d to encoder-only , decoder-only ,\\nencoder-decoder models, respectively . For generation tas ks, superscripts are used to represent the order of the token s.\\ntraining samples is randomly selected from this collection at a time, and LossBER T is accumulated\\nover these training samples. Then, the model parameters are updated via gradient descent or its\\nvariants. This process is repeated many times until some sto pping criterion is satisﬁed, such as\\nwhen the training loss converges.\\n1.3.1.1 Loss Functions\\nIn general, BERT models are used to represent a single sentence or a pair of sentences, and thus\\ncan handle various downstream language understanding prob lems. In this section we assume that\\nthe input representation is a sequence containing two sente nces SentA and SentB , expressed as\\n[CLS] Sent A [SEP] Sent B [SEP]\\nHere we follow the notation in BERT’s paper and use [SEP] to denote the separator.\\nGiven this sequence, we can obtain LossMLM and LossNSP separately . For masked language\\nmodeling, we predict a subset of the tokens in the sequence. T ypically , a certain percentage of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 29, 'page_label': '23'}, page_content='1.3 Example: BERT 23\\ntokens are randomly selected, for example, in the standard B ERT model, 15% of the tokens in\\neach sequence are selected. Then the sequence is modiﬁed in t hree ways\\n• T oken Masking . 80% of the selected tokens are masked and replaced with the symbo l\\n[MASK]. For example\\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nMasked: [CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SE P]\\nwhere the selected tokens are underlined. Predicting maske d tokens makes the model learn\\nto represent tokens from their surrounding context.\\n• Random Replacement . 10% of the selected tokens are changed to a random token. For\\nexample,\\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nRandom T oken: [CLS] It is raining . [SEP] I need an hat . [SEP]\\nThis helps the model learn to recover a token from a noisy inpu t.\\n• Unchanged. 10% of the selected tokens are kept unchanged. For example,\\nOriginal: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nUnchanged T oken: [CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nThis is not a difﬁcult prediction task, but can guide the mode l to use easier evidence for\\nprediction.\\nLet A(x) be the set of selected positions of a given token sequence x, and ¯x be the modiﬁed\\nsequence of x. The loss function of masked language modeling can be deﬁned as\\nLossMLM = −\\n∑\\ni∈A(x)\\nlog Pr i(xi |¯x) (1.18)\\nwhere Pri(xi|¯x) is the probability of predicting xi at the position igiven ¯x. Figure 1.5 shows a\\nrunning example of computing LossMLM .\\nFor next sentence prediction, we follow the method describe d in Section 1.2.2.3. Each training\\nsample is classiﬁed into a label set {IsNext,NotNext}, for example,\\nSequence: [CLS] It is raining . [SEP] I need an umbrella . [SEP ]\\nLabel: IsNext\\nSequence: [CLS] The cat sleeps on the windowsill . [SEP] Appl es grow on trees . [SEP]\\nLabel: NotNext'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 30, 'page_label': '24'}, page_content='24 Pre-training\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]Input:\\nSelect tokens with a probability of 15%\\n[CLS] It is raining . [SEP] I need an umbrella . [SEP]T oken Selection:\\nMask selected tokens with a probability of 80%\\n[CLS] It is [MASK] . [SEP] I need [MASK] umbrella . [SEP]T oken Masking:\\nAlter selected tokens with a probability of 10%\\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]T oken:\\nReplacement\\nKeep selected tokens unchanged with a probability of 10%\\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]Unchanged:\\nTrain the Transformer encoder with the modiﬁed sequence\\n[CLS] It is [MASK] . [SEP] I need [MASK] hat . [SEP]\\ne0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11\\nh0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11\\ntraining I an umbrella\\nTransformer Encoder\\nFig. 1.5: A running example of BERT -style masked language modeling. F irst, 15% tokens are randomly selected.\\nThese selected tokens are then processed in one of three ways : replaced with a [MASK] token ( 80% of the time),\\nreplaced with a random token ( 10% of the time), or kept unchanged ( 10% of the time). The model is trained to predict\\nthese selected tokens based on the modiﬁed sequence. ei represents the embedding of the token at the position i. Gray\\nboxes represent the Softmax layers.\\nThe output vector of the encoder for the ﬁrst token[CLS] is viewed as the sequence representation,\\ndenoted by hcls (or h0). A classiﬁer is built on top of hcls. Then, we can compute the probability of\\na label cgiven hcls, i.e., Pr(c|hcls ). There are many loss functions one can choose for classiﬁcat ion\\nproblems. For example, in maximum likelihood training, we c an deﬁne LossNSP as\\nLossNSP = − log Pr( cgold |hcls) (1.19)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 31, 'page_label': '25'}, page_content='1.3 Example: BERT 25\\nwhere cgold is the correct label for this sample.\\n1.3.1.2 Model Setup\\nAs shown in Figure\\n1.6, BERT models are based on the standard Transformer encoder a rchitecture.\\nThe input is a sequence of embeddings, each being the sum of th e token embedding, the positional\\nembedding, and the segment embedding.\\ne = x + epos + eseg (1.20)\\nBoth the token embedding ( x) and positional embedding ( epos ) are regular, as in Transformer\\nmodels. The segment embedding ( eseg ) is a new type of embedding that indicates whether a token\\nbelongs to SentA or SentB . This can be illustrated by the following example.\\nT oken [CLS] It is raining . [SEP] I need an umbrella . [SEP]\\nx x0 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 x11\\nepos PE(0) PE(1) PE(2) PE(3) PE(4) PE(5) PE(6) PE(7) PE(8) PE(9) P E(10) PE(11)\\neseg eA eA eA eA eA eA eB eB eB eB eB eB\\nThe main part of BERT models is a multi-layer Transformer net work. A Transformer layer\\nconsists of a self-attention sub-layer and an FFN sub-layer . Both of them follow the post-norm\\narchitecture: output = LNorm( F(input) + input) , where F(·) is the core function of the sub-\\nlayer (either a self-attention model or an FFN), and LNorm(·) is the layer normalization unit.\\nT ypically , a number of Transformer layers are stacked to for m a deep network. At each position of\\nthe sequence, the output representation is a real-valued ve ctor which is produced by the last layer\\nof the network.\\nThere are several aspects one may consider in developing BER T models.\\n• V ocabulary Size (|V|). In Transformers, each input token is represented as an ent ry in a\\nvocabulary V. Large vocabularies can cover more surface form variants of words, but may\\nlead to increased storage requirements.\\n• Embedding Size (de ). Every token is represented as a de-dimensional real-valued vector.\\nAs presented above, this vector is the sum of the token embedd ing, positional embedding,\\nand segment embedding, all of which are also de -dimensional real-valued vectors.\\n• Hidden Size (d). The input and output of a sub-layer are of ddimensions. Besides, most\\nof the hidden states of a sub-layer are d-dimensional vectors. In general, dcan be roughly\\nviewed as the width of the network.\\n• Number of Heads (nhead). In self-attention sub-layers, one needs to specify the nu mber\\nof heads used in multi-head self-attention. The larger this number is, the more sub-spaces\\nattention is performed. In practical systems, we often set nhead ≥ 4.\\n• FFN Hidden Size (dﬀn ). The size of the hidden layer of the FFNs used in Transformer s is\\ntypically larger than d. For example, a typical setting is dﬀn = 4 d. For larger Transformers,\\nsuch as recent large models, dﬀn may be set to a very large value.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 32, 'page_label': '26'}, page_content='26 Pre-training\\nSelf-attention\\nPositionT oken Segment\\nx0x1 ...xm\\nLayer Normalization\\nFFN\\nLayer Normalization\\nOutput Layer\\n...\\nh0h1 ...hm\\nlayers\\nInput\\nxi corresponds to an entry of V\\nEmbedding\\ne= x + epos + eseg ∈ Rde\\nSelf-attention Sub-layer\\nhidden size:d\\nnumber of heads: nhead\\nFFN Sub-layer\\nhidden size:d\\nFFN hidden size: dﬀn\\nEncoder Output\\nhi ∈ Rd is the contextual\\nrepresentation of xi\\nFig. 1.6: The model Architecture of BERT (Transformer encoder). The i nput tokens are ﬁrst represented as embed-\\ndings, each of which is the sum of the corresponding token emb edding, positional embedding and segment embedding.\\nThen, the embedding sequence is processed by a stack of Trans former layers. Each layer in this stack includes a self-\\nattention sub-layer and a FFN sub-layer. The output of the BE RT model is a sequence of vectors produced by the ﬁnal\\nTransformer layer.\\n• Model Depth (L). Using deep networks is an effective way to improve the expr essive power\\nof Transformers. For BERT models, Lis typically set to 12 or 24. However, networks with\\neven greater depth are also feasible and can be applied for fu rther enhancements.\\nDifferent settings of these hyper-parameters lead to diffe rent model sizes. There are two\\nwidely-used BERT models.\\n• BERT base: d= 768 , L= 12 , nhead = 12 , total number of parameters = 110 M.\\n• BERT large: d= 1 ,024, L= 14 , nhead = 16 , total number of parameters = 340 M.\\nTraining BERT models follows the standard training process of Transformers. Training larger\\nmodels such as BERT large requires more training effort and time. This is a common prob lem\\nfor pre-training, especially when a model is trained on a ver y large amount of data. In practice,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 33, 'page_label': '27'}, page_content='1.3 Example: BERT 27\\nthere are often considerations of training efﬁciency . For e xample, a practice is to ﬁrst train a\\nBERT model on relatively short sequences for a large number o f training steps, and then continue\\ntraining it on full-length sequences for the remaining trai ning steps.\\n1.3.2 More T raining and Larger Models\\nBERT is a milestone model in NLP , sparking many subsequent efforts to improve it. One direction\\nis to scale up the model itself, including increasing traini ng data and developing larger models.\\nRoBERT a, an extension of the standard BERT model, is an example of suc h efforts [\\nLiu et al. ,\\n2019]. It introduces two major improvements. First, simply usin g more training data and more\\ncompute can improve BERT models without need of changing the model architectures. Second,\\nremoving the NSP loss does not decrease the performance on do wnstream tasks if the training is\\nscaled up. These ﬁndings suggest exploring a general direct ion of pre-training: we can continue\\nto improve pre-training by scaling it up on simple pre-train ing tasks.\\nA second approach to improving BERT models is to increase the number of model parame-\\nters. For example, in He et al. [2021]’s work, a 1.5 billion-parameter BERT -like model is built b y\\nincreasing both the model depth and hidden size. However, sc aling up BERT and various other\\npre-trained models introduces new challenges in training, for example, training very large models\\noften becomes unstable and difﬁcult to converge. This makes the problem more complicated, and\\nrequires careful consideration of various aspects, includ ing model architecture, parallel computa-\\ntion, parameter initialization, and so on. In another examp le, Shoeybi et al. [2019] successfully\\ntrained a 3.9 billion-parameter BERT -like model, where hun dreds of GPUs were used to manage\\nthe increased computational demands.\\n1.3.3 More Efﬁcient Models\\nCompared to its predecessors, BERT is a relatively large model for the time it was proposed.\\nThis increase in model size results in larger memory require ments and a consequent slowdown in\\nsystem performance. Developing smaller and faster BERT mod els is part of the broader challenge\\nof building efﬁcient Transformers, which has been extensiv ely discussed in\\nT ay et al. [2020]’s\\nwork and Xiao and Zhu [2023]’s work. However, a deeper discussion of this general topic is\\nbeyond the scope of our current discussion. Here we instead c onsider a few efﬁcient variants of\\nBERT .\\nSeveral threads of research are of interest to NLP researche rs in developing efﬁcient BERT\\nmodels. First, work on knowledge distillation, such as trai ning student models with the output\\nof well-trained teacher models, shows that smaller BERT mod els can be obtained by transferring\\nknowledge from larger BERT models. Given that BERT models ar e multi-layer networks with\\nseveral different types of layers, knowledge distillation can be applied at different levels of repre-\\nsentation. For example, beyond distilling knowledge from t he output layers, it is also possible to\\nincorporate training loss that measures the difference in o utput of hidden layers between teacher\\nmodels and student models [ Sun et al. , 2020; Jiao et al. , 2020]. Indeed, knowledge distillation has\\nbeen one of the most widely-used techniques for learning sma ll pre-trained models.\\nSecond, conventional model compression methods can be dire ctly applied to compress BERT\\nmodels. One common approach is to use general-purpose pruni ng methods to prune the Trans-\\nformer encoding networks [ Gale et al. , 2019]. This generally involves removing entire layers'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 34, 'page_label': '28'}, page_content='28 Pre-training\\n[Fan et al. , 2019] or a certain percentage of parameters in the networks [ Sanh et al. , 2020; Chen et al. ,\\n2020]. Pruning is also applicable to multi-head attention model s. For example, Michel et al.\\n[2019] show that removing some of the heads does not signiﬁcantly d ecrease the performance\\nof BERT models, but speeds up the inference of these models. A nother approach to compress-\\ning BERT models is quantization [ Shen et al. , 2020]. By representing model parameters as low-\\nprecision numbers, the models can be greatly compressed. Wh ile this method is not speciﬁc to\\nBERT models, it proves effective for large Transformer-bas ed architectures.\\nThird, considering that BERT models are relatively deep and large networks, another thread\\nof research uses dynamic networks to adapt these models for e fﬁcient inference. An idea in this\\nparadigm is to dynamically choose the layers for processing a token, for example, in depth-\\nadaptive models we exit at some optimal depth and thus skip th e rest of the layers in the layer\\nstack [ Xin et al. , 2020; Zhou et al. , 2020]. Similarly , we can develop length-adaptive models in\\nwhich the length of the input sequence is dynamically adjust ed. For example, we can skip some of\\nthe tokens in the input sequence so that the model can reduce c omputational load on less important\\ntokens, enhancing overall efﬁciency .\\nFourth, it is also possible to share parameters across layer s to reduce the size of BERT models.\\nA simple way to do this is to share the parameters of a whole Tra nsformer layer across the layer\\nstack [ Dehghani et al. , 2018; Lan et al. , 2020]. In addition to the reduced number of parameters,\\nthis enables reuse of the same layer in a multi-layer Transfo rmer network, leading to savings of\\nmemory footprint at test time.\\n1.3.4 Multi-lingual Models\\nThe initial BERT model was primarily focused on English. Soon after this model was proposed,\\nit was extended to many languages. One simple way to do this is to develop a separate model\\nfor each language. Another approach, which has become more p opular in recent work on large\\nlanguage models, is to train multi-lingual models directly on data from all the languages. In\\nresponse, multi-lingual BERT (mBERT) models were developed by training them on text from\\n104 languages\\n6 . The primary difference from monolingual BERT models is tha t mBERT models\\nuse larger vocabularies to cover tokens from multiple langu ages. As a result, the representations\\nof tokens from different languages are mapped into the same s pace, allowing for the sharing of\\nknowledge across languages via this universal representat ion model.\\nOne important application of multi-lingual pre-trained mo dels is cross-lingual learning. In the\\ncross-lingual setting, we learn a model on tasks in one langu age, and apply it to the same tasks\\nin another language. In cross-lingual text classiﬁcation, for example, we ﬁne-tune a multi-lingual\\npre-trained model on English annotated documents. Then, we use the ﬁne-tuned model to classify\\nChinese documents.\\nAn improvement to multi-lingual pre-trained models like mB ERT is to introduce bilingual data\\ninto pre-training. Rather than training solely on monoling ual data from multiple languages, bilin-\\ngual training explicitly models the relationship between t okens in two languages. The resulting\\nmodel will have innate cross-lingual transfer abilities, a nd thus can be easily adapted to different\\nlanguages. Lample and Conneau [2019] propose an approach to pre-training cross-lingual lan-\\nguage models (XLMs). In their work, a cross-lingual language model can be train ed in either the\\ncausal language modeling or masked language modeling manne r. For masked language modeling\\n6 https://github.com/google-research/bert/'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 35, 'page_label': '29'}, page_content='1.3 Example: BERT 29\\npre-training, the model is treated as an encoder. The traini ng objective is the same as BERT: we\\nmaximize the probabilities of some randomly selected token s which are either masked, replaced\\nwith random tokens, or kept unchanged in the input. If we cons ider bilingual data in pre-training,\\nwe sample a pair of aligned sentences each time. Then, the two sentences are packed together to\\nform a single sequence used for training. For example, consi der an English-Chinese sentence pair\\n鲸鱼 是 哺乳 动物 。 ↔ Whales are mammals .\\nW e can pack them to obtain a sequence, like this\\n[CLS] 鲸鱼 是 哺乳 动物 。 [SEP] Whales are mammals . [SEP]\\nW e then select a certain percentage of the tokens and replace them with [MASK].\\n[CLS] [MASK] 是 [MASK] 动物 。 [SEP] Whales [MASK] [MASK] . [SEP]\\nThe goal of pre-training is to maximize the product of the pro babilities of the masked tokens given\\nthe above sequence. By performing training in this way , the m odel can learn to represent both the\\nEnglish and Chinese sequences, as well as to capture the corr espondences between tokens in the\\ntwo languages. For example, predicting the Chinese token 鲸鱼 may require the information\\nfrom the English token Whales. Aligning the representations of the two languages essenti ally\\ntransforms the model into a “translation” model. So this tra ining objective is also called transla-\\ntion language modeling . Figure 1.7 shows an illustration of this approach.\\nA beneﬁt of multi-lingual pre-trained models is their inher ent capability of handling code-\\nswitching. In NLP and linguistics, code-switching refers t o switching among languages in a text.\\nFor example, the following is a mixed language text containi ng both Chinese and English:\\n周末 我们 打算 去 做 hiking ， 你 想 一起 来 吗 ？\\n(W e plan to go hiking this weekend, would you like to join us?)\\nFor multi-lingual pre-trained models, we do not need to iden tify whether a token is Chinese or\\nEnglish. Instead, every token is just an entry of the shared v ocabulary . This can be imagined as\\ncreating a “new” language that encompasses all the language s we want to process.\\nThe result of multi-lingual pre-training is inﬂuenced by se veral factors. Given that the model\\narchitecture is ﬁxed, one needs to specify the size of the sha red vocabulary , the number (or per-\\ncentage) of samples in each language, the size of the model, a nd so on. Conneau et al. [2020]\\npoint out several interesting issues regarding large-scal e multi-lingual pre-training for XLM-like\\nmodels. First, as the number of supported languages increas es, a larger model is needed to handle\\nthese languages. Second, a larger shared vocabulary is help ful for modeling the increased diver-\\nsity in languages. Third, low-resource languages more easi ly beneﬁt from cross-lingual transfer\\nfrom high-resource languages, particularly when similar h igh-resource languages are involved in\\npre-training. However, interference may occur if the model is trained for an extended period,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 36, 'page_label': '30'}, page_content='30 Pre-training\\n[CLS] [MASK] 是 [MASK] 动物 。 [SEP] Whales [MASK] [MASK] . [SEP]\\n(zh) (zh) (zh) (zh) (zh) (zh) (zh) (en) (en) (en) (en) (en)\\ne0 e1 e2 e3 e4 e5 e6 e7 e8 e9 e10 e11\\nh0 h1 h2 h3 h4 h5 h6 h7 h8 h9 h10 h11\\n鲸鱼 哺乳 are mammals\\nTransformer Encoder\\nFig. 1.7: An illustration of translation language modeling. For ease of understanding, we present a simple example\\nwhere all the selected tokens are masked. The model is traine d to predict these masked tokens. As the sequence\\ncontains tokens in two languages, predicting a token in one l anguage allows access to tokens in the other language,\\nthereby enabling cross-lingual modeling. In Lample and Conneau [2019]’s work, an input embedding (i.e., ei ) is the\\nsum of the token embedding, positional embedding, and langu age embedding. This requires that each token is assigned\\nwith a language label. Thus we can distinguish tokens in diff erent languages. In multi-lingual pre-training, particul arly\\nin work using shared vocabularies, specifying the language to which a token belongs is not necessary . The use of\\nlanguage embeddings in turn makes it difﬁcult to handle code -switching. Therefore, we assume here that all token\\nrepresentations are language-independent.\\nmeaning the overall performance of the pre-trained model starts decreasing at a certain point dur-\\ning pre-training. Thus, in practical systems, one may need t o stop the pre-training early to prevent\\ninterference.\\n1.4 Applying BERT Models\\nOnce a BERT model is pre-trained, it can then be used to solve NLP problems. But BERT models\\nare not immediately ready for performing speciﬁc downstrea m tasks. In general, additional ﬁne-\\ntuning work is required to make them adapt. As a ﬁrst step, we n eed a predictor to align the\\noutput of the model with the problem of interest. Let BER Tˆθ (·) be a BERT model with pre-\\ntrained parameters ˆθ, and Predictω (·) be a prediction network with parameters ω. By integrating\\nthe prediction network with the output of the BERT model, we d evelop a model to tackle the\\ndownstream tasks. This model can be expressed as\\ny = Predict ω (BER Tˆθ (x)) (1.21)\\nwhere x is the input and y is the output that ﬁts the problem. For example, in classiﬁca tion\\nproblems, the model outputs a probability distribution ove r labels.\\nThen, we collect a set of labeled samples D, and ﬁne-tune the model by\\n( ˜ω,˜θ) = arg min\\nω, ˆθ +\\n∑\\n(x, ygold )∈D\\nLoss(yω, ˆθ + ,ygold ) (1.22)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 37, 'page_label': '31'}, page_content='1.4 Applying BERT Models 31\\nwhere (x,ygold ) represents a tuple of an input and its corresponding output. The notation of this\\nequation seems a bit complicated, but the training/tuning p rocess is standard. W e optimize the\\nmodel by minimizing the loss over the tuning samples. The out come is the optimized parameters\\n˜ωand ˜θ. The optimization starts with the pre-trained parameters ˆθ. Here we use ˆθ+ to indicate that\\nthe parameters are initialized with ˆθ, and use yω, ˆθ + to denote the model output computed using\\nthe parameters ωand ˆθ+.\\nWith the ﬁne-tuned parameters ˜ωand ˜θ, we can apply the model Predict˜ω (BER T˜θ (·)) to new\\ndata of the same tasks for which the model was ﬁne-tuned. The f orm of the downstream tasks\\ndetermines the input and output formats of the model, as well as the architecture of the prediction\\nnetwork. In the following we list some tasks to which BERT mod els are generally suited.\\n• Classiﬁcation (Single T ext). One of the most widely-used applications of B ERT models is\\ntext classiﬁcation. In this task, a BERT model receives a seq uence of tokens and encodes\\nit as a sequence of vectors. The ﬁrst output vector hcls (or h0 ) is typically used as the\\nrepresentation of the entire text. The prediction network t akes hcls as input to produce a\\ndistribution of labels. Let [CLS]x1x2...xm be an input text. See below for an illustration of\\nBERT -based text classiﬁcation.\\n[CLS] x1 x2 ... xm [SEP]\\necls e1 e2 ... em em+1\\nhcls h1 h2 ... hm hm+1\\nClass\\nBER T\\nHere the gray box denotes the prediction network. Many NLP pr oblems can be categorized\\nas text classiﬁcation tasks, and there have been several tex t classiﬁcation benchmarks for\\nevaluating pre-trained models. For example, we can classif y texts by their grammatical cor-\\nrectness (grammaticality) or emotional tone (sentiment) [ Socher et al. , 2013; W arstadt et al. ,\\n2019]. Note that the prediction network could be any classiﬁcati on model, such as a deep\\nneural network or a more traditional classiﬁcation model. T he entire model can then be\\ntrained or ﬁne-tuned in the manner of a standard classiﬁcati on model. For example, the pre-\\ndiction network can be simply a Softmax layer and the model pa rameters can be optimized\\nby maximizing the probabilities of the correct labels.\\n• Classiﬁcation (Pair of T exts). Classiﬁcation can also be performed on a pai r of texts. Sup-\\npose we have two texts, x1...xm and y1...yn. W e can concatenate these texts to form a single\\nsequence with a length len. Then, we predict a label for this combined text sequence bas ed\\non the hcls vector, as follows'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 38, 'page_label': '32'}, page_content='32 Pre-training\\n[CLS] x1 x2 ... xm [SEP] y1 y2 ... yn [SEP]\\nT ext 1 T ext 2\\necls e1 e2 ... em em+1 em+2 em+3 ... elen−1 elen\\nhcls h1 h2 ... hm hm+1 hm+2 hm+3 ... hlen−1 hlen\\nClass\\nBER T\\nwhere len = n+ m+ 2 . T ext pair classiﬁcation covers several problems, includi ng se-\\nmantic equivalence judgement (determine whether two texts are semantically equivalent)\\n[Dolan and Brockett , 2005], text entailment judgement (determine whether a hypothes is\\ncan be logically inferred or entailed from a premise) [ Bentivogli and Giampiccolo , 2011;\\nWilliams et al. , 2018], grounded commonsense inference (determine whether an ev ent is\\nlikely to happen given its context) [ Zellers et al. , 2018], and question-answering inference\\n(determine whether an answer corresponds to a given questio n).\\n• Regression. Instead of generating a label distribution, we can have the prediction network\\noutput a real-valued score. For example, by adding a Sigmoid layer to the prediction net-\\nwork, the system can be employed to compute the similarity be tween two given sentences.\\nThe architecture is the same as that of BERT -based classiﬁca tion systems, with only the\\nchange of the output layer.\\n[CLS] x1 x2 ... xm [SEP] y1 y2 ... yn [SEP]\\nT ext 1 T ext 2\\necls e1 e2 ... em em+1 em+2 em+3 ... elen−1 elen\\nhcls h1 h2 ... hm hm+1 hm+2 hm+3 ... hlen−1 hlen\\nNumber (similarity , evaluation score, etc.)\\nBER T\\nFor training or ﬁne-tuning, we can minimize the regression l oss of the model output as\\nusual.\\n• Sequence Labeling . Sequence labeling is a machine learning approach applicab le to a wide\\nrange of NLP problems. This approach assigns a label to each t oken in an input sequence,\\nand some linguistic annotations can then be derived from thi s sequence of labels. An ex-\\nample of sequence labeling in NLP is part-of-speech (POS) ta gging. W e label each word\\nin a sentence with its corresponding POS tag. Another exampl e is named entity recognition\\n(NER) in which we label each word with an NER tag, and named ent ities are identiﬁed\\nusing these tags. See below for an illustration of the model a rchitecture for NER.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 39, 'page_label': '33'}, page_content='1.4 Applying BERT Models 33\\n[CLS] x1 x2 ... xm [SEP]\\necls e1 e2 ... em em+1\\nhcls h1 h2 ... hm hm+1\\n{B, I, O}{B, I, O} { B, I, O}\\nT ag T ag T ag\\nBER T\\nHere {B,I,O} is the tag set of NER. For example, B-ORG means the beginning of an\\norganization, I-ORG means the word is inside an organization, and O means the word does\\nnot belong to any named entity . This NER model can output a dis tribution over the tag set\\nat each position, denoted as pi. The training or ﬁne-tuning of the model can be performed\\nover these distributions {p1 ,...,pm }. For example, suppose pi(tagi ) is the probability of\\nthe correct tag at position i. The training loss can be deﬁned to be the negative likelihoo d\\nLoss = − 1\\nm\\nm∑\\ni=1\\nlog pi(tagi) (1.23)\\nFinding the best label sequence given a trained NER model is a well-studied issue in NLP .\\nThis is often achieved via dynamic programming, which, in th e context of path ﬁnding over\\na lattice, has linear complexity [ Huang, 2009].\\n• Span Prediction . Some NLP tasks require predicting a span in a text. A common e xample\\nis reading comprehension. In this task, we are given a query x1...xm and a context text\\ny1...yn. The goal is to identify a continuous span in y1...yn that best answers the query .\\nThis problem can be framed as a sequence labeling-like task i n which we predict a label for\\neach yj to indicate the beginning or ending of the span. Following Seo et al. [2017], we add\\ntwo networks on top of the BERT output for yj : one for generating the probability of yj\\nbeing the beginning of the span (denoted by pbeg\\nj), and one for generating the probability\\nof yj being the ending of the span (denoted by pend\\nj). The resulting model architecture is\\nshown as follows\\n[CLS] x1 x2 ... xm [SEP] y1 y2 ... yn [SEP]\\nQuery Context T ext\\necls e1 e2 ... em em+1 em+2 em+3 ... elen−1 elen\\nhcls h1 h2 ... hm hm+1 hm+2 hm+3 ... hlen−1 hlen\\n(pbeg\\n1)\\nBeg\\n(pend\\n1)\\nEnd\\n(pbeg\\n2)\\nBeg\\n(pend\\n2)\\nEnd\\n(pbeg\\nn)\\nBeg\\n(pend\\nn)\\nEnd\\nBER T'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 40, 'page_label': '34'}, page_content='34 Pre-training\\nW e pack the query and context text together to obtain the inpu t sequence. The prediction\\nnetworks are only applied to outputs for the context text, ge nerating the probabilities pbeg\\nj\\nand pend\\njat each position. The loss can be computed by summing the log l ikelihoods of the\\ntwo models across the entire context text.\\nLoss = − 1\\nn\\nn∑\\nj=1\\n(\\nlog pbeg\\nj+ log pend\\nj\\n)\\n(1.24)\\nAt test time, we search for the best span by\\n(ˆj1 ,ˆj2 ) = arg max\\n1≤j1 ≤j2 ≤n\\n(\\nlog pbeg\\nj1 + log pend\\nj2\\n)\\n(1.25)\\n• Encoding for Encoder-decoder Models . While our focus in this section has been primarily\\non language understanding problems, it is worth noting that BERT models can be applied\\nto a broader range of NLP tasks. In fact, BERT models can be use d in all the scenarios\\nwhere we need to encode a piece of text. One application that w e have not mentioned is\\ntext generation which includes a range of tasks such as machi ne translation, summarization,\\nquestion answering, and dialogue generation. These tasks c an be formulated as sequence-\\nto-sequence problems: we use an encoder to represent the sou rce text, and a decoder to\\ngenerate the corresponding target text. A straightforward method to apply BERT models\\nis to consider them as encoders. Before ﬁne-tuning, we can in itialize the parameters of the\\nencoder with those from a pre-trained BERT model. Then, the e ncoder-decoder model can\\nbe ﬁne-tuned on pairs of texts as usual. The following shows t he architecture of a neural\\nmachine translation system where a BERT model is applied on t he source side.\\n[CLS] x1 ... xm [SEP]\\nSource T ext\\nex\\ncls ex\\n1 ... ex\\nm ex\\nm+1\\nBER T (Encoder)\\nAdapter\\n⟨ s⟩ y1 y2 ... yn−1\\ney\\n0 ey\\n1 ey\\n2 ... ey\\nn−1\\nDecoder\\ny1 y2 y3 ... yn\\nT arget T ext\\nHere x1...xm denotes the source sequence, y1...yn denotes the target sequence, ex\\n1...ex\\nm\\ndenotes the embedding sequence of x1 ...xm, and ey\\n1...ey\\nndenotes the embedding sequence\\nof y1...yn. The adapter, which is optional, maps the output of the BERT m odel to the form\\nthat is better suited to the decoder.\\nFine-tuning BERT models is a complicated engineering probl em, inﬂuenced by many factors,\\nsuch as the amount of ﬁne-tuning data, the model size, and the optimizer used in ﬁne-tuning.\\nIn general, we wish to ﬁne-tune these models sufﬁciently so t hat they can perform well in the\\ndownstream tasks. However, ﬁne-tuning BERT models for spec iﬁc tasks may lead to overﬁtting,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 41, 'page_label': '35'}, page_content='1.5 Summary 35\\nwhich in turn reduces their ability to generalize to other ta sks. For example, suppose we have a\\nBERT model that performs well on a particular task. If we then ﬁne-tune it for new tasks, this\\nmay decrease its performance on the original task. This prob lem is related to the catastrophic\\nforgetting problem in continual training, where a neural network forge ts previously learned in-\\nformation when updated on new samples. In practical applica tions, a common way to alleviate\\ncatastrophic forgetting is to add some old data into ﬁne-tun ing and train the model with more\\ndiverse data. Also, one may use methods specialized to catas trophic forgetting, such as experi-\\nence replay [ Rolnick et al. , 2019] and elastic weight consolidation [ Kirkpatrick et al. , 2017]. The\\ninterested reader can refer to some surveys for more detaile d discussions of this issue in continual\\nlearning [ Parisi et al. , 2019; W ang et al. , 2023a;e].\\n1.5 Summary\\nIn this chapter we have discussed the general idea of pre-training in NLP . In particular, we have dis-\\ncussed self-supervised pre-training and its application t o encode-only , decoder-only , and encoder-\\ndecoder architectures. Moreover, we have presented and com pared a variety of pre-training tasks\\nfor these architectures. As an example, BERT is used to illus trate how sequence models are pre-\\ntrained via masked language modeling and applied to differe nt downstream tasks.\\nRecent years have shown remarkable progress in NLP , led by th e large-scale use of self-\\nsupervised pre-training. And sweeping advances are being m ade across many tasks, not only\\nin NLP but also in computer vision and other areas of AI. One id ea behind these advances is that a\\nsigniﬁcant amount of knowledge about the world can be learne d by simply training these AI sys-\\ntems on huge amounts of unlabeled data. For example, a langua ge model can learn some general\\nknowledge of a language by repeatedly predicting masked wor ds in large-scale text. As a result,\\nthis pre-trained language model can serve as a foundation mo del, which can be easily adapted to\\naddress speciﬁc downstream NLP tasks. This paradigm shift i n NLP has enabled the development\\nof incredibly powerful systems for language understanding , generation, and reasoning [\\nManning,\\n2022]. However, it is important to recognize that we are still in t he early stages of creating truly in-\\ntelligent systems, and there is a long way to go. Nevertheles s, large-scale pre-training has opened\\na door to intelligent systems that researchers have long asp ired to develop, though several key re-\\nsearch areas remain open for exploration, such as learning i ntelligence efﬁciently using reasonably\\nsmall-sized data and acquiring complex reasoning and plann ing abilities.\\nNote that this chapter is mostly introductory and cannot cov er all aspects of pre-training. For\\nexample, there are many methods to ﬁne-tune a pre-trained mo del, offering different ways to better\\nadapt the model to diverse situations. Moreover, large lang uage models, which are considered one\\nof the most signiﬁcant achievements in AI in recent years, ar e skipped in this section. W e leave\\nthe discussion of these topics to the following chapters.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 42, 'page_label': '36'}, page_content='CH A P TER 2\\nGenerative Models\\nOne of the most signiﬁcant advances in NLP in recent years mig ht be the development of large\\nlanguage models (LLMs). This has helped create systems that can understand and generate nat-\\nural languages like humans. These systems have even been fou nd to be able to reason, which\\nis considered a very challenging AI problem. With these achi evements, NLP made big strides\\nand entered a new era of research in which difﬁcult problems a re being solved, such as building\\nconversational systems that can communicate with humans sm oothly .\\nThe concept of language modeling or probabilistic language modeling dates back to early ex-\\nperiments conducted by Shannon [1951]. In his work, a language model was designed to estimate\\nthe predictability of English — how well can the next letter of a text be predicted when the pre -\\nceding N letters are known . Although Shannon’s experiments were preliminary , the fun damental\\ngoals and methods of language modeling have remained largel y unchanged over the decades since\\nthen. For quite a long period, particularly before 2010, the dominant approach to language mod-\\neling was the n-gram approach [ Jurafsky and Martin , 2008]. In n-gram language modeling, we\\nestimate the probability of a word given its preceding n− 1 words, and thus the probability of a\\nsequence can be approximated by the product of a series of n-gram probabilities. These proba-\\nbilities are typically estimated by collecting smoothed re lative counts of n-grams in text. While\\nsuch an approach is straightforward and simple, it has been e xtensively used in NLP . For example,\\nthe success of modern statistical speech recognition and ma chine translation systems has largely\\ndepended on the utilization of n-gram language models [ Jelinek, 1998; Koehn, 2010].\\nApplying neural networks to language modeling has long been attractive, but a real break-\\nthrough appeared as deep learning techniques advanced. A wi dely cited study is Bengio et al.\\n[2003]’s work where n-gram probabilities are modeled via a feed-forward network and learned\\nby training the network in an end-to-end fashion. A by-produ ct of this neural language model\\nis the distributed representations of words, known as word e mbeddings. Rather than represent-\\ning words as discrete variables, word embeddings map words i nto low-dimensional real-valued\\nvectors, making it possible to compute the meanings of words and word n-grams in a continu-\\nous representation space. As a result, language models are n o longer burdened with the curse of\\ndimensionality , but can represent exponentially many n-grams via a compact and dense neural\\nmodel.\\nThe idea of learning word representations through neural la nguage models inspired subsequent\\nresearch in representation learning in NLP . However, this a pproach did not attract signiﬁcant in-\\nterest in developing NLP systems in the ﬁrst few years after i ts proposal. Starting in about 2012,\\nthough, advances were made in learning word embeddings from large-scale text via simple word\\nprediction tasks. Several methods, such as W ord2V ec, were p roposed to effectively learn such\\nembeddings, which were then successfully applied in a varie ty of NLP systems [ Mikolov et al. ,\\n2013a;b]. As a result of these advances, researchers began to think o f learning representations of\\nsequences using more powerful language models, such as LSTM -based models [ Sutskever et al. ,\\n2014; Peters et al. , 2018]. And further progress and interest in sequence representa tion exploded\\nafter Transformer was proposed. Alongside the rise of Trans former, the concept of language mod-\\neling was generalized to encompass models that learn to pred ict words in various ways. Many\\n36'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 43, 'page_label': '37'}, page_content='2.1 A Brief Introduction to LLMs 37\\npowerful Transformer-based models were pre-trained using these word prediction tasks, and suc-\\ncessfully applied to a variety of downstream tasks [ Devlin et al. , 2019].\\nIndeed, training language models on large-scale data has le d NLP research to exciting times.\\nWhile language modeling has long been seen as a foundational technique with no direct link to\\nthe goals of artiﬁcial intelligence that researchers had ho ped for, it helps us see the emergence of\\nintelligent systems that can learn a certain degree of gener al knowledge from repeatedly predicting\\nwords in text. Recent research demonstrates that a single, w ell-trained LLM can handle a large\\nnumber of tasks and generalize to perform new tasks with a sma ll adaptation effort [ Bubeck et al. ,\\n2023]. This suggests a step towards more advanced forms of artiﬁc ial intelligence, and inspires\\nfurther exploration into developing more powerful languag e models as foundation models.\\nIn this chapter, we consider the basic concepts of generativ e LLMs. For simplicity , we use the\\nterms large language models or LLMs to refer to generative models like GPT , though this term\\ncan broadly cover other types of models like BERT . W e begin by giving a general introduction\\nto LLMs, including the key steps of building such models. W e t hen discuss two scaling issues of\\nLLMs: how LLMs are trained at scale, and how LLMs can be improv ed to handle very long texts.\\nFinally , we give a summary of these discussions.\\n2.1 A Brief Introduction to LLMs\\nIn this section we give an introduction to the basic ideas of LLMs as required for the rest of this\\nchapter and the following chapters. W e will use terms word and token interchangeably . Both\\nof them refer to the basic units used in language modeling, th ough their original meanings are\\ndifferent.\\nBefore presenting details, let us ﬁrst consider how languag e models work. The goal of lan-\\nguage modeling is to predict the probability of a sequence of tokens occurring. Let {x0,x1 ,...,xm }\\nbe a sequence of tokens, where x0 is the start symbol ⟨ s⟩ (or ⟨ SOS⟩ )\\n1. The probability of this se-\\nquence can be deﬁned using the chain rule\\nPr(x0,...,xm ) = Pr( x0 ) · Pr(x1 |x0 ) · Pr(x2 |x0,x1 ) · · · Pr(xm|x0 ,...,xm−1 )\\n=\\nm∏\\ni=0\\nPr(xi |x0,...,xi−1 ) (2.1)\\nor alternatively in a logarithmic form\\nlog Pr( x0 ,...,xm ) =\\nm∑\\ni=0\\nlog Pr( xi |x0,...,xi−1 ) (2.2)\\nHere Pr(xi|x0 ,...,xi−1 ) is the probability of the token xi given all its previous tokens {x0 ,...,xi−1 }\\n2. In the era of deep learning, a typical approach to language m odeling is to estimate this\\n1 The start symbol can also be [CLS] following BERT models.\\n2 W e assume that when i = 0 , Pr(xi|x0 ,...,x i−1 ) = Pr( x0 ) = 1 . Hence Pr(x0 ,...,x m ) =\\nPr(x0 ) Pr( x1 ,...,x m |x0 ) = Pr( x1 ,...,x m |x0 ).'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 44, 'page_label': '38'}, page_content='38 Generative Models\\nContext Predict Decision Rule Sequence Probability\\n⟨ s⟩ a b arg max x2 ∈V Pr(x2|⟨ s⟩ a) Pr(⟨ s⟩ ) · Pr(a|⟨ s⟩ )· Pr(b|⟨ s⟩ a)\\n⟨ s⟩ a b c arg max x3 ∈V Pr(x3|⟨ s⟩ ab) Pr(⟨ s⟩ ) · Pr(a|⟨ s⟩ ) · Pr(b|⟨ s⟩ a)·\\nPr(c|⟨ s⟩ ab)\\n⟨ s⟩ a b c d arg max x4 ∈V Pr(x4|⟨ s⟩ abc ) Pr(⟨ s⟩ ) · Pr(a|⟨ s⟩ ) · Pr(b|⟨ s⟩ a)·\\nPr(c|⟨ s⟩ ab)· Pr(d|⟨ s⟩ abc )\\nT able 2.1: Illustration of generating the three tokens b c dgiven the preﬁx ⟨ s⟩ avia a language model. In each step,\\nthe model picks a token xi from V so that Pr(xi |x0 ,...,x i−1 ) is maximized. This token is then appended to the end\\nof the context sequence. In the next step, we repeat the same p rocess, but based on the new context.\\nprobability using a deep neural network. Neural networks tr ained to accomplish this task re-\\nceive a sequence of tokens x0,...,xi−1 and produce a distribution over the vocabulary V (de-\\nnoted by Pr(·|x0 ,...,xi−1 )). The probability Pr(xi |x0,...,xi−1 ) is the value of the i-th entry of\\nPr(·|x0 ,...,xi−1 ).\\nWhen applying a trained language model, a common task is to ﬁn d the most likely token given\\nits previous context tokens. This token prediction task can be described as\\nˆxi = arg max\\nxi ∈V\\nPr(xi |x0,...,xi−1 ) (2.3)\\nW e can perform word prediction multiple times to generate a c ontinuous text: each time we\\npredict the best token ˆxi, and then add this predicted token to the context for predict ing the next\\ntoken ˆxi+1. This results in a left-to-right generation process implem enting Eqs. ( 2.1) and ( 2.2). T o\\nillustrate, consider the generation of the following three words given the preﬁx ‘ ⟨ s⟩ a’, as shown\\nin T able 2.1. Now we discuss how LLMs are constructed, trained, and appli ed.\\n2.1.1 Decoder-only T ransformers\\nAs is standard practice, the input of a language model is a sequence of tokens (denoted by\\n{x0,...,xm−1 }). For each step, an output token is generated, shifting the s equence one po-\\nsition forward for the next prediction. T o do this, the langu age model outputs a distribution\\nPr(·|x0 ,...,xi−1 ) at each position i, and the token xi is selected according to this distribution.\\nThis model is trained by maximizing the log likelihood ∑ m\\ni=1 log Pr( xi |x0 ,...,xi−1 )\\n3 .\\nHere, we focus on the decoder-only Transformer architectur e, as it is one of the most popular\\nmodel architectures used in LLMs. The input sequence of toke ns is represented by a sequence\\nof de -dimensional vectors {e0 ,...,em−1 }. ei is the sum of the token embedding of xi and the\\npositional embedding of i. The major body of the model is a stack of Transformer blocks ( or\\nlayers). Each Transformer block has two stacked sub-layers , one for self-attention modeling and\\none for FFN modeling. These sub-layers can be deﬁned using th e post-norm architecture\\noutput = LNorm( F(input) + input) (2.4)\\n3 Note that\\n∑ m\\ni=1 log Pr( xi|x0 ,...,x i−1 ) =\\n∑ m\\ni=0 log Pr( xi|x0 ,...,x i−1 ) since log Pr( x0 ) = 0 .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 45, 'page_label': '39'}, page_content='2.1 A Brief Introduction to LLMs 39\\nor the pre-norm architecture\\noutput = LNorm( F(input)) + input (2.5)\\nwhere input and output denote the input and output, both being an m× dmatrix. The i-th rows\\nof input and output can be seen as contextual representations of the i-th token in the sequence.\\nF(·) is the core function of a sub-layer. For FFN sub-layers, F(·) is a multi-layer FFN. For\\nself-attention sub-layers, F(·) is a multi-head self-attention function. In general, self- attention is\\nexpressed in a form of QKV attention\\nAttqkv (Q,K,V) = Softmax( QKT\\n√\\nd\\n+ Mask)V (2.6)\\nwhere Q, K and V ∈ Rm×d are the queries, keys, and values, respectively . It is impor tant to\\nnote that only previous tokens are considered when predicti ng a token. So a masking variable\\nMask ∈ Rm×m is incorporated into self-attention to achieve this. The en try (i,k) of Mask has\\na value of 0 if i≤ k, and a value of − inf otherwise.\\nGiven a representation H ∈ Rm×d, the multi-head self-attention function can be deﬁned as\\nF(H) = Merge(head 1 ,...,headτ )Whead (2.7)\\nwhere Merge(·) representees a concatenation of its inputs, and Whead ∈ Rd×d represents a pa-\\nrameter matrix. headj is the output of QKV attention on a sub-space of representati on\\nheadj = Att qkv(Q[j],K[j],V[j]) (2.8)\\nQ[j],K[j],and V[j] are the queries, keys, and values projected onto the j-th sub-space via linear\\ntransformations\\nQ[j] = HWq\\nj (2.9)\\nK[j] = HWk\\nj (2.10)\\nV[j] = HWv\\nj (2.11)\\nwhere Wq\\nj, Wk\\nj, and Wv\\nj∈ Rd× d\\nτ are the parameter matrices of the transformations.\\nSuppose we have LTransformer blocks. A Softmax layer is built on top of the out put of the\\nlast block. The Softmax layer outputs a sequence of mdistributions over the vocabulary , like this\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nPr(·|x0 ,...,xm−1 )\\n.\\n.\\n.\\nPr(·|x0 ,x1 )\\nPr(·|x0 )\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n= Softmax( HL Wo) (2.12)\\nwhere HL is the output of the last Transformer block, and Wo ∈ Rd×|V | is the parameter matrix.\\nFigure\\n2.1 shows the Transformer architecture for language modeling. Applying this language'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 46, 'page_label': '40'}, page_content='40 Generative Models\\nx0 x1 ... xm−1\\ne0 e1 ... em−1\\nhL\\n0 hL\\n1 ... hL\\nm−1\\n...\\nPr(x1 |x0 )\\nPr(x2 |x0 x1 )\\nPr(xm |x0 x1 ...x m−1 )\\nx1 x2 ... xm\\nLanguage Model\\nz0 z1 ... zm−1\\nPost-norm or Pre-norm\\nPost-norm or Pre-norm\\nSelf-attention\\nFFN\\nLBlocks\\nFig. 2.1: The Transformer-decoder architecture for language modeli ng. The central components are Lstacked Trans-\\nformer blocks, each comprising a self-attention sub-layer and an FFN sub-layer. T o prevent the model from accessing\\nthe right-context, a masking variable is incorporated into self-attention. The output layer uses a Softmax function to\\ngenerate a probability distribution for the next token, giv en the sequence of previous tokens. During inference, the\\nmodel takes the previously predicted token to predict the ne xt one, repeating this process until the end of the sequence\\nis reached. {z0 ,..., zm−1 } denote the inputs of a Transformer block, and {hL\\n0,..., hL\\nm−1 } denote the outputs of the\\nlast Transformer block.\\nmodel follows an autoregressive process. Each time the language model takes a token xi−1 as\\ninput and predicts a token xi that maximizes the probability Pr(xi|x0 ,...,xi−1 ). It is important\\nto note that, despite different implementation details, ma ny LLMs share the same architecture\\ndescribed above. These models are called large because both their depth and width are signiﬁcant.\\nT able\\n2.2 shows the model sizes for a few LLMs, as well as their model set ups.\\n2.1.2 T raining LLMs\\nNow suppose that we are given a training setD comprising K sequences. The log-likelihood of\\neach sequence x = x0...xm in D can be calculated using a language model\\nLθ (x) =\\nm∑\\ni=1\\nlog Pr θ (xi |x0,...,xi−1 ) (2.13)\\nHere the subscript θafﬁxed to L(·) and Pr(·) denotes the parameters of the language model. Then,\\nthe objective of maximum likelihood training is deﬁned as\\nˆθ = arg max\\nθ\\n∑\\nx∈D\\nLθ (x) (2.14)\\nTraining Transformer-based language models with the above objective is commonly viewed\\nas a standard optimization process for neural networks. Thi s can be achieved using gradient de-\\nscent algorithms, which are widely supported by off-the-sh elf deep learning toolkits. Somewhat'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 47, 'page_label': '41'}, page_content='2.1 A Brief Introduction to LLMs 41\\nLLM # of Parameters Depth L Width d # of Heads\\n(Q/KV)\\nGPT -1 [Radford et al. , 2018] 0.117B 12 768 12/12\\nGPT -2 [Radford et al. , 2019] 1.5B 48 1,600 25/25\\nGPT -3 [Brown et al. , 2020] 175B 96 12,288 96/96\\nLLaMA2 [ T ouvron et al. , 2023b]\\n7B 32 4,096 32/32\\n13B 40 5,120 40/40\\n70B 80 8,192 64/64\\nLLaMA3/3.1 [ Dubey et al. , 2024]\\n8B 32 4,096 32/8\\n70B 80 8,192 64/8\\n405B 126 16,384 128/8\\nGemma2 [ T eam et al. , 2024]\\n2B 26 2,304 8/4\\n9B 42 3,584 16/8\\n37B 46 4,608 32/16\\nQwen2.5 [ Y ang et al. , 2024]\\n0.5B 24 896 14/2\\n7B 28 3,584 28/4\\n72B 80 8,192 64/8\\nDeepSeek-V3 [ Liu et al. , 2024a] 671B 61 7,168 128/128\\nFalcon [ Penedo et al. , 2023]\\n7B 32 4,544 71/71\\n40B 60 8,192 128/128\\n180B 80 14,848 232/232\\nMistral [ Jiang et al. , 2023a] 7B 32 4,096 32/32\\nT able 2.2: Comparison of some LLMs in terms of model size, model depth, m odel width, and number of heads ( a/b\\nmeans aheads for queries and bheads for both keys and values).\\nsurprisingly , better results were continuously yielded as language models were evolved into more\\ncomputationally intensive models and trained on larger dat asets [ Kaplan et al. , 2020]. These suc-\\ncesses have led NLP researchers to continue increasing both the training data and model size in\\norder to build more powerful language models.\\nHowever, as language models become larger, we confront new t raining challenges, which\\nsigniﬁcantly change the problem compared to training relat ively small models. One of these\\nchallenges arises from the need for large-scale distribute d systems to manage the data, model\\nparameters, training routines, and so on. Developing and ma intaining such systems requires a\\nsigniﬁcant amount of work in both software and hardware engi neering, as well as expertise in deep\\nlearning. A related issue is that when the training is scaled up, we need more computing resources\\nto ensure the training process can be completed in an accepta ble time. For example, it generally\\nrequires hundreds or thousands of GPUs to train an LLM with te ns of billions of parameters\\nfrom scratch. This requirement drastically increases the c ost of training such models, especially\\nconsidering that many training runs are needed as these mode ls are developed. Also, from the\\nperspective of deep learning, the training process can beco me unstable if the neural networks are\\nvery deep and/or the model size is very large. In response, we typically need to modify the model\\narchitecture to adapt LLMs to large-scale training. In Sect ion 2.2 we will present more discussions\\non these issues.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 48, 'page_label': '42'}, page_content='42 Generative Models\\n2.1.3 Fine-tuning LLMs\\nOnce we have pre-trained an LLM, we can then apply it to perform various NLP tasks. Tradi-\\ntionally language models are used as components of other sys tems, for example, they are widely\\napplied to score translations in statistical machine trans lation systems. By contrast, in generative\\nAI, LLMs are considered complete systems and are employed to address NLP problems by mak-\\ning use of their generation nature. A common approach is to de scribe the task we want to address\\nin text and then prompt LLMs to generate text based on this des cription. This is a standard text\\ngeneration task where we continue or complete the text start ing from a given context.\\nMore formally , let x = x0...xm denote a token sequence of context given by users, and\\ny = y1...yn denote a token sequence following the context. Then, the inf erence of LLMs can be\\ndeﬁned as a problem of ﬁnding the most likely sequence y based on x:\\nˆy = arg max\\ny\\nlog Pr( y|x)\\n= arg max\\ny\\nn∑\\ni=1\\nlog Pr( yi|x0 ,...,xm ,y1,...,yi−1 ) (2.15)\\nHere ∑ n\\ni=1 log Pr( yi|x0 ,...,xm ,y1 ,...,yi−1 ) essentially expresses the same thing as the right-\\nhand side of Eq. (\\n2.2). It models the log probability of predicting tokens from po sition m+ 1 ,\\nrather than position 0. Throughout this chapter and subsequent ones, we will emplo y separate\\nvariables x and y to distinguish the input and output of an LLM, though they can be seen as sub-\\nsequences from the same sequence. By adopting such notation , we see that the form of the above\\nequation closely resembles those used in other text generat ion models in NLP , such as neural\\nmachine translation models.\\nT o illustrate how LLMs are applied, consider the problem of d etermining the grammaticality\\nfor a given sentence. W e can deﬁne a template like this\\n{*sentence*}\\nQuestion: Is this sentence grammatically correct?\\nAnswer:\\nHere represents the text we intend to generate. {*sentence*} is a placeholder variable that\\nwill be replaced by the actual sentence provided by the users . For example, suppose we have a\\nsentence “ John seems happy today. ”. W e can replace the {*sentence*} in the template with this\\nsentence to have an input to the language model\\nJohn seems happy today .\\nQuestion: Is this sentence grammatically correct?\\nAnswer:\\nT o perform the task, the language model is given the context x =“John seems happy today . \\\\n\\nQuestion : Is this sentence grammatically correct? \\\\n Answer :” 4 . It then generates the following\\n4 \\\\n is a special character used for line breaks.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 49, 'page_label': '43'}, page_content='2.1 A Brief Introduction to LLMs 43\\ntext as the answer, based on the context. For example, the lan guage model may output “Y es” (i.e.,\\ny = “Y es” ) if this text is the one with the maximum probability of predi ction given this context.\\nLikewise, we can deﬁne more templates to address other tasks . For example, we can translate\\nan English sentence into Chinese using the following templa te\\n{*sentence*}\\nQuestion: What is the Chinese translation of this English sentence?\\nAnswer:\\nor using an instruction-like template\\n{*sentence*}\\nTranslate this sentence from English into Chinese.\\nor using a code-like template.\\n[src-lang] = English [tgt-lang] = Chinese [input] = {*sente nce*}\\n[output] =\\nThe above templates provide a simple but effective method to “prompt” a single LLM to per-\\nform various tasks without adapting the structure of the mod el. However, this approach requires\\nthat the LLM can recognize and follow the instructions or que stions. One way to do this is to incor-\\nporate training samples with instructions and their corres ponding responses into the pre-training\\ndataset. While this method is straightforward, building an d training LLMs from scratch is com-\\nputationally expensive. Moreover, making instruction-fo llowing data effective for pre-training\\nrequires a signiﬁcant amount of such data, but collecting la rge-scale labeled data for all tasks of\\ninterest is very difﬁcult.\\nA second method, which has been a de facto standard in recent r esearch, is to adapt LLMs\\nvia ﬁne-tuning. As such, the token prediction ability learn ed in the pre-training phase can be\\ngeneralized to accomplish new tasks. The idea behind ﬁne-tu ning is that some general knowledge\\nof language has been acquired in pre-training, but we need a m echanism to activate this knowledge\\nfor applying it to new tasks. T o achieve this, we can slightly ﬁne-tune the model parameters using\\ninstruction-following data. This approach is called instruction ﬁne-tuning .\\nAn instruction ﬁne-tuning sample, which is represented by a sequence of tokens, can be seen\\nas a tuple consisting of an input and the desired output. Here , the input includes instructions,\\nsystem information (or system preﬁx), and any other user-pr ovided information 5 . T o illustrate,\\nconsider the following examples (blue text = input and under lined text = output).\\n5 System information refers to a sequence of tokens added at th e beginning of an input in order to guide the behavior\\nof an LLM, such as, you are a helpful assistant and should not output toxic conte nt.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 50, 'page_label': '44'}, page_content='44 Generative Models\\nRead the following sentence, and answer whether it is gramma tically correct.\\nLLMs are powerful models but are expensive to build.\\nY es\\nLLMs are powerful models but are expensive to build.\\nDoes this sentence make sense grammatically?\\nAnswer Option\\n- Y es\\n- No\\nY es\\nY es or No? Y es = the following sentence is grammatically corr ect. No = it con-\\ntains grammatical errors.\\nLLMs are powerful models but are expensive to build.\\nY es\\nAll these samples describe the same binary classiﬁcation ta sk, but with different instructions.\\nT o increase the diversity in the ﬁne-tuning data and make LLM s generalize, we can deﬁne more\\ntasks using instructions.\\nWrite a short story about three characters.\\nCharacter 1: A boy whose name is Andy .\\nCharacter 2: Andy’s dog.\\nCharacter 3: Andy’s friend, Jane.\\nIn the heart of a bustling city , Andy , his loyal dog Rocket, and his friend\\nJane discovered a hidden garden beneath the glow of twilight. Surrounded by\\nancient trees and whispering winds, they shared dreams and secrets, forging an\\nunbreakable bond. As the night deepened, they made a pact under the stars to\\nalways ﬁnd magic in the mundane. T ogether, they realized that true adventure lay\\nin the moments spent with friends, and every day was a treasure waiting to be\\ndiscovered.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 51, 'page_label': '45'}, page_content='2.1 A Brief Introduction to LLMs 45\\nShow me a recipe for making ice cream.\\nIngredients:\\n2 cups heavy cream; 1 cup milk; 3/4 cup sugar; 2 tsp vanilla ext ract;\\nPinch of salt.\\nSteps:\\n1. Whisk all ingredients until sugar dissolves.\\n2. Chill mixture for 2 hours in the fridge.\\n3. Freeze in a shallow dish, stirring every 30 minutes, 3-4 ti mes.\\n4. Freeze solid for 2 hours.\\n5. Serve your homemade ice cream.\\nIf you buy 5 apples and each apple costs $1.20, how much do you s pend in total?\\n$6.00\\nWrite a Python program to calculate the sum of squares of the f ollowing numbers.\\n1 , 2 , 10 , -9 , 78\\nnumbers = [1,2,10,-9 ,78]\\nsum_of_squares = sum(x**2 for x in numbers)\\nprint(sum_of_squares)\\nT o acquire instruction-following abilities, a certain amo unt of ﬁne-tuning data is required. This\\ndata may include diverse instructions and possible respons es. It has been found that scaling the\\nnumber of ﬁne-tuning tasks is beneﬁcial for improving the pe rformance of LLMs [ Chung et al. ,\\n2022]. Note that although more ﬁne-tuning data is favorable, the amount of this data is generally\\norders of magnitude smaller than that of the pre-training da ta. For example, LLMs can be ﬁne-\\ntuned with tens or hundreds of thousands of samples, or even f ewer if these samples are of high\\nquality [ Zhou et al. , 2023a; Chen et al. , 2023b], whereas pre-training such models may require\\nbillions or trillions of tokens, resulting in signiﬁcantly larger computational demands and longer\\ntraining times [ T ouvron et al. , 2023a].\\nIt is also worth noting that we should not expect the ﬁne-tuni ng data to cover all the down-\\nstream tasks to which we intend to apply LLMs. A common unders tanding of how the pre-training\\n+ ﬁne-tuning approach works is that LLMs have gained knowled ge for understanding instructions\\nand generating responses in the pre-training phase. Howeve r, these abilities are not fully activated\\nuntil we introduce some form of supervision. The general ins truction-following behavior emerges\\nas we ﬁne-tune the models with a relatively small amount of la beled data. As a result, we can\\nachieve some level of zero-shot learning : the ﬁne-tuned models can handle new tasks that they\\nhave not been explicitly trained or ﬁne-tuned for [ Sanh et al. , 2022; W ei et al. , 2022a]. This zero-\\nshot learning ability distinguishes generative LLMs from e arlier pre-trained models like BERT ,\\nwhich are primarily ﬁne-tuned for speciﬁc tasks.\\nOnce we have prepared a collection of instruction-describe d data, the ﬁne-tuning process is\\nrelatively simple. This process can be viewed as a standard t raining process as pre-training, but on\\na much smaller training dataset. Let Dtune be the ﬁne-tuning dataset and ˆθbe the model parameters'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 52, 'page_label': '46'}, page_content='46 Generative Models\\noptimized via pre-training. W e can modify Eq. ( 2.14) to obtain the objective of ﬁne-tuning\\n˜θ = arg max\\nˆθ +\\n∑\\nsample∈Dtune\\nLˆθ + (sample) (2.16)\\nHere ˜θdenotes the optimal parameters. The use of notation ˆθ+ means that the ﬁne-tuning starts\\nwith the pre-trained parameters ˆθ.\\nFor each sample ∈ D tune, we divide it into an input segment xsample and an output segment\\nysample, that is,\\nsample = [ ysample ,xsample ] (2.17)\\nW e then deﬁne the loss function to be\\nLˆθ + (sample) = − log Pr ˆθ + (ysample |xsample) (2.18)\\nIn other words, we compute the loss over the sub-sequence ysample, rather than the entire sequence.\\nIn a practical implementation of back-propagation for this equation, the sequence [ysample ,xsample ]\\nis constructed in the forward pass as usual. However, in the b ackward pass, error gradients are\\npropagated back only through the parts of the network that co rrespond to ysample , leaving the rest\\nof the network unchanged. As an example, consider a sequence\\n⟨ s⟩ Square this number . 2 .\\ued19 \\ued18\\ued17 \\ued1a\\nContext (Input)\\nThe result is 4 .\\ued19 \\ued18\\ued17 \\ued1a\\nPrediction (Output)\\nThe loss is calculated and back propagated only for The resul t is 4 ..\\nInstruction ﬁne-tuning also requires substantial enginee ring work. In order to achieve satis-\\nfactory results, one may experiment with different setting s of the learning rate, batch size, number\\nof ﬁne-tuning steps, and so on. This typically requires many ﬁne-tuning runs and evaluations. The\\ncost and experimental effort of ﬁne-tuning remain critical and should not be overlooked, though\\nthey are much lower than those of the pre-training phase.\\nWhile we focus on instruction ﬁne-tuning for an illustrativ e example here, ﬁne-tuning tech-\\nniques play an important role in developing various LLMs and are more widely used. Examples\\ninclude ﬁne-tuning LLMs as chatbots using dialog data, and a dapting these models to handle very\\nlong sequences. The wide application of ﬁne-tuning has led r esearchers to improve these tech-\\nniques, such as designing more efﬁcient ﬁne-tuning algorit hms. While the research on ﬁne-tuning\\nis fruitful, in this section we just give a ﬂavour of the key st eps involved. W e will see more detailed\\ndiscussions on this topic in the following chapters.\\n2.1.4 Aligning LLMs with the W orld\\nInstruction ﬁne-tuning provides a simple way to adapt LLMs to tasks that can be well deﬁned. This\\nproblem can broadly be categorized as an alignment problem. Here, alignment is referred to as a\\nprocess of guiding LLMs to behave in ways that align with huma n intentions. The guidance can\\ncome from labeled data, human feedback, or any other form of h uman preferences. For example,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 53, 'page_label': '47'}, page_content='2.1 A Brief Introduction to LLMs 47\\nwe want LLMs not only to be accurate in following instruction s, but also to be unbiased, truthful,\\nand harmless. So we need to supervise the models towards huma n values and expectations. A\\ncommon example is that when we ask an LLM how to build a weapon, it may provide a list of key\\nsteps to do so if it is not carefully aligned. However, a respo nsible model should recognize and\\navoid responding to requests for harmful or illegal informa tion. Alignment in this case is crucial\\nfor ensuring that LLMs act responsibly and in accordance wit h ethical guidelines.\\nA related concept to alignment is AI safety . One ultimate goa l of AI is to build intelligent\\nsystems that are safe and socially beneﬁcial. T o achieve thi s goal we should keep these systems\\nrobust, secure, and subjective, in any conditions of real-w orld use, even in conditions of misuse\\nor adverse use. For LLMs, the safety can be increased by align ing them with appropriate human\\nguidance, such as human labeled data and interactions with u sers during application.\\nAlignment is difﬁcult as human values and expectations are d iverse and shifting. Sometimes,\\nit is hard to describe precisely what humans want, unless we s ee the response of LLMs to user\\nrequests. This makes alignment no longer a problem of tuning LLMs on predeﬁned tasks, but a\\nbigger problem of training them with the interactions with t he real world.\\nAs a result of the concerns with controlling AI systems, ther e has been a surge in research\\non the alignment issue for LLMs. T ypically , two alignment st eps are adopted after LLMs are\\npre-trained on large-scale unlabeled data.\\n• Supervised Fine-tuning (SFT). This involves continuing the training of pre-trained LLM s\\non new , task-oriented, labelled data. A commonly used SFT te chnique is instruction ﬁne-\\ntuning. As described in the previous subsection, by learnin g from instruction-response\\nannotated data, LLMs can align with the intended behaviors f or following instructions,\\nthereby becoming capable of performing various instructio n-described tasks. Supervised\\nﬁne-tuning can be seen as following the pre-training + ﬁne-t uning paradigm, and offers a\\nrelatively straightforward method to adapt LLMs.\\n• Learning from Human Feedback . After an LLM ﬁnishes pre-training and supervised ﬁne-\\ntuning, it can be used to respond to user requests if appropri ately prompted. But this model\\nmay generate content that is unfactual, biased, or harmful. T o make the LLM more aligned\\nwith the users, one simple approach is to directly learn from human feedback. For example,\\ngiven some instructions and inputs provided by the users, ex perts are asked to evaluate how\\nwell the model responds in accordance with their preference s and interests. This feedback\\nis then used to further train the LLM for better alignment.\\nA typical method for learning from human feedback is to consi der it as a reinforcement learn-\\ning (RL) problem, known as reinforcement learning from human feedback (RLHF) [ Ouyang et al. ,\\n2022]. The RLHF method was initially proposed to address general sequential decision-making\\nproblems [ Christiano et al. , 2017], and was later successfully employed in the development of\\nthe GPT series models [ Stiennon et al. , 2020]. As a reinforcement learning approach, the goal of\\nRLHF is to learn a policy by maximizing some reward from the en vironment. Speciﬁcally , two\\ncomponents are built in RLHF:\\n• Agent. An agent, also called an LM agent, is the LLM that we want to tr ain. This agent\\noperates by interacting with its environment: it receives a text from the environment and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 54, 'page_label': '48'}, page_content='48 Generative Models\\noutputs another text that is sent back to the environment. Th e policy of the agent is the\\nfunction deﬁned by the LLM, that is, Pr(y|x).\\n• Reward Model . A reward model is a proxy of the environment. Each time the ag ent\\nproduces an output sequence, the reward model assigns this o utput sequence a numerical\\nscore (i.e., the reward). This score tells the agent how good the output sequence is.\\nIn RLHF , we need to perform two learning tasks: 1) reward mode l learning, which involves\\ntraining a reward model using human feedback on the output of the agent, and 2) policy learning,\\nwhich involves optimizing a policy guided by the reward mode l using reinforcement learning\\nalgorithms. Here is a brief outline of the key steps involved in RLHF .\\n• Build an initial policy using pre-training and instructio n ﬁne-tuning.\\n• Use the policy to generate multiple outputs for each input, and then collect human feedback\\non these outputs (e.g., comparisons of the outputs).\\n• Learn a reward model from the human feedback.\\n• Fine-tune the policy with the supervision from the reward model.\\nFigure\\n2.2 shows an overview of RLHF . Given that this section serves onl y as a brief intro-\\nduction to concepts of LLMs, a detailed discussion of RLHF te chniques will not be included. W e\\ninstead illustrate the basic ideas behind RLHF using a simpl e example.\\nSuppose we have trained an LLM via pre-training and instruct ion ﬁne-tuning. This LLM is\\ndeployed to respond to requests from users. For example, a us er may input\\nHow can I live a more environmentally friendly life?\\nW e use the LLM to generate 4 different outputs (denoted by {y1 ,...,y4 }) by sampling the\\noutput space\\nOutput 1 ( y1 ): Consider switching to an electric vehicle or bicycle inst ead of\\ntraditional cars to reduce carbon emissions and protect our planet.\\nOutput 2 ( y2 ): Adopt a minimalist lifestyle. Own fewer possessions to re duce\\nconsumption and the environmental impact of manufacturing and\\ndisposal.\\nOutput 3 ( y3 ): Go off-grid. Generate your own renewable energy and colle ct\\nrainwater to become completely self-sufﬁcient and reduce r eliance\\non non-renewable resources.\\nOutput 4 ( y4 ): Support local farm products to reduce the carbon footprin t of\\ntransporting food, while enjoying fresh, healthy food.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 55, 'page_label': '49'}, page_content='2.1 A Brief Introduction to LLMs 49\\nLLM\\nPre-training Data\\nI love the food here! ...\\nHow can I get there? ...\\nSFT Data\\nweather in London . ...\\nWrite a poem about the\\nPre-training &\\nSupervised ﬁne-tuning\\n(a) Learning an Initial LLM\\nLLM\\nUser Input\\nenvironmentally friendly?\\nHow can I live more\\nModel Output\\n3. ............ 4. ............\\n1. ............ 2. ............\\nPredicting\\nComparisons\\ny1 ≻ y4 ≻ y2 ≻ y3\\nAnnotating Data with Human Preferences\\n(b) Annotating Data with Human Preferences\\nReward Model\\nComparison Data\\n{(x, yk1 ≻ yk2 )}\\nTraining\\n(c) Training the Reward Model\\nLLM\\n(Policy)\\nDataset D\\nx ∼ D\\nInput-output Pairs\\n{x, y}\\nSampling y via the Policy Pr(y|x)\\nReward Model\\nReward Scores\\n{r(x, y)}\\nEvaluate the Input-output Pairs\\n(d) Training/Fine-tuning the Policy\\nRL Fine-tuning\\nFig. 2.2: An overview of RLHF . There are 4 key steps involved: a) traini ng an initial LLM (i.e., policy) using pre-\\ntraining and supervised ﬁne-tuning; b) collecting human pr eference data by ranking the outputs of the LLM; c) training\\na reward model using the ranking results; d) RL ﬁne-tuning of the policy based on the reward model. Double line\\narrows mean training or ﬁne-tuning.\\nW e then ask annotators to evaluate these outputs. One straig htforward way is to assign a rating\\nscore to each output. In this case, the reward model learning problem can be framed as a task of\\ntraining a regression model. But giving numerical scores to LLM outputs is not an easy task for\\nannotators. It is usually difﬁcult to design an annotation s tandard that all annotators can agree on\\nand easily follow . An alternative method, which is more popu lar in the development of LLMs, is\\nto rank these outputs. For example, a possible ranking of the above outputs is\\ny1 ≻ y4 ≻ y2 ≻ y3'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 56, 'page_label': '50'}, page_content='50 Generative Models\\nA reward model is then trained using this ranking result. In g eneral, a reward model in RLHF\\nis a language model that shares the same architecture as the t arget LLM, but with a smaller model\\nsize. Given the input x and output yk , we concatenate them to form a sequence seqk = [ x,yk ].\\nThis sequence is processed from left to right using forced de coding. Since each position can\\nonly access its left context in language modeling, the outpu t of the top-most Transformer layer at\\nthe ﬁrst position cannot be used as the representation of the sequence. Instead, a special symbol\\n(e.g., ⟨\\\\ s⟩ ) is added to the end of the sequence, and the corresponding ou tput of the Transformer\\nlayer stack is considered as the representation of the entir e sequence. An output layer, such as a\\nlinear transformation layer, is built on top of this represe ntation to generate the reward, denoted\\nby R(seqk ) or R(x,yk ).\\nW e train this reward model using ranking loss. For example, a pair-wise ranking loss function\\ncan be written in the form\\nLossω (Dr ) = −E(x, yk1 , yk2 )∼Dr log(Sigmoid(Rω (x,yk1 ) − Rω (x,yk2 ))) (2.19)\\nwhere ω represents the parameters of the reward model, and Dr represents a set of tuples of an\\ninput and a pair of outputs. (x,yk1 ,yk2 ) ∼ D r is a sampling operation which draws a sample\\n(x,yk1 ,yk2 ) from Dr with some probability . As an example, suppose we ﬁrst draw a m odel\\ninput x with a uniform distribution and then draw a pair of model outp uts with a probability of\\nyk1 ≻ yk2 given x (denoted by Pr(yk1 ≻ yk2 |x)). The corresponding loss function is given by\\nLossω (Dr )\\n= −\\n∑\\nPr(x) · Pr(yk1 ≻ yk2 |x) · log(Sigmoid(Rω (x,yk1 ) − Rω (x,yk2 )))\\n= − 1\\nK\\n∑\\nPr(yk1 ≻ yk2 |x) · log(Sigmoid(Rω (x,yk1 ) − Rω (x,yk2 ))) (2.20)\\nwhere K represents the number of model inputs involved in sampling. While the form of these\\nfunctions may seem complex, their idea is simple: we penaliz e the model if the predicted ranking\\nof two outputs differs from the human-labeled ranking. By co ntrast, the model receives a bonus,\\nif the predicted ranking matches the human-labeled ranking .\\nW e can train the reward model by minimizing the above ranking loss\\nˆω = arg min\\nω\\nLossω (Dr ) (2.21)\\nThe resulting model Rˆω (·) can be employed to evaluate any given pair of input and output . Note\\nthat although the reward model is trained using a ranking-ba sed objective, it is used for scoring.\\nThis allows it to provide continuous supervision signals, w hich is very beneﬁcial for training other\\nmodels.\\nW e now turn to the policy learning problem. A commonly adopte d objective is to maximize\\nthe reward on a set of input-output pairs. Following an analo gous form of Eq. ( 2.16), we obtain a\\nsimple training objective for RL ﬁne-tuning\\n˜θ = arg max\\nˆθ +\\nE(x, y ˆθ + )∼Drlft Rˆω (x,yˆθ + ) (2.22)\\nwhere the optimal parameters ˜θare obtained by ﬁne-tuning the pre-trained parameters ˆθ. Drlft is'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 57, 'page_label': '51'}, page_content='2.1 A Brief Introduction to LLMs 51\\nthe RL ﬁne-tuning dataset. For each sample (x,yˆθ + ), x is sampled from a prepared dataset of\\ninput sequences, and yˆθ + is sampled from the distribution Prˆθ + (y|x) given by the policy .\\nIn practice, more advanced reinforcement learning algorit hms, such as proximal policy opti-\\nmization (PPO), are often used for achieving more stable training, as well as better performance.\\nW e leave the detailed discussion of reinforcement learning algorithms to the following parts of\\nthis book where RLHF is extensively used for alignment.\\nAn interesting question arises here: why not consider learn ing from human preferences as\\na standard supervised learning problem? This question is cl osely related to our aforementioned\\ndiscussion on the difﬁculty of data annotation. Often, desc ribing human values and goals is chal-\\nlenging, and it is even more difﬁcult for humans to provide ou tputs that are well aligned. As an\\nalternative, annotating the preferences of a given list of m odel outputs offers a simpler task. By\\ndoing so, we can create a model that understands human prefer ences, which can then be used as\\na reward model for training policies. From the perspective o f machine learning, RLHF is par-\\nticularly useful for scenarios where the desired behavior o f an agent is difﬁcult to demonstrate\\nbut can be easily recognized by humans. Another advantage of RLHF is its ability to explore the\\nsample space. By employing sampling techniques, models tra ined with reinforcement learning\\ncan venture beyond the annotated data set to explore additio nal samples. This exploratory ability\\nallows RLHF to discover potentially beneﬁcial policies tha t are not immediately apparent from\\nthe labeled data alone.\\n2.1.5 Prompting LLMs\\nW e have so far shown that LLMs can be used to perform various tasks by giving them appropriate\\nprompts. There are no restrictions on these prompts, which c an include any information we wish\\nto ask or communicate with LLMs, such as natural language ins tructions and the context of con-\\nversations. Since this approach requires no additional tra ining or tuning, adapting LLMs becomes\\nhighly efﬁcient once they are developed. This somewhat inﬂu ences the paradigms in NLP: we\\nno longer need to develop speciﬁc systems for individual tas ks but can instead use a single, well-\\ntrained LLM to perform different tasks by prompting it. An ap pealing aspect of LLM prompting\\narises as a result: users can easily have “customized” syste ms by designing their own prompts\\nfor LLMs. Given the important role played by prompting in LLM s, prompting engineering has\\nbecome a very active area of research in NLP .\\nThe term prompt is used in many different ways in the literature. In this chap ter, this term\\nrefers to the entire input to LLMs, and so we use the terms prompt and model input interchange-\\nably . Before discussing prompting further, let us ﬁrst see a few examples where the prompts\\nare more complex than those presented in the previous subsec tions. Note that this subsection is\\nnot aimed at writing high-quality prompts but rather at high lighting some interesting issues in\\nprompting LLMs.\\nOne of the popular ways to use LLMs is to assign them a “role” pl ayed in generating responses.\\nFor example, LLMs can act as a psychologist when answering qu estions.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 58, 'page_label': '52'}, page_content='52 Generative Models\\nPlease explain what delayed gratiﬁcation is?\\nNote, you are a researcher with a deep background in psychology . Y our own\\nﬁeld of study is developmental psychology in children, with a focus on aspects\\nof control and intelligence. Y ou are very familiar with the c oncept of delayed\\ngratiﬁcation and have applied it to your own research. Y ou ha ve published papers\\non delayed gratiﬁcation and conducted related experiments .\\nDelayed gratiﬁcation is the process of resisting an immediate reward in\\nanticipation of receiving a more valuable reward in the future. It is a signiﬁcant\\nconcept in developmental psychology , especially in relation to how children learn\\nself-control and develop cognitive abilities. Research shows that the ability to\\ndelay gratiﬁcation is associated with positive outcomes in academic performance,\\nsocial competence, and emotional well-being. Studies and experiments in this\\narea highlight the importance of cultivating this skill early in life to support\\nlong-term success and resilience.\\nAnother example is the use of LLMs in detecting and correctin g errors such as syntactic or\\nsemantic mistakes in text. For an LLM which is trained on both code and natural language data,\\nwe may use it for code debugging 6 .\\nFix the bugs in this C language program.\\n#include<stdio.h>\\nint main() {\\nprintg(\"Hello, W orld!\")\\nreturn 0;\\n}\\n1. The function name ’printg’ is incorrect . It should be ’pri ntf’.\\n2. There is a missing semicolon at the end of the ’printf’ func tion call.\\nHere is the corrected version of the program:\\n#include <stdio.h>\\nint main() {\\nprintf(\"Hello, W orld!\");\\nreturn 0;\\n}\\nAlso, LLMs can be ﬁne-tuned to act as a conversation partner 7.\\n6 In this example, the code is not tokenized for easier reading .\\n7 T o ﬁne-tune an LLM for multi-turn dialogue, one needs to cons ider conversation history in the context for pre-\\ndicting the response in the current round of conversation. T his makes the actual prompt used in response generation\\nrelatively longer than that used in single-turn dialogue.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 59, 'page_label': '53'}, page_content='2.1 A Brief Introduction to LLMs 53\\nHi! I’m an assistant. How can I help you?\\nWho won the FIF A W orld Cup 2022?\\nArgentina won the FIF A W orld Cup 2022.\\nWhere was it held?\\nThe 2022 FIF A W orld Cup was held in Qatar.\\nHow many times has Argentina won the W orld Cup?\\nArgentina has won the FIF A W orld Cup three times.\\n...\\nThese examples and previous ones have shown that appropriat e responses can be generated\\nvia prompts involving clear instructions and questions. Ho wever, when problem solving requires\\nknowledge that is not explicitly speciﬁed, LLMs may make mis takes, even though the instructions\\nare sufﬁciently clear and precise. A family of challenging t asks for LLMs involves arithmetic\\nreasoning and commonsense reasoning. For example, we can as k an LLM to solve primary school\\nmath problems presented in natural language.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\\napples. The next day , Jack gave 3 apples to his friend John. Ho w many apples\\ndoes Jack have left in the end?\\nThe answer is 10.\\nThe correct answer should be 7, so the model output is incorre ct.\\nOne approach to addressing such issues is to incorporate lea rning into prompts, called in-\\ncontext learning or ( ICL). The idea of ICL is to demonstrate the ways to solve problems in\\nprompts, and condition predictions on these demonstration s. Here is an example where a similar\\nproblem and the corresponding answer are presented in the pr ompt (green = demonstrations).\\nT om has 12 marbles. He wins 7 more marbles in a game with his fri end but then\\nloses 5 marbles the next day . His brother gives him another 3 m arbles as a gift.\\nHow many marbles does T om have now?\\nThe answer is 17.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\\napples. The next day , Jack gave 3 apples to his friend John. Ho w many apples\\ndoes Jack have left in the end?\\nThe answer is 12.\\nBut the LLM still made mistakes this time. A reason for this mi ght be that solving math\\nproblems does not only involve problem-answer mappings but also, to a larger extent, the under-\\nlying logical inference in multiple steps. A method to impro ve the inference abilities of LLMs\\nis chain-of-thought prompting (COT prompting ) [ W ei et al. , 2022c]. In COT prompting, we\\ndecompose complex reasoning problems into multiple proble m-solving intermediate steps. These'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 60, 'page_label': '54'}, page_content='54 Generative Models\\nsteps are demonstrated in prompts so that LLMs can be prompte d to learn to mimic the underlying\\nthought processes in solving the problems. COT prompting ha s been found to be very useful for\\nachieving good results on challenging tasks, such as the GSM 8K mathematical reasoning bench-\\nmark.\\nConsider the above demonstration example T om has 12 marbles. He wins 7 more marbles in\\na game with his friend but then loses 5 marbles the next day. Hi s brother gives him another 3\\nmarbles as a gift. How many marbles does T om have now ? . The corresponding logical reasoning\\nsteps can be structured:\\n1. T om starts with 12 marbles.\\n2. He wins 7 more marbles in a game, so we add those to his initial count: 12 marbles + 7\\nmarbles = 19 marbles.\\n3. The next day , T om loses 5 marbles, so we subtract those from his current total: 19 marbles\\n- 5 marbles = 14 marbles.\\n4. His brother gives him 3 more marbles, so we add those to his c urrent total: 14 marbles + 3\\nmarbles = 17 marbles.\\n5. After all these events, T om has 17 marbles.\\nW e can add these reasoning steps into the prompt, and get the c orrect answer.\\nT om has 12 marbles. He wins 7 more marbles in a game with his fri end but then\\nloses 5 marbles the next day . His brother gives him another 3 m arbles as a gift.\\nHow many marbles does T om have now?\\nT om initially has 12 marbles. After winning a game, he gains 7more, increasing\\nhis total to 19 marbles. He loses 5 marbles the next day , bring ing his total down\\nto 14 marbles. His brother gifts him 3 more marbles, increasi ng his total to 17\\nmarbles. Therefore, T om now has 17 marbles. So the answer is 1 7.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\\napples. The next day , Jack gave 3 apples to his friend John. Ho w many apples\\ndoes Jack have left in the end?\\nJack starts with 7 apples. He eats 2 apples for dinner, so we subtract 2 from 7,\\nleaving him with 5 apples. His mom gives him 5 more apples, adding 5 to his\\ncurrent total. Now he has 5 + 5 = 10 apples. The next day , Jack gives 3 apples to\\nhis friend John. Subtracting 3 from 10, Jack is left with 7 apples. Therefore, Jack\\nhas 7 apples left. So the answer is 7.\\nThis approach is also called one-shot COT prompting . Likewise, we can add more demon-\\nstrations to have a few-shot COT prompting approach.\\nFurthermore, a special kind of COT , zero-shot COT , introduces instructions that provoke\\nLLMs to think, without presenting any intermediate reasoni ng steps in the prompts [ Kojima et al. ,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 61, 'page_label': '55'}, page_content='2.1 A Brief Introduction to LLMs 55\\n2022]. An interesting example is that by appending a simple sente nce “Let’s think step by step. ”\\nto the end of a prompt, LLMs can generate reasoning steps to re ach the ﬁnal answer.\\nJack has 7 apples. He ate 2 of them for dinner, but then his mom g ave him 5 more\\napples. The next day , Jack gave 3 apples to his friend John. Ho w many apples\\ndoes Jack have left in the end?\\nLet’s think step by step.\\n1. Initial Quantity: Jack starts with 7 apples.\\n2. After Dinner: He eats 2 apples, so 7 − 2 = 5 apples remain.\\n3. His Mom Gives More: His mom gives him 5 more apples, so 5 + 5 = 10\\napples are now with him.\\n4. Giving to John: The next day , Jack gives 3 apples to his frie nd John,\\nso 10 − 3 = 7 apples are left.\\nIn the end, Jack has 7 apples left.\\nZero-shot, one-shot, and few-shot learning are common conc epts in the area of in-context\\nlearning for LLMs and are not restricted to COT prompting. Br oadly speaking, any prompting\\nthat involves only simple instructions without any demonst rations can be considered a form of\\nzero-shot learning. This zero-shot learning ability emerg es as LLMs are pre-trained and/or ﬁne-\\ntuned. Also, one-shot and few-shot learning methods are mor e often considered when LLMs do\\nnot acquire the corresponding zero-shot learning ability . These methods are therefore important\\nfor in-context learning when addressing new tasks. Example s include those for performing various\\nNLP tasks by demonstrating task-formatted samples. See the following examples for sentiment\\nsentence classiﬁcation and phrase translation via few-sho t learning.\\nGiven the following text snippets, classify their sentimen t as Positive, Negative,\\nor Neutral.\\nExample 1: “I had an amazing day at the park!”\\nSentiment: Positive\\nExample 2: “The service at the restaurant was terrible. ”\\nSentiment: Negative\\nExample 3: “I think it’s going to rain today . ”\\nSentiment: Neutral\\nT ext: “This movie was a fantastic journey through imaginati on. ”\\nSentiment: Positive'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 62, 'page_label': '56'}, page_content='56 Generative Models\\nTranslate the following Chinese phrases into English.\\nExample 1: “ 你好”\\nTranslation: “Hello”\\nExample 2: “谢谢你”\\nTranslation: “Thank you”\\nPhrase to translate: “ 早上好”\\nTranslation: “Good Morning”\\nAbove, we have presented examples to illustrate the fundame ntal in-context learning capa-\\nbilities of prompting LLMs. This section, however, does not include more advanced prompting\\ntechniques in order to keep the content concise and compact. More discussions on prompting can\\nbe found in Chapter 3.\\n2.2 T raining at Scale\\nAs a ﬁrst step in developing LLMs, we need to train these models on large amounts of data.\\nThe training task is itself standard: the objective is to max imize the likelihood, which can be\\nachieved via gradient descent. However, as we scale up both t he model size and the amount\\nof data, the problem becomes very challenging, for example, large models generally make the\\ntraining unstable. In this section, we discuss several issu es of large-scale training for LLMs,\\nincluding data preparation, model modiﬁcation, and distri buted training. W e also discuss the\\nscaling laws for LLMs, which help us understand their traini ng efﬁciency and effectiveness.\\n2.2.1 Data Preparation\\nThe importance of data cannot be overstated in NLP . As largerneural networks are developed,\\nthe demand for data continues to increase. For example, deve loping LLMs may require trillions\\nof tokens in pre-training (see T able\\n2.3), orders of magnitude larger than those used in training\\nconventional NLP models. In general, we may want to gather as much training data as possible.\\nHowever, larger training datasets do not mean better traini ng results, and the development of\\nLLMs raises new issues in creating or collecting these datas ets.\\nA ﬁrst issue is the quality of data. High-quality data has lon g been seen as crucial for training\\ndata-driven NLP systems. Directly using raw text from vario us sources is in general undesirable.\\nFor example, a signiﬁcant portion of the data used to train re cent LLMs comes from web scraping,\\nwhich may contain errors and inappropriate content, such as toxic information and fabricated\\nfacts. Also, the internet is ﬂooded with machine-generated content due to the widespread use of\\nAI, presenting further challenges for processing and using web-scraped data. Researchers have\\nfound that training LLMs on unﬁltered data is harmful [ Raffel et al. , 2020]. Improving data quality\\ntypically involves incorporating ﬁltering and cleaning st eps in the data processing workﬂow . For\\nexample, Penedo et al. [2023] show that by adopting a number of data processing technique s, 90%\\nof their web-scraped data can be removed for LLM training. In addition to large-scale web-scraped\\ndata, LLM training data often includes books, papers, user- generated data on social media, and\\nso on. Most of the latest LLMs are trained on such combined dat asets, which are found to be'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 63, 'page_label': '57'}, page_content='2.2 Training at Scale 57\\nLLM # of T okens Data\\nGPT3-175B [ Brown et al. , 2020] 0.5T W ebpages, Books, Wikipedia\\nFalcon-180B [ Almazrouei et al. , 2023] 3.5T W ebpages, Books, Conversations,\\nCode, T echnical Articles\\nLLaMA2-65B [ T ouvron et al. , 2023a] 1.0T ∼ 1.4T W ebpages, Code, Wikipedia,\\nBooks, Papers, Q&As\\nPaLM-450B [ Chowdhery et al. , 2022] 0.78T W ebpages, Books, Conversations,\\nCode, Wikipedia, News\\nGemma-7B [ Gemma T eam, 2024] 6T W ebpages, Mathematics, Code\\nT able 2.3: Amounts of training data used in some LLMs in terms of the numb er of tokens.\\nimportant for the strong performance of the resulting model s.\\nA second issue is the diversity of data. W e want the training d ata to cover as many types of\\ndata as possible, so that the trained models can adapt to diff erent downstream tasks easily . It has\\nbeen widely recognized that the quality and diversity of tra ining data both play very important\\nroles in LLMs. An interesting example is that incorporating programming code into training data\\nhas been found to be beneﬁcial for LLMs. The beneﬁts are demon strated not only in enhancing the\\nprogramming abilities of LLMs, but also in improving reason ing for complex problems, especially\\nthose requiring COT prompting. The concept “diversity” can be extended to include language\\ndiversity as well. For example, many LLMs are trained on mult i-lingual data, and therefore we\\ncan handle multiple languages using a single model. While th is approach shows strong abilities\\nin multi-lingual and cross-lingual tasks, its performance on speciﬁc languages largely depends on\\nthe volume and quality of the data for those languages. It has been shown in some cases to provide\\npoor results for low-resource languages.\\nA third issue is the bias in training data. This is not a proble m that is speciﬁc to LLMs but\\nexists in many NLP systems. A common example is gender bias, w here LLMs show a preference\\nfor one gender over another. This can partly be attributed to class imbalance in the training data,\\nfor example, the term nurses is more often associated with women. In order to debias the da ta,\\nit is common practice to balance the categories of different language phenomena, such as gender,\\nethnicity , and dialects. The bias in data is also related to t he diversity issue mentioned above.\\nFor example, since many LLMs are trained and aligned with Eng lish-centric data, they are bi-\\nased towards the cultural values and perspectives prevalen t among English-speaking populations.\\nIncreasing language diversity in training data can somewha t mitigate the bias.\\nAnother issue with collecting large-scale data is the priva cy concern. If LLMs are trained\\non data from extensive sources, this potentially leads to ri sks regarding the exposure of sensitive\\ninformation, such as intellectual property and personal da ta. This is particularly concerning given\\nthe capacity of LLMs to represent patterns from the data they are trained on, which might in-\\nadvertently involve memorizing and reproducing speciﬁc de tails. A simple approach to privacy\\nprotection is to remove or anonymize sensitive information . For example, anonymization tech-\\nniques can be applied to remove personally identiﬁable info rmation from training data to prevent\\nLLMs from learning from such data. However, in practice, era sing or redacting all sensitive data\\nis difﬁcult. Therefore, many LLMs, particularly those laun ched for public service, typically work\\nwith systems that can detect the potential exposure of sensi tive data, or are ﬁne-tuned to reject'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 64, 'page_label': '58'}, page_content='58 Generative Models\\ncertain requests that could lead to information leakage.\\n2.2.2 Model Modiﬁcations\\nTraining LLMs is difﬁcult. A commonly encountered problem is that the training process be-\\ncomes more unstable as LLMs get bigger. For example, one need s to choose a small learning rate\\nto achieve stable training with gradient descent, but this i n turn results in much longer training\\ntimes. Sometimes, even when the training conﬁguration is ca refully designed, training may di-\\nverge at certain points during optimization. The training o f LLMs is generally inﬂuenced by many\\nfactors, such as parameter initialization, batching, and r egularization. Here, we focus on common\\nmodiﬁcations and improvements to the standard Transformer architecture, which are considered\\nimportant in developing trainable LLMs.\\n2.2.2.1 Layer Normalization with Residual Connections\\nLayer normalization is used to stabilize training for deep neural networks. It is a process of\\nsubtracting the mean and dividing by the standard deviation . By normalizing layer output in\\nthis way , we can effectively reduce the covariate shift prob lem and improve the training stability .\\nIn Transformers, layer normalization is typically used tog ether with residual connections. As\\ndescribed in Section\\n2.1.1, a sub-layer can be based on either the post-norm architectu re, in which\\nlayer normalization is performed right after a residual blo ck, or the pre-norm architecture, in\\nwhich layer normalization is performed inside a residual bl ock. While both of these architectures\\nare widely used in Transformer-based systems [ W ang et al. , 2019], the pre-norm architecture has\\nproven to be especially useful in training deep Transformer s. Given this, most LLMs are based on\\nthe pre-norm architecture, expressed as output = LNorm( F(input)) + input .\\nA widely-used form of the layer normalization function is gi ven by\\nLNorm(h) = α· h − µ\\nσ+ ǫ + β (2.23)\\nwhere h is a d-dimensional real-valued vector, µis the mean of all the entries of h, and σis the\\ncorresponding standard deviation. ǫis introduced for the sake of numerical stability . α∈ Rd and\\nβ∈ Rd are the gain and bias terms.\\nA variant of layer normalization, called root mean square (R MS) layer normalization, only\\nre-scales the input vector but does not re-center it [ Zhang and Sennrich , 2019]. The RMS layer\\nnormalization function is given by\\nLNorm(h) = α· h\\nσrms + ǫ + β (2.24)\\nwhere σrms is the root mean square of h, that is, σrms = ( 1\\nd\\n∑ d\\nk=1 h2\\nk)\\n1\\n2 . This layer normalization\\nfunction is used in LLMs like the LLaMA series.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 65, 'page_label': '59'}, page_content='2.2 Training at Scale 59\\n2.2.2.2 Activation Functions in FFNs\\nIn Transformers, FFN sub-layers are designed to introduce non-linearities into representation\\nlearning, and are found to be useful for preventing the repre sentations learned by self-attention\\nfrom degeneration\\n8 [Dong et al. , 2021]. A standard form of the FFNs used in these sub-layers can\\nbe expressed as\\nFFN(h) = σ(h Wh + bh)Wf + bf (2.25)\\nwhere Wh ∈ Rd×dh , bh ∈ Rdh , Wf ∈ Rdh ×d , and bf ∈ Rd are the parameters, and dh is the\\nhidden size. σ(·) is the activation function of the hidden layer. A common choi ce for σ(·) is the\\nrectiﬁed linear unit (ReLU), given by\\nσrelu(h) = max(0 ,h) (2.26)\\nIn practical implementations, increasing dh is helpful and thus it is often set to a larger number\\nin LLMs. But a very large hidden size poses challenges for bot h training and deployment. In this\\ncase, the design of the activation function plays a relative ly more important role in wide FFNs.\\nThere are several alternatives to the ReLU in LLMs. One of the se is the gaussian error linear\\nunit (GeLU) which can be seen as a smoothed version of the ReLU. Rather th an controlling the\\noutput by the sign of the input, the GeLU function weights its input by the percentile Pr(h≤ h).\\nHere his a d-dimensional vector whose entries are drawn from the standa rd normal distribution\\nGaussian(0,1)9 . Speciﬁcally , the GeLU function is deﬁned to be\\nσgelu(h) = h Pr(h≤ h)\\n= hΦ( h) (2.27)\\nwhere Φ( h) is the cumulative distribution function of Gaussian(0,1), which can be implemented\\nin convenient ways [ Hendrycks and Gimpel , 2016]. The GeLU function has been adopted in\\nseveral LLMs, such as BERT , GPT -3, and BLOOM.\\nAnother family of activation functions which is popular in L LMs is gated linear unit (GLU)-\\nbased functions. The basic form of GLUs is given by\\nσglu(h) = σ(h W1 + b1) ⊙ (W2 + b2) (2.28)\\nwhere W1 ∈ Rd×d , b1 ∈ Rd, W2 ∈ Rd×d, and b2 ∈ Rd are model parameters. Different choices\\nof σ(·) result in different versions of GLU functions. For example, if σ(·) is deﬁned to be the\\nGeLU function, we will have the GeGLU function\\nσgeglu(h) = σgelu(h W1 + b1 ) ⊙ (W2 + b2 ) (2.29)\\nThis activation function has been successfully applied in L LMs like Gemma.\\nAs another example, consider σ(·) to be the Swish function σswish(h) = h ⊙ Sigmoid(ch)\\n8 Here degeneration refers to the phenomenon in which the rank of a matrix is reduced after some processing.\\n9 Pr(h ≤ h) is an informal notation. It refers to a vector, with each entr y representing the percentile for the\\ncorresponding entry of h.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 66, 'page_label': '60'}, page_content='60 Generative Models\\n[Ramachandran et al. , 2017]. Then, the SwiGLU function is given by\\nσswiglu(h) = σswish(h W1 + b1 ) ⊙ (W2 + b2 ) (2.30)\\nBoth the PaLM and LLaMA series are based on the SwiGLU functio n. For more discussions of\\nGLUs, the reader can refer to Shazeer [2020]’s work.\\n2.2.2.3 Removing Bias T erms\\nAnother popular model design is to remove the bias terms in afﬁne transformations used in LLMs.\\nThis treatment can be applied to layer normalization, trans formations of the inputs to QKV atten-\\ntion, and FFNs. For example, we can modify Eq. (\\n2.25) to obtain an FFN with no bias terms\\nFFN(h) = σ(h Wh )Wf (2.31)\\nChowdhery et al. [2022] report that removing bias terms helps improve the training stability\\nof LLMs. This method has been used in several recent LLMs, suc h as LLaMA and Gemma.\\n2.2.2.4 Other Issues\\nMany LLMs also involve modiﬁcations to their positional embedding models. For example, one\\ncan replace sinusoidal positional encodings with rotary po sition embeddings so that the learned\\nLLMs can handle long sequences better. These models will be d iscussed in Section\\n2.3.\\nNote that while model modiﬁcations are common in training LL Ms, the stability of training\\ncan be improved in many different ways. For example, increas ing the batch size as the training\\nproceeds has been found to be useful for some LLMs. In general , achieving stable and efﬁcient\\nlarge-scale LLM training requires carefully designed setu ps, including learning schedules, opti-\\nmizer choices, training parallelism, mixed precision trai ning, and so on. Some of these issues are\\nhighly engineered, and therefore, we typically need a numbe r of training runs to obtain satisfactory\\nLLMs.\\n2.2.3 Distributed T raining\\nTraining LLMs requires signiﬁcant amounts of computational resources. A common approach to\\nimproving training efﬁciency is to use large-scale distrib uted systems. Fortunately , alongside the\\nrise of neural networks in AI, deep learning-oriented softw are and hardware have been developed,\\nmaking it easier to implement LLMs and perform computations . For example, one can now easily\\nﬁne-tune an LLM using deep learning software frameworks and a machine with multiple GPUs.\\nHowever, scaling up the training of LLMs is still challengin g, and requires signiﬁcant efforts in\\ndeveloping hardware and software systems for stable and efﬁ cient distributed training.\\nAn important consideration of distributed training is para llelism. There are several forms\\nof parallelism: data parallelism, model parallelism, tens or parallelism, and pipeline parallelism.\\nDespite different ways to distribute computations across d evices, these parallelism methods are\\nbased on a similar idea: the training problem can be divided i nto smaller tasks that can be ex-\\necuted simultaneously . The issue of parallelism in trainin g LLMs has been extensively studied'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 67, 'page_label': '61'}, page_content='2.2 Training at Scale 61\\n[Narayanan et al. , 2021; Fedus et al. , 2022]. Here we sketch the basic concepts.\\n• Data Parallelism . This method is one of the most widely used parallelism metho ds for\\ntraining neural networks. T o illustrate, consider the simp lest case where the standard delta\\nrule is used in gradient descent\\nθt+1 = θt − lr· ∂Lθ t (Dmini )\\n∂θt\\n(2.32)\\nwhere the new parameters θt+1 is obtained by updating the latest parameters θt with a small\\nstep lr in the direction of the negative loss gradient. ∂L θ t (Dmini )\\n∂θ t is the gradient of the loss\\nwith respect to the parameters θt, and is computed on a minibatch of training sample Dmini.\\nIn data parallelism, we divide Dmini into N smaller batches, denoted by {D1 ,...,DN }.\\nThen, we distribute these batches to N workers, each with a corresponding batch. Once\\nthe data is distributed, these workers can work at the same ti me. The gradient of the entire\\nminibatch is obtained by aggregating the gradients compute d by the workers, like this\\n∂Lθ t (Dmini )\\n∂θt\\n= ∂Lθ t (D1 )\\n∂θt\\ued19 \\ued18\\ued17 \\ued1a\\nworker 1\\n+ ∂Lθ t (D2 )\\n∂θt\\ued19 \\ued18\\ued17 \\ued1a\\nworker 2\\n+ · · · + ∂Lθ t (DN )\\n∂θt\\ued19 \\ued18\\ued17 \\ued1a\\nworker N\\n(2.33)\\nIn ideal cases where the workers coordinate well and the comm unication overhead is small,\\ndata parallelism can achieve nearly an N-fold speed-up for training.\\n• Model Parallelism . Although data parallelism is simple and effective, it requ ires each\\nworker to run the entire LLM and perform the complete forward and backward process.\\nAs LLMs grow larger, it sometimes becomes unfeasible to load and execute an LLM on a\\nsingle device. In this case, we can decouple the LLM into smal ler components and run these\\ncomponents on different devices. One simple way to do this is to group consecutive layers\\nin the layer stack and assign each group to a worker. The worke rs operate in the order of\\nthe layers in the stack, that is, in the forward pass we proces s the input from lower-level to\\nupper-level layers, and in the backward pass we propagate th e error gradients from upper-\\nlevel to lower-level layers. Consider, for example, a Trans former decoder with Lstacked\\nblocks. T o distribute the computation load, each block is as signed to a worker. See the\\nfollowing illustration for a single run of the forward and ba ckward passes of this model.\\nW orker L BL (↑) BL (↓)\\n... ... ...\\nW orker 2 B2 (↑) B2 (↓)\\nW orker 1 B1 (↑) B1 (↓)\\nHere Bl denotes the computation of block l, and the symbols ↑ and ↓ denote the forward and\\nbackward passes, respectively . Note that this parallelism method forces the workers to run\\nin sequence, so a worker has to wait for the previous worker to ﬁnish their job. This results\\nin the devices being idle for most of the time. In practical sy stems, model parallelism is\\ngenerally used together with other parallelism mechanisms to maximize the use of devices.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 68, 'page_label': '62'}, page_content='62 Generative Models\\n• T ensor Parallelism . Parallelism can also be performed in a single computation s tep. A\\ncommon example is splitting a large parameter matrix into ch unks, multiplying an input\\ntensor with each of these chunks separately , and then concat enating the results of these\\nmultiplications to form the output. For example, consider t he multiplication of the repre-\\nsentation h ∈ Rd with the parameter matrix Wh ∈ Rd×dh in an FFN sub-layer (see Eq.\\n(2.25)). W e can slice the matrix Wh ∈ Rd×dh vertically to a sequence of M sub-matrices\\nWh =\\n[\\nW1\\nh W2\\nh ... WM\\nh\\n]\\n(2.34)\\nwhere each sub-matrix Wk\\nhhas a shape of d× dh\\nM . The multiplication of h with Wh can be\\nexpressed as\\nh Wh = h\\n[\\nW1\\nh W2\\nh ... WM\\nh\\n]\\n=\\n[\\nh W1\\nh h W2\\nh ... h WM\\nh\\n]\\n(2.35)\\nW e can perform matrix multiplications {h W1\\nh,h W2\\nh,...,h WM\\nh} on M devices separately .\\nAs a result, we distribute a large matrix multiplication acr oss multiple devices, each of\\nwhich may have relatively small memory . From the perspectiv e of the design of modern\\nGPUs, tensor parallelism over GPUs provides a two-level, ti le-based approach to parallel\\ncomputing. First, at a higher level, we decompose a matrix mu ltiplication into sub-matrix\\nmultiplications that can directly ﬁt into the memory of GPUs . Then, at a lower level, we\\nexecute these sub-matrix multiplications on GPUs using til e-based parallel algorithms that\\nare speciﬁcally optimized for GPUs.\\n• Pipeline Parallelism . Above, in model parallelism, we have described a simple app roach\\nto spreading groups of model components across multiple dev ices. But this method is in-\\nefﬁcient because only one device is activated at a time durin g processing. Pipeline par-\\nallelism addresses this issue by introducing overlaps betw een computations on different\\ndevices [\\nHarlap et al. , 2018; Huang et al. , 2019]. T o do this, a batch of samples is divided\\ninto a number of micro-batches, and then these micro-batche s are processed by each worker\\nas usual. Once a micro-batch is processed by a worker and pass ed to the next one, the\\nfollowing micro-batch immediately occupies the same worke r. In other words, we create\\na pipeline in which different computation steps can overlap if multiple jobs are given to\\nthe pipeline. The following shows an illustration of pipeli ne parallelism for processing 3\\nmicro-batches.\\nW orker L BL, 1 BL, 2 BL, 3 BL, 1 BL, 2 BL, 3\\n... ... ...\\nW orker 2 B2, 1 B2, 2 B2, 3 B2, 1 B2, 2 B2, 3\\nW orker 1 B1, 1 B1, 2 B1, 3 B1, 1 B1, 2 B1, 3\\nHere Bl,k represents the processing of the k-th micro-batch by the l-th worker. Ideally we\\nwould like to maximize the number of micro-batches, and thus minimize the idle time of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 69, 'page_label': '63'}, page_content='2.2 Training at Scale 63\\nworkers. However, in practice, using small micro-batches o ften reduces GPU utilization and\\nincreases task-switching costs. This may , in turn, decreas e the overall system throughput.\\nThe ultimate goal of parallel processing is to achieve linea r growth in efﬁciency , that is, the\\nnumber of samples that can be processed per unit of time incre ases linearly with the number of\\ndevices. However, distributed training is complicated, an d inﬂuenced by many factors in addition\\nto the parallelism method we choose. One problem, which is of ten associated with distributed\\nsystems, is the cost of communication. W e can think of a distr ibuted system as a group of net-\\nworked nodes. Each of these nodes can perform local computat ion or pass data to other nodes. If\\nthere are a large number of such nodes, it will be expensive to distribute and collect data across\\nthem. Sometimes, the time savings brought about by parallel ism are offset by the communica-\\ntion overhead of a large network. Another problem with large -scale distributed systems is that\\nthe synchronization of nodes introduces additional costs. As is often the case, some nodes may\\ntake longer to work, causing others to wait for the slowest on es. While we can use asynchronous\\ntraining to handle heterogeneity in computational resourc es, this may lead to stale gradients and\\nnon-guaranteed convergence. Moreover, as more nodes are ad ded to the network, there is more\\nchance to have crashed nodes during training. In this case, w e need to ensure that the whole\\nsystem is fault tolerant. In many practical settings, to inc rease scalability , one needs to take into\\naccount additional issues, including architecture design , data transfer and computation overlap,\\nload balancing, memory bandwidth and so on.\\nTraining LLMs is so computationally expensive that, even th ough distributed training is al-\\nready in use, researchers and engineers often still employ v arious model compression and speed-\\nup methods to improve training efﬁciency [ W eng, 2021]. One example is mixed precision training,\\nin which low precision data (such as FP16 and FP8 data) is used for gradient computation on each\\nindividual node, and single or double precision data (such a s FP32/FP64 data) is used for updating\\nthe model [ Micikevicius et al. , 2018]. A key operation in this approach is gradient accumulation\\nwhere gradients need to be accumulated and synchronized acr oss nodes. However, due to the\\nnon-associativity of ﬂoating-point addition, this can lea d to slight numerical differences in accu-\\nmulated gradients on different nodes, which may affect mode l convergence and ﬁnal performance.\\nThis problem is more obvious if there are a large number of nod es involved in distributed training,\\nespecially given that low-precision numerical computatio ns may encounter overﬂow and under-\\nﬂow issues, as well as inconsistencies across different har dware devices. Therefore, the design of\\ndistributed systems needs to consider these numerical comp utation issues to ensure satisfactory\\nresults and convergence.\\n2.2.4 Scaling Laws\\nThe success of LLMs reveals that training larger language models using more resources can lead\\nto improved model performance. Researchers have explained this as scaling laws of LLMs. More\\nspeciﬁcally , scaling laws describe the relationships betw een the performance of LLMs and the\\nattributes of LLM training, such as the model size, the amoun t of computation used for training,\\nand the amount of training data. For example,\\nHestness et al. [2017] show that the performance of\\ndeep neural networks is a power-law-like function of the tra ining data size. In the beginning, when\\nthe amount of training data is not large, the performance of t he model improves slowly . Afterward,\\nwhen more training data is used, the model enters a phase of ra pid performance improvement, and\\nthe performance curve resembles a power-law curve. Ultimat ely , the improvement in performance'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 70, 'page_label': '64'}, page_content='64 Generative Models\\nSlow Reduction\\nPhase\\nPower-law Reduction\\nPhase\\nConvergence\\nPhase\\n(Irreducible Error)\\nTraining Dataset Size (Log-scale)\\nNumber of T est Errors (Log-scale)\\nFig. 2.3: A scaling law of test error against a variable of interest (e. g., training dataset size) [ Hestness et al. , 2017]. The\\ncurve of the scaling law can be divided into three phases. At t he beginning, the number of test errors decreases slowly\\nwhen more training data is used, but this only lasts for a shor t period. In the second phase, the number of test errors\\ndecreases drastically , and the curve becomes a power law cur ve. After that, the error reduction slows down again in the\\nthird phase. Note that there are irreducible errors that can not be eliminated, regardless of the amount of training data .\\nbecomes slow again, and more data does not lead to signiﬁcant gains. Figure 2.3 shows an example\\nof such curves.\\nIn NLP , a traditional view holds that the performance gains w ill disappear at a certain point\\nas the training is scaled up. However, recent results show th at, if we consider the problem on\\na larger scale, scaling up training is still a very effective method for obtaining stronger LLMs.\\nFor example, both closed-source and open-source LLMs can be neﬁt from more data, even though\\ntrillions of tokens have already been used for training.\\nWith the increase in the scale of model training, LLMs exhibi t new capabilities, known as the\\nemergent abilities of LLMs. For example, W ei et al. [2022b] studied the scaling properties of\\nLLMs across different model sizes and amounts of computatio nal resources. Their work shows\\nthat some abilities emerge when we scale the model size to cer tain level. The appearance of\\nemergent abilities has demonstrated the role of scaled trai ning in enhancing the performance of\\nLLMs, and it has also, to some extent, motivated researchers to continuously attempt to train larger\\nmodels. As larger and stronger LMs continue to appear, our un derstanding of the scaling laws\\ncontinues to mature. This helps researchers predict the per formance of LLMs during training and\\nestimate the minimal computational resources required to a chieve a given level of performance.\\nT o understand how model performance scales with various fac tors considered during training,\\nit is common to express the model performance as a function of these factors. For example, in\\nthe simplest case, we can express the loss or error of an LLM as a function of a single variable of\\ninterest. However, there are no universal scaling laws that can describe this relationship. Instead,\\ndifferent functions are proposed to ﬁt the learning curves o f LLMs.\\nLet xbe the variable of interest (such as the number of model param eters) and L(x) be the\\nloss of the model given x(such as the cross-entropy loss on test data). The simplest f orm of L(x)\\nis a power law\\nL(x) = axb (2.36)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 71, 'page_label': '65'}, page_content='2.2 Training at Scale 65\\n105 107 109\\n2.4\\n3.2\\n4.0\\n4.8\\n5.6\\nNumber of Parameters\\nT est Loss\\nL(N ) = ( N\\n8. 8·1013 )−0. 076\\n108 109\\n2.7\\n3\\n3.3\\n3.6\\n3.9\\n4.2\\nDataset Size\\nT est Loss\\nL(D) = ( D\\n5. 4·1013 )−0. 095\\nFig. 2.4: T est loss against model size ( N) and training dataset size ( D) (data points are plotted for illustrative purposes).\\nW e plot test loss as a function of N, which is deﬁned as L(N) =\\n( N\\n8. 8×1013\\n) −0. 076\\n, and a function of D, which is\\ndeﬁned as L(D) =\\n( D\\n5. 4×1013\\n) −0. 095\\n[Kaplan et al. , 2020].\\nwhere a and b are parameters that are estimated empirically . Despite its simplicity , this func-\\ntion has successfully interpreted the scaling ability of la nguage models and machine transla-\\ntion systems in terms of model size (denoted by N) and training dataset size (denoted by D)\\n[Gordon et al. , 2021; Hestness et al. , 2017]. For example, Kaplan et al. [2020] found that the per-\\nformance of their language model improves as a power law of ei ther N or D after an initial\\ntransient period, and expressed these relationships using L(N) =\\n( N\\n8. 8×1013\\n) −0. 076 and L(D) =\\n( D\\n5. 4×1013\\n) −0. 095 (see Figure 2.4).\\nAn improvement to this scaling law is to add an irreducible error term to the power law . The\\nform of L(x) is then given by\\nL(x) = axb + ǫ∞ (2.37)\\nwhere ǫ∞ is the irreducible error that accounts for the error due to un known variables, which is\\npresent even as x → ∞ . Eq. ( 2.37) is one of the most widely used forms for designing scaling\\nlaws of LLMs. For example, Rosenfeld et al. [2020] developed a scaling law that involves both\\nmodel scaling and dataset scaling, like this\\nL(N,D) = aNb + cDd + ǫ∞ (2.38)\\nAn example of such formulation is the Chinchilla scaling law . It states that the test loss per\\ntoken is the sum of the inverse proportion functions of N and D, with an additional irreducible\\nerror term. Hoffmann et al. [2022] express this scaling law as\\nL(N,D) = 406.4\\nN0. 34\\n\\ued19 \\ued18\\ued17 \\ued1a\\nmodel scaling\\n+ 410.7\\nD0. 28\\n\\ued19 \\ued18\\ued17 \\ued1a\\ndataset scaling\\n+ 1 .69\\ued19\\ued18\\ued17 \\ued1a\\nirreducible error\\n(2.39)\\nAll the scaling laws mentioned above are based on monotonic f unctions. So they cannot cover\\nfunctions with inﬂection points, such as double descent cur ves. In response, researchers have\\nexplored more sophisticated functions to ﬁt the learning cu rves. Examples of such functions can'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 72, 'page_label': '66'}, page_content='66 Generative Models\\nbe found in Alabdulmohsin et al. [2022] and Caballero et al. [2023]’s work.\\nThe signiﬁcance of scaling laws lies in providing direction al guidance for LLM research: if\\nwe are still in the region of the power law curve, using more re sources to train larger models is a\\nvery promising direction. While this result “forces” big re search groups and companies to invest\\nmore in computational resources to train larger models, whi ch is very expensive, scaling laws\\ncontinuously push the boundaries of AI further away . On the o ther hand, understanding scaling\\nlaws helps researchers make decisions in training LLMs. For example, given the computational\\nresources at hand, the performance of LLMs may be predicted.\\nOne last note on scaling laws in this section. For LLMs, a lowe r test loss does not always\\nimply better performance on all downstream tasks. T o adapt L LMs, there are several steps such\\nas ﬁne-tuning and prompting that may inﬂuence the ﬁnal resul t. Therefore, the scaling laws for\\ndifferent downstream tasks might be different in practice.\\n2.3 Long Sequence Modeling\\nW e have already seen that, in large-scale training, larger language models can be developed by us-\\ning more data and computational resources. However, scalin g up can also occur in other directions.\\nFor instance, in many applications, LLMs are adapted to proc ess signiﬁcantly long sequences. An\\ninteresting example is that we pre-train an LLM on extensive texts of normal length and then ap-\\nply it to deal with very long token sequences, far beyond the l ength encountered in pre-training.\\nHere we use Pr(y|x) to denote the text generation probability where x is the context and y is the\\ngenerated text. There are broadly three types of long sequen ce modeling problems.\\n• T ext generation based on long context (i.e., x is a long sequence). For example, we\\ngenerate a short summary for a very long text.\\n• Long text generation (i.e., y is a long sequence). For example, we generate a long story\\nbased on a few keywords.\\n• Long text generation based on long context (i.e., both x and y are long sequences). For\\nexample, we translate a long document from Chinese to Englis h.\\nRecently , NLP researchers have been more interested in appl ying and evaluating LLMs on\\ntasks where extremely long input texts are involved. Imagin e an LLM, which reads a C++ source\\nﬁle containing tens of thousands of lines, and outlines the f unctionality of the program correspond-\\ning to the source ﬁle. Such models, capable of handling exten sive textual contexts, are sometimes\\ncalled long-context LLMs . In this section we will restrict ourselves to long-context LLMs, but\\nthe methods discussed here can be applicable to other proble ms.\\nFor Transformers, dealing with long sequences is computati onally expensive, as the computa-\\ntional cost of self-attention grows quadratically with the sequence length. This makes it infeasible\\nto train and deploy such models for very long inputs. T wo stra nds of research have tried to adapt\\nTransformers to long-context language modeling.\\n• The ﬁrst explores efﬁcient training methods and model arch itectures to learn self-attention\\nmodels from long-sequence data.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 73, 'page_label': '67'}, page_content='2.3 Long Sequence Modeling 67\\n• The other adapts pre-trained LLMs to handle long sequences with modest or no ﬁne-tuning\\nefforts.\\nHere, we will discuss the former brieﬂy since it can be found i n general discussions of efﬁcient\\nTransformer architectures [ T ay et al. , 2020; Xiao and Zhu , 2023]. W e will focus on the latter,\\nhighlighting popular methods in recent LLMs. W e will also di scuss the strengths and limitations\\nof these long-sequence models.\\n2.3.1 Optimization from HPC Perspectives\\nW e begin our discussion by considering improvements to standard Transformer models from the\\nperspectives of high-performance computing. Most of these improvements, though not speciﬁ-\\ncally designed for LLMs, have been widely applied across var ious deep learning models [\\nKim et al. ,\\n2023]. A commonly used approach is to adopt a low-precision imple mentation of Transformers.\\nFor example, we can use 8-bit or 16-bit ﬁxed-point data types for arithmetic operations, instead\\nof 32-bit or 64-bit ﬂoating-point data types. Using these lo w-precision data types can increase\\nthe efﬁciency and memory throughput, so that longer sequenc es can be processed more easily .\\nAn alternative approach is to improve Transformers by using hardware-aware techniques. For\\nexample, on modern GPUs, the efﬁciency of Transformers can b e improved by using IO-aware\\nimplementations of the self-attention function [ Dao et al. , 2022; Kwon et al. , 2023].\\nAnother way to handle long sequences is through sequence par allelism [ Li et al. , 2023b;\\nKorthikanti et al. , 2023]. Speciﬁcally , consider the general problem of attending t he query qi\\nat the position ito the keys K and values V. W e can divide K by rows and obtain a set of sub-\\nmatrices {K[1],...,K[nu ]}, each corresponding to a segment of the sequence. Similarly , we can\\nobtain the sub-matrices of V, denoted by {V[1] ,...,V[nu ]}. Then, we assign each pair of K[u] and\\nV[u] to a computing node (e.g., a GPU of a GPU cluster). The assigne d nodes can run in parallel,\\nthereby parallelizing the attention operation.\\nRecall that the output of the self-attention model can be wri tten as\\nAttqkv(qi ,K,V) =\\nm−1∑\\nj=0\\nαi,j vj (2.40)\\nwhere αi,j is the attention weight between positions i and j. In Transformers, αi,j is obtained\\nby normalizing the rescaled version of the dot product betwe en qi and kj . Let βi,j denote the\\nattention score between qi and kj . W e have\\nβi,j = qi · kj√\\nd\\n+ Mask( i,j) (2.41)\\nwhere Mask(i,j) is the masking variable for (i,j). Then, we deﬁne the attention weight αi,j to\\nbe\\nαi,j = Softmax( βi,j )\\n= exp(βi,j )∑\\nj′ exp(βi,j ′ ) (2.42)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 74, 'page_label': '68'}, page_content='68 Generative Models\\nOn each computing node, we need to implement these equations . Given the keys and values\\nassigned to this node, computing the numerator of the right- hand side of Eq. ( 2.42) (i.e., exp(βi,j ))\\nis straightforward, as all the required information is stor ed on the node. However, computing the\\ndenominator of the right-hand side of Eq. ( 2.42) involves a sum of exp(βi,j ′ ) over all j′ s, which\\nrequires transferring data to and from other nodes. T o illus trate, suppose that vj and kj are placed\\non node u. W e can rewrite Eq. ( 2.42) as\\nαi,j\\n=\\nnode u\\ued17 \\ued1a\\ued19 \\ued18\\nexp(βi,j )∑\\nkj′ ∈K[1]\\nexp(βi,j ′ )\\n\\ued19 \\ued18\\ued17 \\ued1a\\nnode 1\\n+ · · · +\\n∑\\nkj′ ∈K[u]\\nexp(βi,j ′ )\\n\\ued19 \\ued18\\ued17 \\ued1a\\nnode u\\n+ · · · +\\n∑\\nkj′ ∈K[nu ]\\nexp(βi,j ′ )\\n\\ued19 \\ued18\\ued17 \\ued1a\\nnode nu\\n(2.43)\\nwhere the notation kj′ ∈ K[u] represents that kj′ is a row vector of K[u]. In a straightforward\\nimplementation, we ﬁrst perform the summations {∑\\nkj′ ∈K[u] exp(βi,j ′ )} separately on the corre-\\nsponding nodes. Then, we collect these summation results fr om different nodes to combine them\\ninto a ﬁnal result. This corresponds to a collective operati on in the context of parallel processing.\\nThere are many efﬁcient implementations of such operations , such as the all-reduce algorithms.\\nHence the sum of all exp(βi,j ) values can be computed using optimized routines in collecti ve\\ncommunication toolkits.\\nGiven the attention weights {αi,j }, we then compute the attention results using Eq. ( 2.40).\\nThe problem can be re-expressed as\\nAttqkv (qi,K,V)\\n=\\n∑\\nvj′ ∈V[1]\\nαi,j ′ vj′\\n\\ued19 \\ued18\\ued17 \\ued1a\\nnode 1\\n+ · · · +\\n∑\\nvj′ ∈V[u]\\nαi,j ′ vj′\\n\\ued19 \\ued18\\ued17 \\ued1a\\nnode u\\n+ · · · +\\n∑\\nvj′ ∈V[nu ]\\nαi,j ′ vj′\\n\\ued19 \\ued18\\ued17 \\ued1a\\nnode nu\\n(2.44)\\nLike Eq. ( 2.43), Eq. ( 2.44) can be implemented as a summation program in parallel proce ss-\\ning. First, perform the weighted summations of values on dif ferent nodes simultaneously . Then,\\nwe collect the results from these nodes via collective opera tions.\\nNote that, although this section primarily focuses on long s equence modeling, much of the mo-\\ntivation for sequence parallelism comes from the distribut ed training methods of deep networks,\\nas discussed in Section 2.2.3. As a result, the implementation of these methods can be base d on\\nthe same parallel processing library .\\n2.3.2 Efﬁcient Architectures\\nOne difﬁculty of applying Transformers to long sequences isthat self-attention has a quadratic\\ntime complexity with respect to the sequence length. Moreov er, a key-value cache (or KV cache\\nfor short) is maintained during inference, and its size incr eases as more tokens are processed. Al-\\nthough the KV cache grows linearly with the sequence length, for extremely long input sequences,\\nthe memory footprint becomes signiﬁcant and it is even infea sible to deploy LLMs for such tasks.\\nAs a result, the model architecture of long-context LLMs gen erally moves away from the standard'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 75, 'page_label': '69'}, page_content='2.3 Long Sequence Modeling 69\\nTransformer, turning instead to the development of more efﬁ cient variants and alternatives.\\nOne approach is to use sparse attention instead of standard s elf-attention. This family of\\nmodels is based on the idea that only a small number of tokens a re considered important when\\nattending to a given token, and so most of the attention weigh ts between tokens are close to zero.\\nAs a consequence, we can prune most of the attention weights a nd represent the attention model\\nin a compressed form. T o illustrate, consider the self-atte ntion model\\nAttqkv (Q,K,V) = α(Q,K)V (2.45)\\nwhere the attention weight matrix α(Q,K) ∈ Rm×m is obtained by\\nα(Q,K) = Softmax( QKT\\n√\\nd\\n+ Mask)\\n=\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nα0, 0 0 0 ... 0\\nα1, 0 α1, 1 0 ... 0\\nα2, 0 α2, 1 α2, 2 ... 0\\n.\\n.\\n. .\\n.\\n. .\\n.\\n. . . . .\\n.\\n.\\nαm−1, 0 αm−1, 1 αm−1, 2 ... α m−1,m −1\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n(2.46)\\nEach row vector\\n[\\nαi, 0 ... αi,i 0 ... 0\\n]\\ncorresponds to a distribution of attending the i-th\\ntoken to every token of the sequence. Since language models p redict next tokens only based on\\ntheir left-context, we normally write the output of the atte ntion model at position ias\\nAttqkv(qi ,K≤i,V≤i ) =\\n[\\nαi, 0 ... αi,i\\n]\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nv0\\n.\\n.\\n.\\nvi\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n=\\ni∑\\nj=0\\nαi,j vj (2.47)\\nwhere K≤i =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nk0\\n.\\n.\\n.\\nki\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fband V≤i =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nv0\\n.\\n.\\n.\\nvi\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fbare the keys and values up to position i.\\nIn the original version of self-attention\\n[\\nαi, 0 ... αi,i\\n]\\nis assumed to be dense, that is, most of\\nthe values are non-zero. In sparse attention, some of the ent ries of\\n[\\nαi, 0 ... αi,i\\n]\\nare considered\\nnon-zero, and the remaining entries are simply ignored in co mputation. Suppose G⊆ { 0,...,i} is\\nthe set of indices of the non-zero entries. For language mode ls, the output of the sparse attention\\nmodel at position iis given by\\nAttsparse(qi ,K≤i ,V≤i) =\\n∑\\nj∈G\\nα′\\ni,j vj (2.48)\\nHere {α′\\ni,j } are normalized over G. Hence their values are different from the original attenti on\\nweights (in fact we have α′\\ni,j >αi,j ). The sparsity of the model is determined by how large Gis.\\nSparse attention models differ in the way we deﬁne G. One simple approach is to deﬁne Gbased'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 76, 'page_label': '70'}, page_content='70 Generative Models\\non heuristically designed patterns. For example, a widely- used pattern involves having Gcover a\\nwindow of tokens located near position i[Parmar et al. , 2018].\\nWhile sparse attention reduces the computation through the use of sparse operations, such\\nmodels still have signiﬁcant limitations as we must keep the entire KV cache (i.e., K≤i and V≤i)\\nduring inference. If the sequence is very long, storing this cache will become highly memory-\\nintensive. T o address this, we can consider a different form of attention models where the KV\\ncache is not explicitly retained. Linear attention is one su ch approach [ Katharopoulos et al. , 2020].\\nIt uses a kernel function φ(·) to project each query and key onto points q′\\ni = φ(qi ) and k′\\ni = φ(ki ),\\nrespectively . By removing the Softmax function under such t ransformations10 , the form of the\\nresulting attention model is given by\\nAttqkv(qi ,K≤i,V≤i ) ≈ Attlinear(q′\\ni ,K′\\n≤i,V≤i )\\n= q′\\niµi\\nq′\\niνi\\n(2.49)\\nwhere µi and νi are variables that are computed in the recurrent forms\\nµi = µi−1 + k′T\\nivi (2.50)\\nνi = νi−1 + k′T\\ni (2.51)\\nµi and νi can be seen as representations of the history up to position i. A beneﬁt of this model is\\nthat we need not keep all past queries and values. Instead onl y the latest representations µi and\\nνi are used. So the computational cost of each step is a constant , and the model can be easily\\nextended to deal with long sequences.\\nIn fact, this sequential approach to long sequence modeling arises naturally when we adopt a\\nviewpoint of recurrent models. Such models read one token (o r a small number of tokens) at a\\ntime, update the recurrent state using these inputs, and the n discard them before the next token\\narrives. The output at each step is generated based only on th e recurrent state, rather than on all the\\nprevious states. The memory footprint is determined by the r ecurrent state which has a ﬁxed size.\\nRecurrent models can be used in real-time learning scenario s where data arrives in a stream and\\npredictions can be made at any time step. In NLP , applying rec urrent models to language mod-\\neling is one of the earliest successful attempts to learn rep resentations of sequences. Although\\nTransformer has been used as the foundational architecture in LLMs, recurrent models are still\\npowerful models, especially for developing efﬁcient LLMs. More recently , recurrent models have\\nstarted their resurgence in language modeling and have been reconsidered as a promising alterna-\\ntive to Transformers [\\nGu and Dao , 2023]. Figure 2.5 shows a comparison of the models discussed\\nin this subsection.\\n10 In the new space after this transformation, the Softmax norm alization can be transformed into the simple scaling\\nnormalization.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 77, 'page_label': '71'}, page_content='2.3 Long Sequence Modeling 71\\nqikiki−1ki−2· · ·k1k0\\nvivi−1vi−2· · ·v1v0\\nAttqkv(qi ,K≤i,V≤i )\\n(a) Standard Self-attention\\nqikiki−1ki−2· · ·k1k0\\nvivi−1vi−2· · ·v1v0\\nAttqkv(qi ,{k1,ki },{v1,vi })\\n(b) Sparse Attention\\nqikiki−1ki−2· · ·k1k0\\nvivi−1vi−2· · ·v1v0\\nνi = νi−1 + k′T\\ni\\nµi = µi−1 + k′T\\nivi\\nνi\\nµ i⇒\\n⇒\\nAttlinear (qi ,K≤i,V≤i ) = q′\\niµ i\\nq′\\niν i\\n(c) Linear Attention\\nhihi−1hi−2hi−3· · ·h1h0\\ninputi\\nhi = f(hi−1 ,inputi)\\n(d) Recurrent Models\\nFig. 2.5: Illustrations of self-attention, sparse attention, linea r attention and recurrent models. Blue boxes = cached\\nstates for producing the output at position i. f(·) = a recurrent cell.\\n2.3.3 Cache and Memory\\nLLMs based on the standard Transformer architecture are global models. The inference for these\\nmodels involves storing the entire left-context in order to make predictions for future tokens. This\\nrequires a KV cache where the representations (i.e., keys an d values) of all previously-generated'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 78, 'page_label': '72'}, page_content='72 Generative Models\\ntokens are kept, and the cost of caching grows as the inferenc e proceeds. Above, we have dis-\\ncussed methods for optimizing this cache via efﬁcient atten tion approaches, such as sparse atten-\\ntion and linear attention. Another idea, which may have over lap with the previous discussion, is\\nto explicitly encode the context via an additional memory mo del.\\n2.3.3.1 Fixed-size KV Cache\\nA straightforward approach is to represent the keys and values using a ﬁxed-size memory model.\\nSuppose we have a memory Mem which retains the contextual information. W e can write the\\nattention operation at position iin a general form\\nAtt(qi ,Mem) = Att qkv(qi ,K≤i,V≤i ) (2.52)\\nIn this model, Mem is simply the KV cache, i.e., Mem = ( K≤i,V≤i ). Thus the size of\\nMem is determined by i. If we deﬁne Mem as a ﬁxed-size variable, then the cost of performing\\nAtt(qi ,Mem) will be ﬁxed. There are several alternative ways to design Mem.\\n• One of the simplest methods is to consider a ﬁxed-size windo w of previous keys and values.\\nMem is therefore given by\\nMem = ( K[i−nc +1,i ],V[i−nc +1,i ]) (2.53)\\nwhere nc denotes the size of the window . The notation K[i−nc+1,i ] and V[i−nc+1,i ] denote\\nthe keys and values over positions from i− nc + 1 to i.\\n11 This model can be seen as a type\\nof local attention model.\\n• It is also possible to deﬁne Mem as a pair of summary vectors, which leads to a more\\ncompressed representation of the history . A simple way to su mmarize the previous keys\\nand values is to use the moving average of them. For example, Mem can be deﬁned as the\\nunweighted moving average of the previous nc keys and values\\nMem =\\n( ∑ i\\nj=i−nc+1 kj\\nnc\\n,\\n∑ i\\nj=i−nc+1 vj\\nnc\\n)\\n(2.54)\\nAlternatively , we can use a weighted version of moving avera ge\\nMem =\\n( ∑ i\\nj=i−nc+1 βj−i+nc kj\\n∑ nc\\nj=1 βj\\n,\\n∑ i\\nj=i−nc+1 βj−i+nc vj\\n∑ nc\\nj=1 βj\\n)\\n(2.55)\\nHere {β1 ,...,βnc } are the coefﬁcients, which can be either learned as model par ameters\\nor determined via heuristics. For example, they can be set to increasing coefﬁcients (i.e.,\\nβ1 < β2 < ...<βnc −1 <βnc ) in order to give larger weight to positions that are closer t o\\n11 More formally , we write K[i−nc +1,i ] =\\n\\uf8ee\\n\\uf8ef\\n\\uf8f0\\nki−nc +1\\n.\\n.\\n.\\nki\\n\\uf8f9\\n\\uf8fa\\n\\uf8fband V[i−nc +1,i ] =\\n\\uf8ee\\n\\uf8ef\\n\\uf8f0\\nvi−nc +1\\n.\\n.\\n.\\nvi\\n\\uf8f9\\n\\uf8fa\\n\\uf8fb. Sometimes we denote\\nK[i−nc +1,i ] by {ki−nc +1 ,..., ki } and V[i−nc +1,i ] by {vi−nc +1 ,..., vi } for notation simplicity .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 79, 'page_label': '73'}, page_content='2.3 Long Sequence Modeling 73\\ni. W e can extend the moving average to include all the position s up to i. This leads to the\\ncumulative average of the keys and values, given in the form\\nMem =\\n( ∑ i\\nj=0 kj\\ni+ 1 ,\\n∑ i\\nj=0 vj\\ni+ 1\\n)\\n(2.56)\\nIn general, the cumulative average can be written using a rec ursive formula\\nMemi = (ki ,vi) + i· Memi−1\\ni+ 1 (2.57)\\nwhere Memi and Memi−1 denote the cumulative averages of the current and previous p o-\\nsitions, respectively . An advantage of this model is that we only need to store a single\\nkey-value pair during inference, rather than storing all th e key-value pairs. Note that the\\nabove memory models are related to recurrent models, and mor e advanced techniques have\\nbeen used to develop alternatives to self-attention mechan isms in Transformers [ Ma et al. ,\\n2023].\\n• The memory Mem can also be a neural network. At each step, it takes both the pr evious\\noutput of the memory and the current states of the model as inp ut, and produces the new\\noutput of the memory . This neural network can be formulated a s the function\\nMem = Update( Skv,Mempre) (2.58)\\nHere Mem and Mempre represent the outputs of the memory at the current step and th e\\nprevious step, respectively . Skv is a set of key-value pairs, representing the recent states o f\\nthe model. This formulation is general and allows us to devel op various memory models by\\nselecting different Update(·) and Skv conﬁgurations. For example, if Skv only contains the\\nlatest key-value pair (ki ,vi) and Update(·) is deﬁned as a recurrent cell, then Eq. ( 2.58)\\ncan be expressed as an RNN-like model\\nMem = f((ki,vi ),Mempre) (2.59)\\nwhere f(·) is a recurrent cell. Recurrence can also be applied to segmen t-level modeling\\nfor efﬁciency consideration. A simple approach is that we ca n divide the sequence into\\nsegments, and treat Skv as a segment. Applying recurrent models to Update(·) will result in\\nmemory models that operate on segments. A special example is that we deﬁne Update(·) as\\nan FIFO function that adds Skv into the memory and removes the oldest key-value segment\\nfrom the memory , given by\\nMem = FIFO( Skv,Mempre ) (2.60)\\nConsider a memory which includes two segments, one for curre nt segment, and one for the\\nprevious segment. In the attention operation, each positio n can access the history key-value\\npairs in two closest consecutive segments. This essentiall y deﬁnes a local memory , but it\\nand its variants have been widely used segment-level recurr ent models [ Dai et al. , 2019;\\nHutchins et al. , 2022; Bulatov et al. , 2022].\\n• The above memory models can be extended to involve multiple memories. An example'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 80, 'page_label': '74'}, page_content='74 Generative Models\\nof this approach is compressive Transformer [ Rae et al. , 2019]. It employs two distinct\\nﬁxed-size memories: one for modeling local context (denote d by Mem), and the other for\\nmodeling and compressing long-term history (denoted by CMem). The KV cache in this\\nmodel is the combination of Mem and CMem. The attention function can be written as\\nAttcom(qi ,Mem,CMem) = Att qkv (qi,[Mem,CMem]) (2.61)\\nwhere [Mem,CMem] is a combined memory of Mem and CMem. As with other segment-\\nlevel models, the compressive Transformer model operates o n segments of the sequence.\\nEach segment is a sequence of ns consecutive tokens, and we denote Sk\\nkv as the key-value\\npairs corresponding to the tokens of the k-th segment. When a new segment arrives, Mem\\nis updated in an FIFO fashion: we append the nc key-value pairs in Sk\\nkv to Mem, and then\\npop the ns oldest key-value pairs from Mem, which is given by\\nMem = FIFO( Sk\\nkv,Mempre ) (2.62)\\nThe popped key-value pairs are then used to update the compre ssive memory CMem. These\\nns key-value pairs are compressed into ns\\nc key-value pairs via a compression network.\\nCMem is an FIFO which appends the compressed ns\\nc key-value pairs to the tail of the\\nqueue, and drops the ﬁrst ns\\nc key-value pairs of the queue. It is given by\\nCMem = FIFO( Ck\\nkv,CMempre) (2.63)\\nwhere Ck\\nkv represents the set of compressed key-value pairs. Implicit in the compressive\\nTransformer model is that local context should be represent ed explicitly with minimal in-\\nformation loss, while long-range context can be more compre ssed.\\n• W e have already seen that both global and local contexts are useful and can be mod-\\neled using attention models. This view motivates the extens ion to attention models for\\ncombining both local and long-term memories [ Ainslie et al. , 2020; Zaheer et al. , 2020;\\nGupta and Berant , 2020]. A simple but widely-used approach is to involve the ﬁrst fe w to-\\nkens of the sequence in attention, serving as global tokens. This approach is usually applied\\nalong with other sparse attention models. An advantage of in corporating global tokens of\\nthe sequence is that it helps smooth the output distribution of the Softmax function used in\\nattention weight computation, and thus stabilizes model pe rformance when the context size\\nis very large [ Xiao et al. , 2024]. One drawback, however, is that using a ﬁxed-size global\\nmemory may result in information loss. When dealing with lon g sequences, we need to\\nenlarge the KV cache for sufﬁcient representations of the co ntext, but this in turn increases\\nthe computational cost.\\nFigure 2.6 shows illustrations of the above approaches. Note that, whi le we focus on optimiza-\\ntion of the KV cache here, this issue is closely related to tho se discussed in the previous section.\\nAll of the methods we have mentioned so far can broadly be cate gorized as efﬁcient attention\\napproaches, which are widely used in various Transformer va riants.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 81, 'page_label': '75'}, page_content='2.3 Long Sequence Modeling 75\\n· · ·\\n· · ·\\ni i− 1i − 2i − 3i − 4i − 5i − 6i − 7\\nKeys\\nV alues\\nSize = 4 × 2\\nMemory\\n(a) Window-based Cache\\n· · ·\\n· · ·\\ni i− 1i − 2i − 3i − 4i − 5i − 6i − 7\\nKeys\\nV alues\\nSize = 1 × 2\\nMemoryki−3 +ki−2 +ki−1 +ki\\n4 ⇒\\nvi−3 +vi−2 +vi−1 +vi\\n4 ⇒\\n(b) Moving A verage-based Cache\\n· · ·\\n· · ·\\ni i− 1i − 2i − 3i − 4i − 5i − 6i − 7\\nKeys\\nV alues\\nSize = 1 × 2\\nMemoryMem = Update( Skv ,Mempre ) ⇒\\n(c) Recurrent Network as Cache\\n· · ·\\n· · ·\\ni i− 1i − 2i − 3i − 4i − 5i − 6i − 7\\nKeys\\nV alues\\nSize = 4 × 2\\nMemory\\nSize = 2 × 2\\nMemory\\nCompressed\\n(d) Hybrid Cache (Compressed Memory + Local Memory)\\nFig. 2.6: Illustrations of ﬁxed-size KV caches in LLMs. Blue boxes rep resent the keys and values generated during\\nLLM inference, green boxes represent the keys and values sto red or encoded in the primary memory , and orange boxes\\nrepresent the keys and values stored or encoded in the compre ssed memory .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 82, 'page_label': '76'}, page_content='76 Generative Models\\n2.3.3.2 Memory-based Models\\nThe modeling of memories discussed above was based on updates to the KV cache, and the re-\\nsulting models are typically referred to as internal memories . W e now consider another family\\nof models, called external memories , which operate as independent models to access large-scale\\ncontexts for LLMs. Many such models are based on memory-based methods which have been\\nextensively discussed in machine learning [\\nBishop, 2006]. A common example is nearest neigh-\\nbor algorithms: we store context representations in a datas tore, and try to ﬁnd the most similar\\nstored representations to match a given query . The retrieve d context representations are then used\\nto improve attention for this query .\\nHere, we consider the k-nearest neighbors (k-NN) method which is one of the most popular\\nmemory-based methods. Since our focus is language modeling in this section, we deﬁne a sample\\nin the datastore as a key-value pair corresponding to some co ntext state. Note that “context” is a\\nbroad concept here, not just a sequence preﬁx in text generat ion. One might, for example, view\\nthe entire dataset as the context for predicting tokens. Thi s allows us to retrieve the closest context\\nsituation in a set of sequences, rather than a given sequence preﬁx. Although we will restrict\\nourselves to context modeling for a single sequence, in this subsection, we discuss a relatively\\nmore general case.\\nSuppose we have a set of keys {kj } with corresponding values {vj }, and suppose we store\\nthese key-value pairs in a vector database 12 . For each query qi, we ﬁnd its knearest neighbours by\\ngrowing the radius of the sphere centered as qi until it contains kdata points in {kj }. This results\\nin a set of k keys along with their corresponding values, denoted by Memknn. As before, we\\ndenote Mem as the local memory for the query , such as the KV cache of neigh boring tokens. Our\\ngoal is to attend query qi to both the local memory Mem and the long-term memory Memknn.\\nThere are, of course, several ways to incorporate Mem and Memknn into the attention model.\\nFor example, we might simply combine them to form a single KV c ache [Mem,Memknn], and\\nattend qi to [Mem,Memknn] via standard QKV attention. Or we might use Mem and Memknn\\nin separate attention steps. An example of such approaches i s the model developed by Wu et al.\\n[2021]. It linearly combines the two types of attention, given by\\nAtt(qi ,Mem,Memknn) = g ⊙ Attlocal + (1 − g) ⊙ Attknn (2.64)\\nAttlocal = Att( qi ,Mem) (2.65)\\nAttknn = Att( qi ,Memknn) (2.66)\\nHere g ∈ Rd is the coefﬁcient vector, which can be the output of a learned gate.\\nGiven the k-NN-based memory model described above, the remaining task is to determine\\nwhich key-value pairs are retained in the datastore. For sta ndard language modeling tasks, we\\nconsider the previously seen tokens in a sequence as the cont ext, so we can add the keys and\\nvalues of all these tokens into the datastore. In this case, t he resulting k-NN-based attention\\nmodel is essentially equivalent to a sparse attention model [Gupta et al. , 2021].\\nAlternatively , we can extend the context from one sequence t o a collection of sequences.\\nFor example, we might collect all key-value pairs across the sequences in a training dataset and\\nadd them to the datastore to model a larger context. Thus, LLM s can predict tokens based on a\\n12 A vector database, or vector store, is a database that provid es highly optimized retrieval interfaces for ﬁnding stored\\nvectors that closely match a query vector.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 83, 'page_label': '77'}, page_content='2.3 Long Sequence Modeling 77\\ngeneralized context. A problem with this approach is that th e computational cost would be large\\nif many sequences are involved. Since these sequences are pa rt of our training data, we can build\\nand optimize an index for the vectors in the datastore before running the LLMs. As a result, the\\nretrieval of similar vectors can be very efﬁcient, as in most vector databases.\\nIn fact, all the above-mentioned methods can be viewed as ins tances of a retrieval-based ap-\\nproach. Instead of using retrieval results to improve atten tion, we can apply this approach in other\\nways as well. One application of k-NN-based search is k-NN language modeling (or k-NN LM )\\n[Khandelwal et al. , 2020]. The idea is that, although it is attempting to extend the co ntext used\\nin self-attention by incorporating nearest neighbors in re presentation learning, in practice, similar\\nhidden states in Transformers are often highly predictive o f similar tokens in subsequent positions.\\nIn k-NN LM, each item in the datastore is a key-value tuple (z,w), where z represents a hidden\\nstate of the LLM at a position, and w represents the corresponding prediction. A typical way to\\ncreate the datastore is to collect the output vector of the Tr ansformer layer stack and the corre-\\nsponding next token for each position of each sequence in a tr aining dataset. During inference,\\nwe have a representation hi given a preﬁx. Given this representation, we ﬁrst search the datastore\\nfor k closest matching data items {(z1 ,w1 ),...,(zk ,wk )}. Here {w1 ,...,wk } are thought of as\\nreference tokens for prediction, and thus can be used to guid e the token prediction based on hi.\\nOne common way to make use of reference tokens is to deﬁne a dis tribution over the vocabulary\\nV,\\nPrknn(·|hi ) = Softmax(\\n[\\n−d0 · · · − d|V |\\n]\\n) (2.67)\\nwhere dv equals the distance between hi and zj if wj equals the v-th entry of V, and equals 0\\notherwise. W e use a linear function with a coefﬁcient λthat interpolates between the retrieval-\\nbased distribution Prknn(·|hi ) and the LLM output distribution Prlm(·|hi )\\nPr(·|hi ) = λ· Prknn(·|hi ) + (1 − λ) · Prlm(·|hi ) (2.68)\\nThen, as usual, we can choose the next token yby maximizing the probability Pr(y|hi ).\\nAs with information retrieval (IR) systems, the datastore c an also manage texts and provide\\naccess to relevant texts for a query . For example, we can stor e a collection of text documents\\nin a search engine with full-text indexing, and then search i t for documents that match a given\\ntext-based query . Applying IR techniques to LLMs leads to a g eneral framework called retrieval-\\naugmented generation (RAG). The RAG framework works as follows. W e use the context x as\\nthe query and ﬁnd the kmost relevant document pieces {c1 ,...,ck } from the datastore via efﬁcient\\nIR techniques 13 . These search results are combined with the original contex t via a prompting\\n13 In piratical applications, queries are typically generate d using a query generation system, which may expand it\\nwith variations of tokens and query intent.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 84, 'page_label': '78'}, page_content='78 Generative Models\\ntemplate g(·)14 , resulting in an augmented input for the LLM\\nx′ = g(c1 ,...,ck ,x) (2.69)\\nThen, we use x′ as the context and predict the following text using the model Pr(y|x′ ). One\\nadvantage of RAG is that we need not modify the architecture o f LLMs, but instead augment the\\ninput to LLMs via an additional IR system. Figure 2.7 shows a comparison of the use of different\\nexternal memories in LLMs.\\n2.3.3.3 Memory Capacity\\nA memory model in LLMs, in the form of a simple key-value cacheor a datastore, can broadly\\nbe seen as an encoder of contextual information. Ideally , be fore we say that a memory model\\nis representative of the entire context in token prediction , we need to make sure that the model\\ncan accurately represent any part of the context. The standa rd KV cache is one such model that\\ncompletely stores all past history . In this case, the model i s said to have adequate capacity for\\nmemorizing the context. In many practical applications, ho wever, complete memorization is not\\nrequired. Instead, the goal is to enable LLMs to access impor tant contextual information. As a\\nresult, efﬁcient and compressed memory models are develope d, as described in this section. Note\\nthat, the longer the sequence, the more difﬁcult it becomes f or a low-capacity memory model to\\ncapture important contextual information. It is therefore common practice to simply increase the\\nmodel capacity when processing long contexts.\\nWhile high-capacity models are generally favorable, they a re difﬁcult to train and deploy . A\\nchallenging scenario is that the tokens arrive in a stream an d the context continuously grows.\\nDeveloping LLMs for such tasks is difﬁcult as we need to train Transformers on extremely long\\nsequences. A possible way to address this difﬁculty is to use non-parametric methods, such as\\nretrieval-based methods. For example, as discussed above, we can use a vector database to store\\npreviously generated key-value pairs, and thus represent t he context by this external memory\\nmodel. Although this approach side-steps the challenge of r epresenting long context in Trans-\\nformers, building and updating external memory models are c omputationally expensive. These\\nmodels are more often used in problems where the context is gi ven in advance and ﬁxed during\\ninference, and hence unsuitable for streaming context mode ling.\\nIn cases where the size of the context continuously grows, ap plying ﬁxed-size memory models\\nis a commonly used approach. For example, in recurrent model s, a sequence of arbitrary length\\ncan be summarized into a set of hidden states by which we have a ﬁxed computational cost per\\nstep. While recurrent models were initially found to be not v ery good at handling long-distance\\ndependencies in sequence modeling in early applications of deep learning to NLP , recent advance-\\nments have shown that their variants are now effective in mod eling extremely long sequences.\\n[\\nBulatov et al. , 2022; Hutchins et al. , 2022; Munkhdalai et al. , 2024; Ma et al. , 2024].\\n14 For example, the template could be:\\nmessage = {* c1 *} ... {* ck *}\\ninput: {* x*}\\noutput:'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 85, 'page_label': '79'}, page_content='2.3 Long Sequence Modeling 79\\n· · ·\\n· · ·\\nDatastore\\nqi\\nSearch\\nKV Cachek Nearest\\nNeighbors\\nAtt(qi , Mem)Att(qi , Memknn )\\ng ⊙ Att(qi , Mem) + (1 − g) ⊙ Att(qi , Memknn )\\nKeys/values in LLM\\nKeys/values in Datastore\\n(a) k-NN Search Augmented Attention\\n· · ·\\n· · ·\\nDatastore\\nqi\\nSearch\\nKV Cachek Nearest\\nNeighbors\\nAtt(qi , Mem) Att(qi , Mem)\\n· · ·\\nDistribution Pr(·)\\nDistribution Prknn (·)\\nOutput Distribution\\nKeys/values in LLM\\nKeys in Datastore\\nPredicted T okens\\n(b) k-NN Language Modeling\\nInput Context:\\nx=\\nWhat is deep learning?\\nDatastore Search\\n· · ·\\nc2 = Machine learning is ...\\nc1 = Deep network is ...\\nk Nearest\\nNeighbors\\nWhat is deep learning?\\nMessage: deep network ... machine learning ...\\nLLM\\n(c) Retrieval-augmented Generation\\nFig. 2.7: Illustrations of external memories (or datastores) for lan guage modeling.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 86, 'page_label': '80'}, page_content='80 Generative Models\\nThere is no general deﬁnition of memory capacity in LLMs. A si mple approach might consider\\nhow much storage is used to retain contextual information. F or example, memory capacity could\\nbe deﬁned by the size of the KV cache in Transformers or the vec tor database used in retrieval-\\nbased methods. A related concept is model complexity . In mac hine learning, there are several\\nways to deﬁne the model complexity of a model. One of the simpl est methods is by counting the\\nnumber of parameters. However, it should be emphasized that the memory models discussed here\\nprimarily serve to store information, rather than add train able parameters. Therefore, a model with\\na large memory capacity is not necessarily more complex. Nev ertheless, in practice determining\\nthe capacity of a memory model is not straightforward. In gen eral, we need to control the trade-off\\nbetween maximizing the performance and controlling the mem ory footprint.\\n2.3.4 Sharing across Heads and Layers\\nIn Transformers, the KV cache is a data structure that can be dynamically adjusted along multiple\\ndimensions, such as heads, layers, and sequence length. For example, consider an LLM with L\\nlayers. Each layer has τ attention heads, and each head produces a dh-dimensional output. During\\ninference, we store the keys and values for up to mtokens. The space complexity of this caching\\nmechanism is O(L· τ · dh · m). As we have seen previously , this complexity can be reduced b y\\ncaching the keys and values for fewer tokens. For example, in sliding window attention, a ﬁxed-\\nsize window is used to cache the keys and values in local conte xt. And this model has a space\\ncomplexity of O(L· τ · dh · mw ), with mw being the size of the window .\\nIn addition to reducing m, we can also decrease the size of the KV cache along other di-\\nmensions. A widely-used approach is to enable sharing acros s heads in multi-head self-attention.\\nRecall from Section\\n2.1.1 that multi-head self-attention uses multiple sets of queri es, keys, and\\nvalues (each set is called a head), each performing the QKV at tention mechanism as usual. This\\ncan be expressed as\\nOutput = Merge(head 1,...,headτ )Whead (2.70)\\nwhere headj ∈ Rdh is computed using the standard QKV attention function\\nheadj = Att qkv (q[j]\\ni ,K[j]\\n≤i,V[j]\\n≤i ) (2.71)\\nHere, q[j]\\ni , K[j]\\n≤i, and V[j]\\n≤i are the query , keys, and values that are projected onto the j-th feature\\nsub-space. So this model can be interpreted as performing at tention on a group of feature sub-\\nspaces in parallel (see Figure 2.8 (b)). The KV cache needs to retain the keys and values for all\\nthese heads, that is, {(K[1]\\n≤i,V[1]\\n≤i ),...,(K[τ ]\\n≤i ,V[τ ]\\n≤i )}.\\nOne reﬁnement to the multi-head attention model, called multi-query attention (MQA), is to\\nshare keys and values across heads, while allowing queries t o be unique for each head [\\nShazeer,\\n2019]. In MQA, there is a single set of keys and values (K≤i,V≤i). In addition, there are τ\\nqueries {q[1]\\ni ,...,q[τ ]\\ni }, each corresponding to a different head. For each head, we ha ve\\nheadj = Att qkv (q[j]\\ni ,K≤i,V≤i ) (2.72)\\nFigure 2.8 (c) illustrates this model. By sharing keys and values, the s ize of the KV cache would'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 87, 'page_label': '81'}, page_content='2.3 Long Sequence Modeling 81\\nquerykeyvalue\\n(a) Single-head Attention\\nquerykeyvalue\\n(b) Multi-head Attention\\nquerykeyvalue\\n(c) Multi-query Attention\\nquerykeyvalue\\n(d) Grouped Query Attention\\nquerykeyvalue\\nLayer l\\nLayer l− 1\\nSharing\\n(e) Cross-layer Multi-head Attention\\nFig. 2.8: Illustration of QKV attention based on different multi-hea d and sharing mechanisms. (a) = single-head\\nattention, and (b-e) = attention with multiple heads.\\nbeO(L· dh · m).\\nGrouped query attention (GQA) is a natural extension to multi-head attention and MQA\\n[\\nAinslie et al. , 2023]. In GQA, heads are divided into ng groups, each corresponding to a shared\\nset of keys and values. Hence we have ng sets of keys and values {(K[1]\\n≤i,V[1]\\n≤i ),...,(K[ng ]\\n≤i ,V[ng ]\\n≤i )}.\\nSee Figure\\n2.8 (d) for an illustration. Let g(j) be the group id for the j-th head. The GQA model\\ncan be expressed as\\nheadj = Att qkv(q[j]\\ni ,K[g(j)]\\n≤i ,V[g(j)]\\n≤i ) (2.73)\\nThe size of the KV cache of GQA is O(L· ng · dh · m). One beneﬁt of GQA is that we can trade-off\\nbetween computational efﬁciency and model expressiveness by adjusting ng . When ng = τ, the\\nmodel becomes the standard multi-head attention model. By c ontrast, when ng = 1 , it becomes'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 88, 'page_label': '82'}, page_content='82 Generative Models\\nthe GQA model.\\nSharing can also be performed across layers. Such a method fa lls into the family of shared\\nweight and shared activation methods, which have been exten sively used in Transformers [ Dehghani et al. ,\\n2018; Lan et al. , 2020]. For example, one can share KV activations or attention wei ghts across\\nlayers to reduce both computation and memory footprints [ Xiao et al. , 2019; Brandon et al. , 2024].\\nFigure 2.8 (e) shows an illustration of this method, where a query in a la yer directly accesses the\\nKV cache of a lower-level layer.\\n2.3.5 Position Extrapolation and Interpolation\\nSince Transformer layers are order-insensitive to input, we need some way to encode positional\\ninformation in the input tokens. T o do this, it is common to ad d positional embeddings to token\\nembeddings, and then feed these combined embeddings into th e Transformer layer stack as input.\\nIn this case, the embedding at position ican be expressed as\\nei = xi + PE( i) (2.74)\\nwhere xi ∈ Rd denotes the token embedding, and PE(i) ∈ Rd denotes the positional embedding.\\nIn general, the token embedding xi is a position-independent vector, and so the positional emb ed-\\nding PE(i) is used to encode the positional context. A straightforward approach is to treat PE(i)\\nas a learnable variable and train it alongside other model pa rameters. In this way , we can learn\\na unique representation for each position, and thus disting uish the tokens appearing at different\\npositions of a sequence.\\nRepresentations of positions using learned vectors can wor k well in tasks where the sequences\\nat training and test times are of similar lengths. In practic e, however, we often impose length\\nrestrictions on sequences during training to prevent exces sive computational costs, but wish to\\napply the trained models to much longer sequences during inf erence. In this case, using learned\\npositional embeddings has obvious drawbacks, as there are n o trained embeddings for positions\\nthat are not observed in the training phase.\\nAn alternative approach to modeling positional informatio n is to develop positional embed-\\ndings that can generalize: once trained, the embedding mode l can be used to handle longer se-\\nquences. Suppose that we train a positional embedding model on sequences with a maximum\\nlength of ml , and we wish to apply the trained model to a sequence of length m(m >> ml ). If\\nthe embedding model is limited in the range of positions that we can observe from training data,\\nthen this model will simply fail to deal with new data outside that range. See Figure\\n2.9 (a) for\\nan illustration where the learned embedding model cannot mo del data points outside the training\\ndomain if it lacks the ability to extrapolate.\\nThere are several approaches to making positional embeddin g models generalize. They can\\nbe grouped into two classes.\\n• Extrapolation. The model learned on observed data points (i.e., positions ) can be directly\\nemployed to assign meaningful values to data points beyond t he original range. For ex-\\nample, suppose we have a series of numbers 1,2,...,10, and we want to understand the\\nmeaning of a new number, 15. Knowing that these numbers are na tural numbers used for\\nordering, we can easily infer that 15 is a number that follows 10, even though 15 has not'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 89, 'page_label': '83'}, page_content='2.3 Long Sequence Modeling 83\\n0 1 ,024 2 ,048\\n−1\\n0\\n1\\nSequence Length\\n(a) Encoding with No Generalization\\nV alue\\n0 1 ,024 2 ,048\\n−1\\n0\\n1\\nSequence Length\\n(b) Extrapolation\\nV alue\\n0 1 ,024 2 ,048\\n−1\\n0\\n1\\nSequence Length\\n(c) Interpolation\\nV alue\\nFig. 2.9: Illustrations of different positional embedding methods f or a range of positions. Blue points represent the\\npositions that have been observed during training, and red p oints represent the positions that are newly observed at tes t\\ntime. In sub-ﬁgure (a), the encoding model only memorizes th e points seen during training, and cannot generalize. In\\nsub-ﬁgures (b) and (c), the model can generalize through ext rapolation and interpolation.\\nbeen observed before. Figure 2.9 (b) shows an example of this approach, where a function\\nis learned to ﬁt the data points within a speciﬁc range and the n applied to estimate the values\\nof data points outside that range.\\n• Interpolation. This approach maps a larger range of data points into the ori ginal obser-\\nvation range. For example, suppose we have a model designed f or numbers in the range\\n[1,10]. When given a new range of [1,20], we can scale this down by dividing every num-\\nber by 2, thereby ﬁtting all numbers into [1,10]. This scaling allows us to use the model\\ntrained on the range [1,10] to describe data points in the expanded range of [1,20]. See\\nFigure 2.9 (c) for an illustration of this approach.\\nIn fact, positional embeddings in many systems have achieve d some level of generalization.\\nFor example, sinusoidal encoding, the most common position al embedding method, employs sine\\nand cosine functions that can naturally extend to sequences of any length. Although this approach\\nmight seem direct and simple, it does not perform well when we signiﬁcantly extend the sequences\\nfor processing. In this subsection, we will discuss several alternative methods based on either\\nextrapolation or interpolation.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 90, 'page_label': '84'}, page_content='84 Generative Models\\n2.3.5.1 Attention with Learnable Biases\\nOne problem with Eq. (\\n2.74) is that the embedding model treats each token independentl y and\\ntherefore ignores the distance between different tokens. A common improvement to this model,\\ncalled relative positional embedding, is to consider the pa irwise relationship between tokens\\n[Shaw et al. , 2018]. The general idea behind this is to obtain the offset betwee n any pair of posi-\\ntions and incorporate it into the self-attention model. One of the simplest forms of self-attention\\nwith relative positional embedding is given by\\nAttqkv (qi,K≤i ,V≤i) =\\ni∑\\nj=0\\nα(i,j)vj (2.75)\\nα(i,j) = Softmax( qi kT\\nj + PE(i,j)√\\nd\\n+ Mask( i,j)) (2.76)\\nThe only difference between this model and the original self -attention model is that a bias term\\nPE(i,j) is added to the query-key product in this new model. Intuitiv ely , PE(i,j) can be inter-\\npreted as a distance penalty for the pair of positions iand j. As imoves away from j, the value of\\nPE(i,j) decreases.\\nPE(i,j) can be deﬁned in several different ways. Here, we consider th e T5 version of relative\\npositional embedding, called the T5 bias [ Raffel et al. , 2020]. For each pair of query qi and key\\nkj , the offset between them is deﬁned to be 15\\nd(i,j) = i− j (2.77)\\nA simple design for the bias PE(i,j) is to share the same learnable variable for all query-key\\npairs with the same offset, i.e., PE(i,j) = ui−j , where ui−j is the variable corresponding to\\nthe offset i− j. However, simply assigning a unique value to each offset wil l restrict this model\\nto observed offsets. When i− j is larger than the maximum trained offset, the model cannot\\ngeneralize.\\nThe T5 bias instead adopts a generalization of this model. Ra ther than assigning each query-\\nkey offset a unique bias term, it groups difference offsets i nto “buckets”, each corresponding to\\none learnable parameter. More speciﬁcally , the bias terms f or nb + 1 buckets are given as follows.\\n• For buckets 0 to nb +1\\n2 − 1, each bucket corresponds to one offset, that is, bucket 0 ↔ offset\\n0, bucket 1 ↔ offset 1, bucket 2 ↔ offset 2, and so on. W e express this as b(i − j) = i− j.\\n• For buckets nb +1\\n2 to nb, the size of each bucket increases logarithmically . For exa mple, the\\nbucket number for a given offset i− j ≥ nb +1\\n2 can be deﬁned as\\nb(i− j) = nb + 1\\n2 + ⌊ log(i− j) − log( nb +1\\n2 )\\nlog(distmax ) − log( nb +1\\n2 ) · nb + 1\\n2 ⌋ (2.78)\\nwhere the parameter distmax is typically set to a relatively large number to indicate the\\n15 For language modeling, a query is only allowed to attend to it s left-context, and so we have i− j ≥ 0. In the more\\ngeneral case of self-attention, where a token can attend to a ll tokens in the sequence, we may have negative offsets\\nwhen i<j .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 91, 'page_label': '85'}, page_content='2.3 Long Sequence Modeling 85\\n0 1 2 3 · · · 14 15 16 17 18 · · · 32Bucket\\nOffset\\n(i − j)\\n0 1 2 3 14 15 16 ∼ 20 21 ∼ 26 27 ∼ 33 802 ∼ ∞\\nﬁxed bucket size logarithmically increased bucket size\\nFig. 2.10: Illustration of distributing query-key offsets into bucke ts in the T5 model ( nb = 32 and distmax = 1024 ).\\nBoxes represent buckets. In the ﬁrst half of the buckets, we u se a ﬁxed bucket size. In the second half of the buckets,\\nwe increase the bucket size logarithmically . The last bucke t contains all the query-key offsets that are not covered by\\nprevious buckets.\\nmaximum offset we may encounter.\\n• When i− j >distmax , we place i− jin the last bucket. In other words, bucket nb contains\\nall the offsets that are not assigned to the previous buckets .\\nT ogether, these can be expressed as the function\\nb(i− j)\\n=\\n\\uf8f1\\n\\uf8f4\\n\\uf8f2\\n\\uf8f4\\n\\uf8f3\\ni− j 0 ≤ i− j <nb +1\\n2\\nmin(nb,nb +1\\n2 + ⌊\\nlog(i−j)−log( nb +1\\n2 )\\nlog(distmax )−log( nb +1\\n2 ) · nb +1\\n2 ⌋) i− j ≥ nb +1\\n2\\n(2.79)\\nFigure 2.10 shows an illustration of these buckets. W e see that in the ﬁrs t half of the buckets,\\neach bucket is associated with only one value of i− j, while in the second half, the bucket size\\nincreases as i− j grows. The last bucket is designed to handle sequences of arb itrarily long\\nlengths.\\nAll PE(i,j)s in a bucket share the same bias term ub(i−j). Substituting PE(i,j) = ub(i−j)\\ninto Eq. ( 2.76), the attention weight for qi and kj becomes16\\nα(i,j) = Softmax( qikT\\nj + ub(i−j)√\\nd\\n+ Mask( i,j)) (2.81)\\nThe parameters {u0 ,...,unb } are learned as common parameters during training. It should\\nbe emphasized that this model can generalize to long sequenc es. This is because PE(i,j)s with\\nsimilar query-key offsets share the same parameter, and thi s sharing strategy is particularly im-\\nportant for achieving good generalization, given that larg e query-key offsets are rare in training.\\nIn practice, we often set nb to a moderate number, and thus it can help control the overﬁtt ing of\\npositional embedding models.\\n16 Note that, in Raffel et al. [2020]’s T5 model, the rescaling operation for the query-key prod uct is removed. The\\nattention weight α (i,j ) is then given by\\nα (i,j ) = Softmax( qi kT\\nj + ub(i−j) + Mask( i,j )) (2.80)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 92, 'page_label': '86'}, page_content='86 Generative Models\\n2.3.5.2 Attention with Non-learned Biases\\nRelative positional embedding models are based on a set of learned biases for the query-key prod-\\nuct in self-attention. An alternative approach is to give th ese biases ﬁxed values via heuristics,\\nrather than training them on a particular dataset. One beneﬁ t of this heuristics-based approach is\\nthat it does not rely on a training process and thus can be dire ctly applied to any sequences once\\nthe biases are set.\\nOne example of such an approach is\\nPress et al. [2022]’s approach, called attention with\\nlinear biases or ALiBi for short. In the ALiBi approach, the bias term is deﬁned as th e negative\\nscaled query-key offset\\nPE(i,j) = −β· (i− j)\\n= β· (j− i) (2.82)\\nwhere βis the scaling factor. Adding this term to the query-key prod uct, we obtain a new form of\\nattention weights\\nα(i,j) = Softmax( qi kT\\nj + β· (j− i)√\\nd\\n+ Mask( i,j)) (2.83)\\nThis model can be interpreted as adding a ﬁxed penalty to qikT\\nj whenever j moves one step\\naway from i. So we do not need to adapt it to a range of sequence lengths, an d can employ it to\\nmodel arbitrarily long sequences. See Figure 2.11 for a comparison of the T5 bias and the ALiBi\\nbias.\\nIn general, the scalar β should be tuned on a validation dataset. However, Press et al. [2022]\\nfound that setting βto values decreasing geometrically by a factor of 1\\n2a for multi-head attention\\nperforms well on a variety of tasks. Speciﬁcally , for a self- attention sub-layer involving nhead\\nheads, the scalar for the k-th head is given by\\nβk = 1\\n2\\n8\\nk\\n(2.84)\\nThe ALiBi approach provides a simple form of relative positi onal embeddings. There are\\nother similar methods for designing query-key biases using the offset i− j. T able 2.4 shows a\\ncomparison of such biases. As an aside it is worth noting that the form of the right-hand side\\nof Eq. ( 2.82) is very similar to length features used in conventional fea ture-based systems. For\\nexample, in statistical machine translation systems, such features are widely used to model word\\nreordering problems, resulting in models that can generali ze well across different translation tasks\\n[Koehn, 2010].\\n2.3.5.3 Rotary Positional Embedding\\nAs with sinusoidal embeddings, rotary positional embeddings are based on hard-coded values for\\nall dimensions of an embedding [\\nSu et al. , 2024]. Recall that in the sinusoidal embedding model,\\npositions are represented as combinations of sine and cosin e functions with different frequencies.\\nThese embeddings are then added to token embeddings to form t he inputs to the Transformer'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 93, 'page_label': '87'}, page_content='2.3 Long Sequence Modeling 87\\nq0 kT\\n0q1 kT\\n0\\nq1 kT\\n1q2 kT\\n0\\nq2 kT\\n1\\nq2 kT\\n2q3 kT\\n0\\nq3 kT\\n1\\nq3 kT\\n2\\nq3 kT\\n3q4 kT\\n0\\nq4 kT\\n1\\nq4 kT\\n2\\nq4 kT\\n3\\nq4 kT\\n4q5 kT\\n0\\nq5 kT\\n1\\nq5 kT\\n2\\nq5 kT\\n3\\nq5 kT\\n4\\nq5 kT\\n5q6 kT\\n0\\nq6 kT\\n1\\nq6 kT\\n2\\nq6 kT\\n3\\nq6 kT\\n4\\nq6 kT\\n5\\nq6 kT\\n6\\nqikT\\nj u0\\nu1 u0\\nu2 u1 u0\\nu2 u2 u1 u0\\nu3 u2 u2 u1 u0\\nu3 u3 u2 u2 u1 u0\\nu3 u3 u3 u2 u2 u1 u0\\nBias ( ub(i−j))\\n+\\n(a) The T5 bias ( nb = 3 and distmax = 5 )\\nq0 kT\\n0q1 kT\\n0\\nq1 kT\\n1q2 kT\\n0\\nq2 kT\\n1\\nq2 kT\\n2q3 kT\\n0\\nq3 kT\\n1\\nq3 kT\\n2\\nq3 kT\\n3q4 kT\\n0\\nq4 kT\\n1\\nq4 kT\\n2\\nq4 kT\\n3\\nq4 kT\\n4q5 kT\\n0\\nq5 kT\\n1\\nq5 kT\\n2\\nq5 kT\\n3\\nq5 kT\\n4\\nq5 kT\\n5q6 kT\\n0\\nq6 kT\\n1\\nq6 kT\\n2\\nq6 kT\\n3\\nq6 kT\\n4\\nq6 kT\\n5\\nq6 kT\\n6\\nqikT\\nj 0\\n−1β 0\\n−2β −1β 0\\n−3β −2β −1β 0\\n−4β −3β −2β −1β 0\\n−5β −4β −3β −2β −β 0\\n−6β −5β −4β −3β −2β −β 0\\nBias ( −β (i − j))\\n+\\n(b) The ALiBi bias\\nFig. 2.11: Query-key products with biases (above = the T5 bias and below = the ALiBi bias). The color scale of the\\nbiases ranges from light blue denoting small absolute value s to deep blue denoting large absolute values.\\nlayer stack. Rotary positional embeddings instead model po sitional context as rotations to token\\nembeddings in a complex space. This leads to a model expresse d in the form of multiplicative\\nembeddings\\nei = xiR(i) (2.85)\\nwhere R(i) ∈ Rd×d is the rotation matrix representing the rotations performe d on the token\\nembedding xi ∈ Rd.\\nFor simplicity , we will ﬁrst consider embeddings with only t wo dimensions and return to a\\ndiscussion of the more general formulation later. Suppose w e have a 2-dimensional token embed-\\nding x =\\n[\\nx1 x2\\n]\\n. W e can represent it as a vector in a plane, originating at the origin (0,0)\\nand terminating at (x1 ,x2). A counterclockwise rotation of this vector refers to an ope ration of'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 94, 'page_label': '88'}, page_content='88 Generative Models\\nEntry Query-Key Bias ( PE(i,j))\\nT5 [ Raffel et al. , 2020] ub(i−j)\\nALiBi [ Press et al. , 2022] −β· ( i− j)\\nKerple [ Chi et al. , 2022] −β1 ( i− j)β 2 (power)\\n−β1 log(1 + β2 ( i− j)) (logarithmic)\\nSandwich [ Chi et al. , 2023] ∑ ¯d/ 2\\nk=1 cos\\n(\\n( i− j)/100002k/ ¯d )\\nFIRE [ Li et al. , 2024] f\\n(\\nψ( i− j)/ψ(max(mlen,i))\\n)\\nT able 2.4: Query-key biases as relative positional embeddings. β , β1 , β2 , ¯d, and mlen are hyper-parameters. In the T5\\nmodel, b(i− j) denotes the bucket assigned to i− j. In the FIRE model, ψ (·) is a monotonically increasing function\\nsuch as ψ (x) = log( cx+ 1) , and f(·) is an FFN.\\nmoving the vector around the origin while maintaining its ma gnitude, as shown in Figure 2.12 (a).\\nThe degree of rotation is usually deﬁned by a speciﬁc angle, d enoted by θ. The rotation can be\\nexpressed mathematically in the form\\nRo(x,θ) = xRθ\\n=\\n[\\nx1 x2\\n] [\\ncos θ sin θ\\n− sin θ cos θ\\n]\\n=\\n[\\ncos θ· x1 − sin θ· x2 sin θ· x1 + cos θ· x2\\n]\\n(2.86)\\nwhere Rθ =\\n[\\ncos θ sin θ\\n− sin θ cos θ\\n]\\nis the rotation matrix. If two or more rotations are performe d on the\\nsame vector, we can rotate the vector further. This follows f rom the fact that the composition of\\nsuccessive rotations is itself a rotation. More formally , r otating a vector by an angle θfor ttimes\\ncan be expressed as\\nRo(x,tθ) = xRtθ\\n=\\n[\\ncos tθ· x1 − sin tθ· x2 sin tθ· x1 + cos tθ· x2\\n]\\n(2.87)\\nIf we interpret tas the position of a token represented by x in a sequence, then we will ﬁnd\\nthat the above equation deﬁnes a simple positional embeddin g model. As shown in Figure 2.12\\n(b), we start moving the token from position 0. Each time we mo ve one step forward, the vector\\nis rotated by the angle θ. Upon arriving at the position t, the representation of the token with\\npositional context is given by Ro(x,iθ). As the rotations do not change the magnitude of the\\nembedding, the original “meaning” of the token is retained. The positional information is injected\\ninto the embedding, when it gets rotated.\\nA popular way to understand vector rotation is to deﬁne it in c omplex spaces. It is easy\\nto transform each vector x =\\n[\\nx1 x2\\n]\\nin the 2D Euclidean space R2 to a complex number\\nx′ = x1 + ix2 in the complex space C via a bijective linear map. Then, the rotation of x with the\\nangle tθcorresponds to the multiplication by eitθ . Given that eitθ = cos tθ+ i sin tθ, the rotation'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 95, 'page_label': '89'}, page_content='2.3 Long Sequence Modeling 89\\nx1\\nx2\\nθ\\nvector x\\nxRθ\\nrotated vector\\n(a) Single-step Rotation\\nx1\\nx2\\nθ\\nθ\\nθ\\nxxRθ\\nxR2θ\\nxR3θ\\n(b) Multi-step Rotation\\nx1\\nx2\\n7θ\\n7θ\\nsleeping4\\nsleeping11\\ncat2\\ncat9\\nThe1 cat2 is3 sleeping4 peacefully5\\nin6 the7 warm8 sunlight9 .10\\nEvery1 afternoon2 ,3 you4 ’ll 5 ﬁnd 6 that7\\nthe8 cat9 is10 sleeping11 on12 my13 bed14 .15\\n(c) Angles between embeddings of two tokens at different pos itions\\nFig. 2.12: Illustrations of vector rotations in a plane. Sub-ﬁgures (a ) and (b) show rotations of a vector in a single\\nstep and multiple steps, respectively . Sub-ﬁgure (c) shows the embeddings of tokens cat and sleeping in two different\\nsentences. W e show these sentences with a subscript afﬁxed t o each token to indicate its position. If we represent\\ntokens as vectors, we can add positional information by rota ting these vectors. This rotation preserves the “distances ”\\nbetween the vectors. For example, given that the distance be tween cat and sleeping is the same in both sentences, the\\nangle between their embeddings also remains the same during rotation.\\noperation can be re-expressed in the form\\nxRtθ ↦→ x′eitθ\\n= ( x1 + ix2 )(cos tθ+ i sin tθ)\\n= cos tθ· x1 − sin tθ· x2 + i(sin tθ· x1 + cos tθ· x2) (2.88)\\nHere we denote the token representation x′eitθ by C(x,tθ). The inner product of the representa-\\ntions of the tokens at positions tand scan be written as\\n⟨ C(x,tθ),C(y,sθ)⟩ = ( x′ y′)ei(t−s)θ (2.89)\\nwhere y′ is the complex conjugate of y′ . As can be seen, the result of this inner product involves\\na term t− s, and so it can model the offset between the two tokens.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 96, 'page_label': '90'}, page_content='90 Generative Models\\nNow we go back to representations in the 2D Euclidean space. T he dot-product of Ro(x,tθ)\\nand Ro(y,sθ) is can be written as a function of (t− s)θ\\nRo(x,tθ)[Ro(y,sθ)]T = xRtθ [yRsθ ]T\\n= xRtθ [Rsθ ]TyT\\n= xR(t−s)θ yT (2.90)\\nGiven this result, if we consider Ro(x,tθ) and Ro(y,sθ) as the query and the key , then the self-\\nattention operation will implicitly involve the modeling o f relative positional context.\\nThis rotary positional embedding can be extended to multi-d imensional embeddings. For\\na d-dimensional token embedding x =\\n[\\nx1 x2 ... xd\\n]\\n, we can treat it as a d\\n2 -dimensional\\ncomplex vector x′ =\\n[\\nx′\\n1 x′\\n2 ... x′\\nd/ 2\\n]\\n=\\n[\\nx1 + ix2 x3 + ix4 ... xd−1 + ixd\\n]\\n, where\\neach consecutive pair of items forms a complex number. Then, the rotary positional embedding in\\nthe complex space is given by\\nC(x,tθ) =\\nd/ 2∑\\nk=1\\nx′\\nkeitθ k ⃗ ek (2.91)\\nwhere ⃗ ek is the standard basis vector with a single non-zero value in t he k-th coordinate and 0’s\\nelsewhere [\\nBiderman et al. , 2021].\\nAlthough this formula involves a complicated expression, i ts equivalent form in the d-dimensional\\nEuclidean space is relatively easy to understand. W e can wri te it as\\nRo(x,tθ) =\\n[\\nx1 x2 ... xd\\n]\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nRtθ 1\\nRtθ 2\\n. . .\\nRtθ d/ 2\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\n(2.92)\\nwhere Rtθ k =\\n[\\ncos tθk sin tθk\\n− sin tθk cos tθk\\n]\\n. θ =\\n[\\nθ1 ,...,θd/ 2\\n]\\nare the parameters for controlling the an-\\ngles of rotations in different dimensions. T ypically , θk is set to 10000− 2(k−1)\\nd , which is analogous\\nto the setting in sinusoidal embeddings.\\nIn a practical implementation, Eq. ( 2.92) can be rewritten into a form that relies solely on the\\nelement-wise product and addition of vectors.\\nRo(x,tθ) =\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nx1\\nx2\\n.\\n.\\n.\\nxd−1\\nxd\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nT\\n⊙\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\ncos tθ1\\ncos tθ1\\n.\\n.\\n.\\ncos tθd/ 2\\ncos tθd/ 2\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nT\\n+\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\n−x2\\nx1\\n.\\n.\\n.\\n−xd\\nxd−1\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nT\\n⊙\\n\\uf8ee\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8ef\\n\\uf8f0\\nsin tθ1\\nsin tθ1\\n.\\n.\\n.\\nsin tθd/ 2\\nsin tθd/ 2\\n\\uf8f9\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fa\\n\\uf8fb\\nT\\n(2.93)\\nFinally , we rewrite Eq. (\\n2.85) to obtain the form of the embedding at position i'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 97, 'page_label': '91'}, page_content='2.3 Long Sequence Modeling 91\\nei = Ro( xi ,iθ) (2.94)\\n2.3.5.4 Position Interpolation\\nIn position interpolation, our goal is to map the positions in the new sequence to match the ob-\\nserved range in training. Suppose the sequence length for tr aining ranges from 0 to ml . When\\nm>m l at test time, we represent the positions in [0,m] such that our representations ﬁt [0,ml ].\\nT o illustrate, consider the rotary positional embedding mo del described above. The embedding\\nof each token is described by a model Ro(xi ,iθ) in which θ =\\n[\\nθ1,...,θd/ 2\\n]\\nare the parameters.\\nRo(xi ,iθ) can be cast in the form of a linear combination of two periodic functions (see Eq.\\n(\\n2.93))\\ncos iθ =\\n[\\ncos iθ1 ... cos iθd/ 2\\n]\\n(2.95)\\nsin iθ =\\n[\\nsin iθ1 ... sin iθd/ 2\\n]\\n(2.96)\\nθk is a exponential function of kand takes the form\\nθk = b− 2(k−1)\\nd (2.97)\\nwhere bis the base. The period of cos iθk and sin iθk is\\nTk = 2 π· b\\n2(k−1)\\nd (2.98)\\nThe key idea behind position interpolation is to adjust this period so that the new positions can\\nbe encoded within the range [0,ml ]. One way to achieve this is to scale up Tk by m\\nml\\n, given by\\nT′\\nk = m\\nml\\n· 2π· b\\n2(k−1)\\nd (2.99)\\nHence all points in [0,m] are compressed into [0,ml ]. This linear scaling can be easily realized\\nby modifying the input to the embedding model [ Chen et al. , 2023c]. The new model with linear\\npositional interpolation is given by\\nRo′(xi ,iθ) = Ro( xi ,ml\\nmiθ) (2.100)\\nAnother method of positional interpolation is to scale the b ase17 . Suppose that the base bis\\nscaled by λ. W e wish the period of this new model in the last dimension of θ(i.e., dimension d\\n2 )\\nto be equal to that of the linear positional interpolation mo del. This can be expressed as\\n2π· (λb)\\n2( d\\n2 −1)\\nd = m\\nml\\n· 2π· b\\n2( d\\n2 −1)\\nd (2.101)\\n17 This method was ﬁrst proposed in https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/\\nntkaware_scaled_rope_allows_llama_models_to_have/'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 98, 'page_label': '92'}, page_content='92 Generative Models\\nSolving this equation, we obtain\\nλ =\\n( m\\nml\\n) d\\n2( d\\n2 −1)\\n=\\n( m\\nml\\n) d\\nd−2 (2.102)\\nThis gives an embedding model\\nRo′(xi ,iθ) = Ro( xi,iθ′ ) (2.103)\\nwhere\\nθ′ =\\n[\\n(λb)− 0\\nd ,(λb)− 2\\nd ,...,(λb)− d−2\\nd\\n]\\n(2.104)\\nNote that scaling the base provides a non-uniform method for scaling the periods across dif-\\nferent dimensions of θ. This method has been found to be helpful for extending LLMs t o longer\\nsequences, and several improvements have been developed [ Peng et al. , 2024; Ding et al. , 2024].\\n2.3.6 Remarks\\nIn this section, we have presented a variety of methods for long-context language modeling. W e\\nclose this section by discussing some interesting issues re lated to these methods.\\n2.3.6.1 Need for Long Context\\nOne of the ultimate goals of long-context LLMs is that these models can precisely encode inﬁnite\\ncontext. The so-called inﬁnite context refers more to the fa ct that an LLM can continuously read\\nwords. This motivates LLMs that can handle extremely long co ntext or stream data. As discussed\\nin Section\\n2.3.3, it is common to use ﬁxed-size memory models to process conti nuously expanding\\ncontext. Many such systems are based on recurrent architect ures or their variants, because they\\nare inherently suited to model time series problems where th e effects of past inputs continue\\nindeﬁnitely . Another way to achieve inﬁnite memory is to dev elop alternatives to self-attention\\nmodels, for example, one can use continuous-space attentio n models to encode context, which\\nremoves the dependency on context length [ Martins et al. , 2022].\\nWhen studying long-context LLMs, it is natural to wonder wha t mechanisms may explain the\\nuse of long context in language modeling. Can we compress the representation of inﬁnite context\\ninto a relatively small-sized model? Are all context tokens useful for predicting next tokens? How\\ndo LLMs prepare for token prediction when they see the contex t? Can we know in advance which\\ncontextual information will be critical for prediction? Ge neral answers to all these questions\\nare not obvious, but they inspire follow-on research of expl ainable models, and some interesting\\nresults have been found. For example, Deletang et al. [2024] conducted extensive experiments\\nto show that LLMs are powerful in-context compressors. Alth ough viewing predictive models\\nas compression models has long been studied in machine learn ing, it also provides insights into\\nour understanding of the LLM scaling laws. Pal et al. [2023] and Wu et al. [2024] investigated\\nwhether the features learned up to the current step, though n ot intentionally , are already sufﬁcient'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 99, 'page_label': '93'}, page_content='2.3 Long Sequence Modeling 93\\nfor predicting tokens at the following steps. Note that the n eed for long-context in language\\nmodeling is highly dependent on the problem that we address. A related issue is where to apply\\nLLMs and how to evaluate them. For example, in summarization tasks we may only need to distill\\nand focus on a few key aspects of the text, while in retrieval- like tasks we need to “memorize”\\nthe entire context so that the relevant information can be ac cessed. W e will discuss the evaluation\\nissue later in this subsection.\\n2.3.6.2 Pre-training or Adapting LLMs?\\nTraining LLMs requires signiﬁcant computational costs. Although it is straightforward to train\\nLLMs on long sequence data, the training becomes computatio nally unwieldy for large data sets. It\\nis common practice to pre-train LLMs on general datasets, an d then adapt them with modest ﬁne-\\ntuning effort. For example, LLMs with relative or rotary pos itional embeddings can be directly\\ntrained on large-scale data in the pre-training phase. Whil e the resulting models may exhibit some\\nabilities to extrapolate lengths in the inference phase, it may be more effective to ﬁne-tune them\\non longer sequences.\\nIdeally , we would like to pre-train LLMs with standard Trans former architectures and adapt\\nthem to new tasks. This allows us to use many off-the-shelf LL Ms and efﬁciently adapt them to\\nhandle long sequences. However, when new architectures are adopted, it seems inevitable that\\nwe need to train these models from scratch. This poses practi cal difﬁculties for developing long-\\ncontext LLMs, as we cannot leverage well-developed, pre-tr ained models and must instead train\\nthem ourselves. On the other hand, ﬁne-tuning is still an eff ective way to adapt LLMs with certain\\narchitectures that are different from those in pre-trainin g. An example is models augmented with\\nexternal memories. In these models, the pre-trained LLMs ar e ﬁxed, and the focus is on how\\nto make these LLMs collaborate with the memory models. In RAG , for instance, it is common\\nto ﬁne-tune LLMs to improve their use of retrieval-augmente d inputs. Another example of ﬁne-\\ntuning LLMs for long-context modeling is that we train an LLM with full attention models, and\\nthen replace them with sparse attention models in the ﬁne-tu ning phase. The pre-trained LLM\\nprovides initial values of model parameters used in a differ ent model, and this model is then ﬁne-\\ntuned as usual.\\n2.3.6.3 Evaluating Long-context LLMs\\nEvaluating long-context LLMs is important, but it is a new issue in NLP . The general idea is that,\\nif we input a long context to an LLM, then we can check from the o utput of the LLM whether it\\nunderstands the entire context and makes use of it in predict ing following tokens. In conventional\\nresearch of NLP , such evaluations are often aimed at examini ng the ability of NLP models in\\nhandling long-range dependencies. However, the size of con texts used in recent LLMs is much\\nlarger than that used in NLP systems a few years ago. This moti vates researchers to develop new\\nevaluation benchmarks and metrics for long-context LLMs.\\nOne approach is to use the perplexity metric. However, in spi te of its apparent simplicity , this\\nmethod tends to reﬂect more on the LLMs’ ability to make use of local context rather than global\\ncontext. It is therefore tempting to develop evaluation met hods that are speciﬁc to long-context\\nLLMs. Popular methods include various synthetic tasks wher e artiﬁcially generated or modiﬁed'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 100, 'page_label': '94'}, page_content='94 Generative Models\\ndata is used to evaluate speciﬁc capabilities of long-conte xt LLMs. In needle-in-a-haystack 18 and\\npasskey retrieval tasks [ Mohtashami and Jaggi , 2024; Chen et al. , 2023c], for instance, LLMs are\\nrequired to identify and extract a small, relevant piece of i nformation from a large volume of given\\ntext. The assumption here is that an LLM with sufﬁcient memor y should remember earlier parts\\nof the text as it processes new information. This LLM can thus pick out the relevant details, which\\nmight be sparse and hidden among much irrelevant informatio n, from the text. Alternatively ,\\nin copy memory tasks (or copy tasks for short), LLMs are used t o repeat the input text or a\\nspeciﬁc segment multiple times. These tasks were initially proposed to test the extent to which\\nrecurrent models can retain and recall previously seen toke ns [ Hochreiter and Schmidhuber , 1997;\\nArjovsky et al. , 2016], and have been adopted in evaluating recent LLMs [ Bulatov et al. , 2022;\\nGu and Dao , 2023].\\nAnother approach to evaluating long-context LLMs is to test them on NLP tasks that involve\\nvery long input sequences. Examples include long-document or multi-document summarization,\\nlong-document question answering, code completion, and so on. A beneﬁt of this approach is that\\nit can align evaluations with user expectations.\\nAlthough many methods have been developed, there is still no general way to evaluate long-\\ncontext LLMs [ Liu et al. , 2024c]. One problem is that most of these methods focus on speciﬁc\\naspects of LLMs, rather than their fundamental ability to mo del very long contexts. Even though\\nan LLM can pick out the appropriate piece of text from the inpu t, we cannot say that it truly un-\\nderstands the entire context. Instead, it might just rememb er some important parts of the context,\\nor even simply recall the answer via the model learned in pre- training. Moreover, the data used\\nin many tasks is small-scale and relatively preliminary , le ading to discrepancies between evalu-\\nation results and actual application performance. A more in teresting issue is that the results of\\nLLMs are inﬂuenced by many other factors and experimental se tups, for example, using different\\nprompts can lead to very different outcomes. This makes eval uation even more challenging be-\\ncause improvements may not solely result from better modeli ng of long contexts, and there is a\\nrisk of overclaiming our results. Nevertheless, many open q uestions remain in the development\\nand evaluation of long-context LLMs. For example, these mod els still suffer from limitations\\nsuch as restricted context length and high latency . Studyin g these issues is likely to prove valuable\\nfuture directions.\\n2.4 Summary\\nIn this chapter, we have discussed the concept of LLMs and related techniques. This can be consid-\\nered a general, though not comprehensive, introduction to L LMs, laying the foundation for further\\ndiscussions on more advanced topics in subsequent chapters . Furthermore, we have explored two\\nways to scale up LLMs. The ﬁrst focuses on the large-scale pre -training of LLMs, which is cru-\\ncial for developing state-of-the-art models. The second fo cuses on methods for adapting LLMs to\\nlong inputs, including optimizing attention models, desig ning more efﬁcient and compressed KV\\ncaches, incorporating memory models, and exploring better positional embeddings.\\nThe strength of LLMs lies in their ability to break the constr aints of training NLP models for\\na limited number of speciﬁc tasks. Instead, LLMs learn from l arge amounts of text through the\\nsimple task of token prediction — we predict the next token in a sentence given its prior tokens.\\n18 https://github.com/gkamradt/LLMTest_NeedleInAHaystack'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 101, 'page_label': '95'}, page_content='2.4 Summary 95\\nA general view is that, by repeating this token prediction ta sk a large number of times, LLMs can\\nacquire some knowledge of the world and language, which can t hen be applied to new tasks. As a\\nresult, LLMs can be prompted to perform any task by framing it as a task of predicting subsequent\\ntokens given prompts. This emergent ability in language mod els comes from several dimensions,\\nsuch as scaling up training, model size, and context size. It is undeniable that scaling laws are\\ncurrently the fundamental principle adopted in developing large language models, although sim-\\nply increasing model size has yet to prove sufﬁcient for achi eving AGI. These continuously scaled\\nLLMs have been found to show capabilities in general-purpos e language understanding, genera-\\ntion, and reasoning. More recently , it has been found that sc aling up the compute at inference time\\ncan also lead to signiﬁcant improvements in complex reasoni ng tasks [ OpenAI, 2024].\\nGiven their amazing power, LLMs have attracted considerabl e interest, both in terms of tech-\\nniques and applications. As a result, the explosion of resea rch interest in LLMs has also led to a\\nvast number of new techniques and models. However, we do not a ttempt to provide a comprehen-\\nsive literature review on all aspects of LLMs, given the rapi d evolution of the ﬁeld. Nevertheless,\\none can still gain knowledge about LLMs from general reviews [Zhao et al. , 2023; Minaee et al. ,\\n2024] or more focused discussions on speciﬁc topics [ Ruan et al. , 2024].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 102, 'page_label': '96'}, page_content='CH A P TER 3\\nPrompting\\nIn the context of LLMs, prompting refers to the method of providing an LLM with a speciﬁc input\\nor cue to generate a desired output or perform a task. For exam ple, if we want the LLM to translate\\na sentence from English to Chinese, we can prompt it like this\\nTranslate the text from English to Chinese.\\nT ext: The early bird catches the worm.\\nTranslation:\\nPrompting is crucial for LLMs because it directly inﬂuences how effectively these models under-\\nstand and respond to user queries. A well-crafted prompt can guide an LLM to generate more\\naccurate, relevant, and contextually appropriate respons es. Furthermore, this process can be iter-\\natively reﬁned. By analyzing the responses of the LLM, users can adjust their prompts to align\\nmore closely with their speciﬁc needs. Given the importance of prompting in applying LLMs,\\nprompt design has become an essential skill for users and dev elopers working with LLMs. This\\nleads to an active research area, called prompt engineering , in which we design effective prompts\\nto make better use of LLMs and enhance their practical utilit y in real-world applications.\\nAn important concept related to prompting is in-context learning . When prompting an LLM,\\nwe can add new information to the context, such as demonstrat ions of problem-solving. This\\nallows the LLM to learn from this context how to solve the prob lem. Here is an example of\\nprompting LLMs with a few demonstrations of how to classify t ext based on sentiment polarity .\\nHere are some examples of text classiﬁcation.\\nExample 1: W e had a delightful dinner together.→ Label: Positive\\nExample 2: I’m frustrated with the delays. → Label: Negative\\nWhat is the label for “That comment was quite hurtful. ”?\\nLabel:\\nIn-context learning is often seen as an emergent ability of L LMs that arises after pre-training.\\nThough LLMs can be trained or tuned to perform new tasks, in-c ontext learning provides a very\\nefﬁcient way to adapt these models without any training or tu ning effort. Perhaps this is one of\\nthe most notable features of LLMs: they indeed learn general knowledge about the world and\\nlanguage during pre-training, which we can easily apply to n ew challenges. Moreover, in-context\\nlearning reﬂects the broader trend of making AI systems more generalizable and user-friendly .\\nInstead of requiring specialized engineers to ﬁne-tune mod els for every unique task, users can\\ninteract with LLMs in a more intuitive way , simply providing examples or adjusting the context\\nas needed.\\nIn this chapter, we focus on prompting techniques in LLMs. W e begin by considering several\\ninteresting prompt designs commonly used in prompt enginee ring. Then, we discuss a series of\\n96'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 103, 'page_label': '97'}, page_content='3.1 General Prompt Design 97\\nreﬁnements to these methods. Finally , we explore approache s for automating prompt design.\\n3.1 General Prompt Design\\nThis section presents basic concepts in prompt design, along with examples of how to prompt\\nLLMs for various NLP tasks. Since the effectiveness of promp ting is highly dependent on the\\nLLMs being used, prompts often vary across different LLMs, m aking it difﬁcult to provide a\\ncomprehensive list of prompts for all LLMs and downstream ta sks. Therefore, this discussion is\\nnot focused on any speciﬁc LLM. Instead, the goal is to provid e guiding principles for prompt\\ndesign.\\n3.1.1 Basics\\nThe termprompt is used in many different ways. In this chapter we deﬁne a prom pt as the input\\ntext to an LLM, denoted by x. The LLM generates a text y by maximizing the probability Pr(y|x).\\nIn this generation process, the prompt acts as the condition on which we make predictions, and it\\ncan contain any information that helps describe and solve th e problem.\\nA prompt can be obtained using a prompt template (or template for short) [\\nLiu et al. , 2023a].\\nA template is a piece of text containing placeholders or vari ables, where each placeholder can\\nbe ﬁlled with speciﬁc information. Here are two templates fo r asking the LLM for weekend\\nsuggestions.\\nPlease give me some suggestions for a fun weekend.\\nIf {∗premise∗}, what are your suggestions for a fun weekend.\\nIn the ﬁrst template, we simply instruct the LLM to return som e suggestions. So the template\\nis just a piece of text with no variables. In the second templa te, the variable {∗premise∗} needs to\\nbe speciﬁed by the users to provide a premise for making sugge stions. For example, if we input\\npremise = the weather is nice this weekend\\nthen we can generate a prompt\\nIf the weather is nice this weekend,\\nwhat are your suggestions for a fun weekend.\\nW e can also design a template with multiple variables. Here i s an example in which we\\ncompare the two sentences in terms of their semantic similar ity .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 104, 'page_label': '98'}, page_content='98 Prompting\\nHere is a sentence\\n{∗sentence1∗}\\nHere is another sentence\\n{∗sentence2∗}\\nCompute the semantic similarity between the two sentences\\nA popular way to format prompts is to write each input or outpu t in a “name:content” style.\\nFor example, we can describe a conversation between two peop le, named John and David, and use\\nthe LLM to continue the conversation. A template of such prom pts is given by\\nJohn: {∗utterance1∗}\\nDavid: {∗utterance2∗}\\nJohn: {∗utterance3∗}\\nDavid: {∗utterance4∗}\\nJohn: {∗utterance5∗}\\nDavid: {∗utterance6∗}\\nJohn: {∗utterance7∗}\\nDavid:\\nThe “name:content” format can be used to deﬁne the task that w e want the LLM to perform.\\nFor example, given that “Q” and “ A ” are commonly used abbrevi ations for “Question” and “ An-\\nswer”, respectively , we can use the following template to do question-answering.\\nQ: {∗question∗}\\nA:\\nThis format can be used to describe more complex tasks. For ex ample, the following is an\\nexample of providing a speciﬁcation for a translation task\\nT ask: Translation\\nSource language: English\\nT arget language: Chinese\\nStyle: Formal text\\nT emplate: Translate the following sentence:{∗sentence∗}\\nIn practical systems, it is common to represent and store suc h data in key-value pairs, such as the\\nJSON format 1 .\\nWhen the problem is difﬁcult to describe in an attribute-bas ed manner, it is more common\\nto instruct LLMs with a clear and detailed description. Ther e are many ways to do this. One\\n1 The JSON representation is'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 105, 'page_label': '99'}, page_content='3.1 General Prompt Design 99\\nexample is to assign a role to LLMs and provide sufﬁcient cont ext. The following is a template\\nthat instructs an LLM to act as an expert and answer questions from children.\\nY ou are a computer scientist with extensive knowledge in the ﬁeld\\nof deep learning.\\nPlease explain the following computer-related concept to achild\\naround 10 years old, using simple examples whenever possibl e.\\n{∗concept∗}\\nHere the text “Y ou are a computer scientist ... deep learning . ” is sometimes called system\\ninformation, and is provided to help the LLM understand the c ontext or constraints of the task it\\nis being asked to perform.\\n3.1.2 In-context Learning\\nLearning can occur during inference. In-context learning is one such method, where prompts\\ninvolve demonstrations of problem-solving, and LLMs can le arn from these demonstrations how\\nto solve new problems. Since we do not update model parameter s in this process, in-context\\nlearning can be viewed as a way to efﬁciently activate and reo rganize the knowledge learned in\\npre-training without additional training or ﬁne-tuning. T his enables quick adaptation of LLMs to\\nnew problems, pushing the boundaries of what pre-trained LL Ms can achieve without task-speciﬁc\\nadjustments.\\nIn-context learning can be illustrated by comparing three m ethods: zero-shot learning, one-\\nshot learning and few-shot learning. Zero-shot learning, a s its name implies, does not involve a\\ntraditional “learning” process. It instead directly appli es LLMs to address new problems that were\\nnot observed during training. In practice, we can repetitiv ely adjust prompts to guide the LLMs in\\ngenerating better responses, without demonstrating probl em-solving steps or providing examples.\\nConsider the following example. Suppose we want to use an LLM as an assistant that can help\\ncorrect English sentences. A zero-shot learning prompt is g iven by\\n{\\n\"T ask\": \"Translation\"\\n\"Source language\": \"English\"\\n\"T arget language\": \"Chinese\"\\n\"Style\": \"Formal text\"\\n\"T emplate\": \"Translate the following sentence:{∗sentence∗}\"\\n}'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 106, 'page_label': '100'}, page_content='100 Prompting\\nSYSTEM Y ou are a helpful assistant, and are great at grammar correct ion.\\nUSER Y ou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: She don’t like going to the park.\\nOutput:\\nHere the gray words are used to indicate different ﬁelds of th e prompt.\\nIn one-shot learning, we extend this prompt by adding a demon stration of how to correct\\nsentences, thereby allowing the LLM to learn from this newly -added experience.\\nSYSTEM Y ou are a helpful assistant, and are great at grammar correct ion.\\nDEMO Y ou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: There is many reasons to celebrate.\\nOutput: There are many reasons to celebrate.\\nUSER Y ou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: She don’t like going to the park.\\nOutput:\\nFurthermore, we can add more demonstrations to enable few-s hot learning.\\nSYSTEM Y ou are a helpful assistant, and are great at grammar correct ion.\\nDEMO1 Y ou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: There is many reasons to celebrate.\\nOutput: There are many reasons to celebrate.\\nDEMO2 Y ou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: Me and my friend goes to the gym every day .\\nOutput: My friend and I go to the gym every day .\\nUSER Y ou will be provided with a sentence in English. The task is\\nto output the correct sentence.\\nInput: She don’t like going to the park.\\nOutput:\\nIn few-shot learning, we essentially provide a pattern that maps some inputs to the corre-\\nsponding outputs. The LLM attempts to follow this pattern in making predictions, provided that\\nthe prompt includes a sufﬁcient number of demonstrations, a lthough generally small. It is also'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 107, 'page_label': '101'}, page_content='3.1 General Prompt Design 101\\npossible to use simpler patterns to achieve this. For exampl e, one can use the following few-shot\\nlearning prompt for translating words from Chinese to Engli sh.\\nDEMO 现在 → now\\n来 → come\\n去 → go\\n男孩 → boy\\nUSER 女孩 →\\nIf the LLM is powerful enough, few-shot learning can enable i t to address complex prob-\\nlems, such as mathematical reasoning. For example, conside r the following task of summing two\\nnumbers and then dividing the sum by their product.\\nDEMO 12 5 → (12 + 5) /(12 × 5) = 0 .283\\n3 1 → (3 + 1) /(3 × 1) = 1 .33\\n−9 4 → (−9 + 4) /(−9 × 4) = 0 .138\\n15 15 → (15 + 15) /(15 × 15) = 0 .133\\nUSER 19 73 →\\nIn many practical applications, the effectiveness of in-co ntext learning relies heavily on the\\nquality of prompts and the fundamental abilities of pre-tra ined LLMs. On one hand, we need a\\nsigniﬁcant prompt engineering effort to develop appropria te prompts that help LLMs learn more\\neffectively from demonstrations. On the other hand, strong er LLMs can make better use of in-\\ncontext learning for performing new tasks. For example, sup pose we wish to use an LLM to\\ntranslate words from Inuktitut to English. If the LLM lacks p re-training on Inuktitut data, its\\nunderstanding of Inuktitut will be weak, and it will be difﬁc ult for the model to perform well in\\ntranslation regardless of how we prompt it. In this case, we n eed to continue training the LLM\\nwith more Inuktitut data, rather than trying to ﬁnd better pr ompts.\\nIt might be interesting to explore how in-context learning e merges during pre-training and\\nwhy it works during inference. One simple understanding is t hat LLMs have gained some knowl-\\nedge of problem-solving, but there are many possible predic tions, which are hard to distinguish\\nwhen the models confront new problems. Providing demonstra tions can guide the LLMs to fol-\\nlow the “correct” paths. Furthermore, some researchers hav e tried to interpret in-context learn-\\ning from several different perspectives, including Bayesi an inference [ Xie et al. , 2022], gradient\\ndecent [ Dai et al. , 2023; V on Oswald et al. , 2023], linear regression [ Akyürek et al. , 2023], meta\\nlearning [ Garg et al. , 2022], and so on.\\n3.1.3 Prompt Engineering Strategies\\nDesigning prompts is highly empirical. In general, there are many ways to prompt an LLM for\\nperforming the same task, and we need to perform a number of tr ial-and-error runs to ﬁnd a\\nsatisfactory prompt. T o write good prompts more efﬁciently , one can follow certain strategies.\\nExamples of common prompting principles include'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 108, 'page_label': '102'}, page_content='102 Prompting\\n• Describing the task as clearly as possible . When we apply an LLM to solve a problem,\\nwe need to provide a precise, speciﬁc, and clear description of the problem and instruct the\\nLLM to perform as we expect. This is particularly important w hen we want the output of\\nthe LLM to meet certain expectations. For example, suppose w e are curious about climate\\nchange. A simple prompt for asking the LLM to provide some inf ormation is\\nT ell me about climate change.\\nSince this instruction is too general, the LLM may generate a response that addresses any\\naspect of climate change, which may not align with our speciﬁ c interests. In this case, we\\ncan instead use prompts that are speciﬁc and detailed. One su ch example is\\nProvide a detailed explanation of the causes and effects of c limate change,\\nincluding the impact on global temperatures, weather patte rns, and sea\\nlevels. Also, discuss possible solutions and actions being taken to mitigate\\nthese effects.\\nNow suppose we intend to explain climate change to a 10-year- old child. W e can adjust the\\nabove prompt further.\\nExplain the causes and effects of climate change to a 10-year -old child.\\nT alk about how it affects the weather, sea levels, and temper atures. Also,\\nmention some things people are doing to help. Try to explain i n simple\\nterms and do not exceed 500 words.\\n• Guiding LLMs to think . LLMs have exhibited surprisingly good capabilities to “th ink”.\\nA common example is that well-developed LLMs have achieved i mpressive performance\\nin mathematical reasoning tasks, which are considered chal lenging. In prompt engineering,\\nthe “thinking” ability of LLMs needs to be activated through appropriate prompting, espe-\\ncially for problems that require signiﬁcant reasoning effo rts. In many cases, an LLM that\\nis instructed to “think” can produce completely different r esults compared with the same\\nLLM that is instructed to perform the task straightforwardl y . For example, Kojima et al.\\n[2022] found that simply appending “Let’s think step by step” to th e end of each prompt\\ncan improve the performance of LLMs on several reasoning tas ks. LLMs can be prompted\\nto “think” in a number of ways. One method is to instruct LLMs t o generate steps for rea-\\nsoning about the problem before reaching the ﬁnal answer. Fo r example, consider a task of\\nsolving mathematical problems. See below for a simple promp t for this task.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 109, 'page_label': '103'}, page_content='3.1 General Prompt Design 103\\nY ou are a mathematician. Y ou will be provided with a math prob lem.\\nPlease solve the problem.\\nSince solving math problems requires a detailed reasoning p rocess, LLMs would probably\\nmake mistakes if they attempted to work out the answer direct ly . So we can explicitly ask\\nLLMs to follow a given reasoning process before coming to a co nclusion.\\nY ou are a mathematician. Y ou will follow these detailed reas oning steps\\nwhen solving math problems.\\nStep 1: Problem Interpretation.\\nThe mathematician carefully listens to your query and understands the in-\\ntricate details of the mathematical challenge you have pres ented.\\nStep 2: Strategy Formulation.\\nDrawing upon their extensive knowledge, the mathematicianchooses the\\nmost effective strategy tailored to the type of math problem , whether it is\\nalgebra, calculus, or geometry .\\nStep 3: Detailed Calculation.\\nWith precision and expertise, the mathematician performs the necessary\\ncalculations step by step, adhering to all mathematical pri nciples.\\nStep 4: Solution Review .\\nBefore providing the ﬁnal answer, the mathematician meticulously checks\\nthe calculations for accuracy and offers a concise explanat ion or rationale\\nfor the solution.\\nY ou will be provided with a math problem. Please solve the problem.\\n{∗problem∗}\\nAnother method to guide LLMs to “think” is through multiple r ounds of interaction with\\nLLMs. For example, as a ﬁrst step, we can instruct LLMs to solv e the problem directly\\nY ou will be provided with a math problem. Please solve the pro blem.\\n{∗problem∗}\\nNow we have an initial answer to the problem. As a second step, we prompt LLMs to\\nevaluate the correctness of the answer and, if necessary , re work it to ﬁnd a better solution.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 110, 'page_label': '104'}, page_content='104 Prompting\\nY ou will be provided with a math problem, along with a solutio n. Evaluate\\nthe correctness of this solution, and identify any errors if present. Then,\\nwork out your own solution.\\nProblem:{∗problem∗}\\nSolution: {∗solution∗}\\nThe prompts presented here are closely related to a long line of research on reasoning prob-\\nlems in LLMs. It is impossible to provide a complete discussi on of all related issues because\\nthis topic covers a large family of methods. But we will see a r elatively more detailed dis-\\ncussion on how to improve prompting through more reasoning i n Section 3.2.\\n• Providing reference information . As discussed in the previous section, we can include\\ndemonstrations in prompts and allow LLMs to in-context lear n from these demonstrations\\nhow to perform the task. In fact, given the remarkable abilit y of language understanding of\\nLLMs, we can add any type of text into the prompts and so these m odels can predict based\\non enriched contexts. In many applications, we have various information that is relevant\\nto user queries. Instead of using LLMs to make unconstrained predictions, we often want\\nLLMs to produce outputs that are conﬁned to the relevant text . One such example is RAG,\\nwhere the relevant text for the user query is provided by call ing an IR system, and we\\nprompt LLMs to generate responses based on this provided rel evant text. The following\\nprompt shows an example.\\nY ou are an expert that can generate answers to input queries. Y ou have now\\nbeen provided with a query and the corresponding context inf ormation.\\nPlease generate an answer based on this context information . Note that\\nyou need to provide the answer in your own words, not just copy from the\\ncontext provided.\\nContext information:{∗IR-result∗}\\nQuery: {∗query∗}\\nIf the context information is highly reliable, we can even re strict LLMs to answering using\\nonly the provided text. An example prompt is shown as follows'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 111, 'page_label': '105'}, page_content='3.1 General Prompt Design 105\\nY ou are an expert tasked with generating answers from input q ueries. Y ou\\nhave been provided with a query and corresponding context in formation,\\norganized in a table where each row represents a useful recor d. Please\\ngenerate an answer using only this context information. Ens ure that you\\nprovide the answer in your own words.\\nContext information:{∗table∗}\\nQuery: {∗query∗}\\nWhen dealing with real-world problems, we often have prior k nowledge and additional\\ninformation about the problems that help produce better ans wers. Considering such infor-\\nmation in prompting is generally helpful in improving the re sult.\\n• Paying attention to prompt formats . In general, the performance of LLMs is highly\\nsensitive to the prompts we input. Sometimes a small modiﬁca tion to a prompt can lead to a\\nbig change in model output. An interesting example is that ch anging the order of sentences\\nin a prompt may cause LLMs to generate different results. T o m ake prompts easy to read\\nand reduce ambiguity , it is common to format them in a way that ensures clarity . One\\nexample is that we deﬁne several ﬁelds for prompts and ﬁll dif ferent information in each\\nﬁeld. Another example is we can use code-style prompts for LL Ms which can understand\\nand generate both natural language and code. See the followi ng for a code-style prompt that\\nperforms translation where one demonstration is presented .\\n[English] = [I have an apple.]\\n[German] = [Ich habe einen Apfel.]\\n[English] = [I have an orange.]\\n[German] =\\nLLMs can receive text in various formats. This allows us to us e control characters, XML\\ntags, and speciﬁc formatting to represent complex data. And it is useful to specify how the\\ninput and output should be formatted or structured. For exam ple, we can delimit sections of\\ntext using quotes and prompt LLMs accordingly (e.g., adding a sentence like “the input text\\nis delimited by double quotes” to the prompt).\\nAbove, we have discussed only a few strategies for writing go od prompts. There are, of course,\\nmany such methods, and one needs to develop their own through practice. Interested readers can\\nrefer to various online documents for more information, suc h as OpenAI’s manual on the GPT\\nseries models 2 .\\n2 See https://platform.openai.com/docs/guides/prompt-engineering/\\nsix-strategies-for-getting-better-results .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 112, 'page_label': '106'}, page_content='106 Prompting\\n3.1.4 More Examples\\nIn this subsection, we consider more examples of prompting LLMs to perform various NLP tasks.\\nThe motivation here is not to give standard prompts for these tasks, but rather to use simple\\nexamples to illustrate how LLMs can be prompted to deal with N LP problems.\\n3.1.4.1 T ext Classiﬁcation\\nT ext classiﬁcation is perhaps one of the most common problems in NLP . Many tasks can be\\nbroadly categorized as assigning pre-deﬁned labels to a giv en text. Here we consider the polarity\\nclassiﬁcation problem in sentiment analysis. W e choose pol arity classiﬁcation for illustration be-\\ncause it is one of the most popular and well-deﬁned text class iﬁcation tasks. In a general setup of\\npolarity classiﬁcation, we are required to categorize a giv en text into one of three categories: neg-\\native, positive, or neutral. Below is a simple prompt for doi ng this (for easy reading, we highlight\\nthe task description in the prompt).\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral.\\nT ext:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nThe polarity of the text can be classiﬁed as positive.\\nT o make the example complete, we show the response generated by the LLM (underlined text).\\nAlthough the answer is correct, the LLM gives this answer not in labels but in text describing\\nthe result. The problem is that LLMs are designed to generate text but not to assign labels to text\\nand treat classiﬁcation problems as text generation proble ms. As a result, we need another system\\nto map the LLM’s output to the label space (call it label mapping ), that is, we extract “positive”\\nfrom “The polarity of the text can be classiﬁed as positive”. This is trivial in most cases because\\nwe can identify label words via simple heuristics. But occas ionally , LLMs may not express the\\nclassiﬁcation results using these label words. In this case , the problem becomes more complicated,\\nas we need some way to map the generated text or words to predeﬁ ned label words.\\nOne method to induce output labels from LLMs is to reframe the problem as a cloze task. For\\nexample, the following shows a cloze-like prompt for polari ty classiﬁcation.\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral.\\nT ext:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nThe polarity of the text is positive'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 113, 'page_label': '107'}, page_content='3.1 General Prompt Design 107\\nW e can use LLMs to complete the text and ﬁll the blank with the m ost appropriate word. Ide-\\nally , we wish the ﬁlled word would be positive, negative, or neutral. However, LLMs are not\\nguaranteed to generate these label words. One method to addr ess this problem is to constrain the\\nprediction to the set of label words and select the one with th e highest probability . Then, the output\\nlabel is given by\\nlabel = arg max\\ny∈Y\\nPr(y|x) (3.1)\\nwhere ydenotes the word ﬁlled in the blank, and Y denotes the set of label words\\n{positive,negative,neutral}.\\nAnother method of using LLMs to generate labels is to constra in the output with prompts. For\\nexample, we can prompt LLMs to predict within a controlled se t of words. Here is an example.\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral.\\nT ext:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nWhat is the polarity of the text?\\nJust answer: positive, negative, or neutral.\\nPositive\\nSentiment analysis is a common NLP problem that has probably been well understood by\\nLLMs through pre-training or ﬁne-tuning. Thus we can prompt LLMs using simple instructions\\nto perform the task. However, for new classiﬁcation problem s, it may be necessary to provide\\nadditional details about the task, such as the classiﬁcatio n standards, so that the LLMs can perform\\ncorrectly . T o do this, we can add a more detailed description of the task and/or demonstrate\\nclassiﬁcation examples in the prompts. T o illustrate, cons ider the following example.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 114, 'page_label': '108'}, page_content='108 Prompting\\nAnalyze the polarity of the following text and classify it as positive, negative, or\\nneutral. Here’s what each category represents:\\nPositive: This indicates that the text conveys a positive emotion or attitude. For\\nexample, texts expressing happiness, satisfaction, excitement, or admiration are\\nconsidered positive.\\nNegative: This refers to a text that expresses a negative emotion or attitude. It\\nencompasses feelings of sadness, anger, frustration, or criticism.\\nNeutral: Neutral sentiment is used to describe texts that do not exhibit clear posi-\\ntive or negative emotions but instead convey informational, factual, or indifferent\\ntones.\\nT ext:\\nThe service at the restaurant was slower than expected, which was a bit frustrat-\\ning.\\nWhat is the polarity of the text?\\nPositive\\nWhile it seems straightforward to use LLMs for classiﬁcatio n problems, there are still issues\\nthat have not been well addressed. For example, when dealing with a large number of categories,\\nit remains challenging to effectively prompt LLMs. Note tha t if we face a very difﬁcult classiﬁca-\\ntion problem and have a certain amount of labeled data, ﬁne-t uning LLMs or adopting “BERT +\\nclassiﬁer”-like architectures is also desirable.\\n3.1.4.2 Information Extraction\\nMany NLP problems can be regarded as information extractionproblems, involving the identiﬁ-\\ncation or extraction of speciﬁc pieces of information from u nstructured text. This information can\\ninclude named entities, relationships, events, and other r elevant data points. The goal of infor-\\nmation extraction is to transform raw data into a format that can be easily analyzed and used in\\nvarious downstream applications.\\nAs information extraction covers a wide range of problems, w e cannot discuss them all here.\\nInstead, we start with the task of named entity recognition — a task that has long been a concern\\nin NLP . Named entity recognition is a process that detects an d classiﬁes key information in text\\ninto speciﬁc groups. These key pieces of information, known as named entities, typically include\\nproper names and are categorized into distinct classes such as people, locations, organizations,\\ndates, monetary values, and percentages. Consider ﬁrst a si mple example of extracting person\\nnames from a text\\n3 .\\n3 The text is from https://edition.cnn.com/travel'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 115, 'page_label': '109'}, page_content='3.1 General Prompt Design 109\\nIdentify all person names in the provided text.\\nT ext:\\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\\nFor T om Jenkins, CEO of the European T ourism Organisation, i t’s the latter. “I\\nthink the UK is doing perfectly well but we’ll see more people going to Europe, ”\\nhe says of 2024, adding that there’s “allegedly a slight plat eau of American de-\\nmand for the UK. ”\\nThe person name identiﬁed in the provided text is: T om Jenkins\\nThen, we can extract all named entities.\\nIdentify and classify all named entities in the provided text into categories such\\nas person names, locations, dates, and organizations. List each entity with its type\\non one line.\\nT ext:\\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\\nFor T om Jenkins, CEO of the European T ourism Organisation, i t’s the latter. “I\\nthink the UK is doing perfectly well but we’ll see more people going to Europe, ”\\nhe says of 2024, adding that there’s “allegedly a slight plat eau of American de-\\nmand for the UK. ”\\n1. T om Jenkins - Person Name\\n2. European T ourism Organisation - Organization\\n3. UK - Location\\n4. Europe - Location\\n5. 2024 - Date\\nGiven these identiﬁed named entities, we can further proces s the text using other information\\nextraction tasks. For example, we can identify the relation ships between these named entities (call\\nit relation extraction ). The corresponding prompt is shown as follows.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 116, 'page_label': '110'}, page_content='110 Prompting\\nGiven a text and a list of named entities identiﬁed within it, analyze and describe\\nthe relationships between these entities. Explain how each entity is contextually\\nrelated to others.\\nT ext:\\nIs the UK really doing that badly or have travel trends shifte d, post-pandemic?\\nFor T om Jenkins, CEO of the European T ourism Organisation, i t’s the latter. “I\\nthink the UK is doing perfectly well but we’ll see more people going to Europe, ”\\nhe says of 2024, adding that there’s “allegedly a slight plat eau of American de-\\nmand for the UK. ”\\nNamed Entity:\\nT om Jenkins, European T ourism Organisation, UK, Europe, 2024\\nT om Jenkins - European T ourism Organisation: T om Jenkins is the CEO of the\\nEuropean T ourism Organisation. This indicates a professional association where\\nT om Jenkins likely has a signiﬁcant role in directing the organization’s activities,\\nespecially those related to tourism in Europe.\\n...\\nIf LLMs have been ﬁne-tuned with instruction following for i nformation extraction tasks, it\\nis relatively easy to perform various information extracti on tasks. For example, the following is a\\nprompt template for information extraction.\\nY ou will be provided with a text. Y our task is to {∗task-description∗}\\nT ext: {∗text∗}\\nHere task-description is the instruction for the task. Examples of the instruction include\\nextract the keywords from the text.\\nextract the key events from the text.\\ndetect the coreference and anaphoric links between entities from the text.\\nand so on. Of course, we can provide a more detailed descripti on of the task to prompt LLMs\\nmore accurately .\\n3.1.4.3 T ext Generation\\nIn NLP , the termtext generation is used broadly to cover a wide range of different problems, m any\\nof which focus on generating text based on given text. These p roblems can be roughly categorized\\ninto two classes: 1) continual writing based on the input tex t (call it text completion ), and 2)\\ntransformation of the input text into another text (call it text transformation ). Here, we illustrate\\nthe prompts for the former and consider the prompts for the la tter in the following discussion.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 117, 'page_label': '111'}, page_content='3.1 General Prompt Design 111\\nA simple example of text completion is continuing to write a s tory starting with a few given\\nsentences.\\nY ou are a writer who is great at writing stories. Y ou are provided with a few\\nsentences to start with. Please continue the story from the provided sentences.\\nStart of the story:\\nOne sunny afternoon, Lily discovered a tiny , golden key hidden under a rock in\\nher backyard. Curious, she picked it up and wondered what it m ight unlock. Just\\nthen, her cat, Whiskers, began to meow excitedly , leading he r towards the old\\noak tree at the edge of the garden.\\nLily followed Whiskers, her heart pounding with excitement and a touch of\\nnervousness. The old oak tree had always seemed mysterious to her, its gnarled\\nbranches casting long shadows even on the brightest days. As they reached the\\ntree, Whiskers darted ahead and started pawing at something buried beneath a\\npile of autumn leaves ...\\nLikewise, we can create a conversation given the conversati onal context.\\nY ou are provided with a conversation between a tourist and a taxi driver. Please\\ncomplete the conversation.\\nT ourist: Could you take me to the downtown museum, please?\\nT axi Driver: Of course! First time in the city?\\nT ourist: Y es, any must-see places around?\\nT axi Driver: Deﬁnitely check out the botanical gardens after the museum.\\nThey’re nearby and quite beautiful.\\nT ourist: Great, thanks for the tip!\\nT axi Driver: No problem. Enjoy your visit!\\nAnother application of text completion is generating text b ased on user requirements. The\\nrequirements can include style, tone, length, and any speci ﬁc content that the text should contain.\\nFor example, users may need a formal report, a creative story , or a piece of programming code.\\nHere is a prompt for generating a ﬁve-character regulated Ch inese poem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 118, 'page_label': '112'}, page_content='112 Prompting\\nY ou are a poet. Please write a traditional Chinese poem. Here is the requirement.\\nTheme: Spring’s rejuvenation\\nStructure: Five-character regulated poem\\nEmotion: Convey a sense of happiness and renewal through vivid spring imagery .\\n春意盎然\\n翠柳风中舞，\\n红花雨后新。\\n山明水又绿，\\n天宽鸟自频。\\nIf the LLM is trained to generate language and code, we can pro mpt it to perform code com-\\npletion tasks. Here is an example.\\nPlease write a Python function to calculate the average of a list of numbers.\\ndef calculate_average(numbers):\\nif numbers: # Check if the list is not empty\\nreturn sum(numbers) / len(numbers)\\nelse:\\nreturn 0 # Return 0 if the list is empty\\n3.1.4.4 T ext T ransformation\\nT ypical text transformation tasks include machine translation, summarization, text style transfer,\\nand so on. The prompts used to perform these tasks are not sign iﬁcantly different from the ones\\npresented previously . Here, we present a few examples to sho wcase these prompts.\\nTranslate the following text from English to Spanish.\\nT ext:\\nThe quick brown fox jumps over the lazy dog.\\nEl rápido zorro marrón salta sobre el perro perezoso.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 119, 'page_label': '113'}, page_content='3.1 General Prompt Design 113\\nSummarize the following this article in no more than 50 words:\\nArticle:\\nIn recent years, urban areas have been facing signiﬁcant challenges related to sus-\\ntainability and environmental management. Cities are expe riencing higher rates\\nof pollution, increased trafﬁc congestion, and greater dem ands on infrastructure.\\nThis growth has led to numerous environmental issues, inclu ding elevated lev-\\nels of air and water pollution, increased waste production, and strained public\\nservices ...\\nUrban areas are grappling with sustainability challenges, such as rising pollution,\\ntrafﬁc congestion, and infrastructure demands ...\\nRewrite this text in a formal tone.\\nT ext:\\nHey , what’s up? Long time no see!\\nHello, how have you been? It has been quite some time since we last met!\\n3.1.4.5 Question Answering\\nThe question-answering format is inherently simple. For a given question, there is an answer that\\ncorresponds to it. For example, in open-domain question ans wering, we expect the system to\\nreturn an answer in response to a user-submitted question. P rompt templates for general-purpose\\nquestion answering can be:\\n{∗question∗}\\nQuestion: {∗question∗}\\nAnswer:\\nQuestion answering is important in NLP because many problem s can be framed as question-\\nanswering tasks. In particular, many recent reasoning task s are deﬁned in the form of question\\nanswering. For example, in the MMLU benchmark [ Hendrycks et al. , 2021], each example con-\\nsists of a multiple-choice question, and LLMs are required t o select the correct answer. See the\\nfollowing for an example prompt for answering a question in t his dataset.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 120, 'page_label': '114'}, page_content='114 Prompting\\n(Z,∗) is a group with a∗ b= a+ b+ 1 for all a, bin Z. The inverse of ais\\n(A) 0\\n(B) −2\\n(C) a− 2\\n(D) (2 + a) ∗ − 1\\nD\\nAnother widely-used benchmark is the GSM8K dataset [ Cobbe et al. , 2021]. It consists of\\nthousands of grade school math word problems. For each probl em, we use LLMs to return a\\nsolution in natural language. For example, the following is a prompt used for solving a problem\\nin GSM8K.\\nDEMO Q: Jerry’s two daughters play softball on different teams. T hey each have\\n8 games this season. Each team practices 4 hours for every gam e they\\nplay . If each game lasts for 2 hours, how many hours will Jerry spend at\\nthe ﬁeld watching his daughters play and practice altogethe r?\\nA: Jerry will spend 8 games * 2 hours per game = ≪ 8 ∗ 2 = 16 ≫ 16\\nhours watching one daughter play her games. He will spend 16 ∗ 2 = ≪\\n16 ∗ 2 = 32 ≫ 32 hours watching both daughters play their games. He\\nwill spend 8 games * 4 hours of practice = ≪ 8 ∗ 4 = 32 ≫ 32 hours\\nwatching one daughter practice. He will spend 32∗2 = ≪ 32∗2 = 64 ≫\\n64 hours watching both daughters practice. He will spend a tota l of 32\\nhours watching games + 64 hours watching practice =≪ 32 + 64 =\\n96 ≫ 96 hours. #### 96\\nQ: Mary bought six apples from the store. From the apples she b ought,\\nfor each that Mary ate, she planted two trees from the remaini ng ones.\\nHow many apples did Mary eat?\\nA: She planted eight trees. This means she used half of that amount of\\napples, which is 8 trees /2 trees/apple =≪ 8/2 = 4 ≫ 4 apples. That\\nmeans that she planted four of the six apples she bought, leav ing only 6\\napples − 4 apples =≪ 6 − 4 = 2 ≫ 2 apples to be eaten. #### 2\\nUSER Q: Boris has 100 apples. Beck has 23 fewer apples than Boris. I f Boris\\ngives Beck 10 apples, how many fewer apples does Beck have tha n Boris\\nnow?\\nA:\\nBoris starts with 100 apples and gives Beck 10 apples, leaving\\nhim with 100 apples −10 apples =≪ 100 − 10 = 90 ≫ 90 apples.\\nInitially , Beck has 23 fewer apples than Boris, giving him a\\nstarting count of 100 − 23 = ≪ 100 − 23 = 77 ≫ 77 apples. After\\nreceiving 10 apples from Boris, Beck’s total comes to 77 apples +10\\napples =≪ 77 + 10 = 87 ≫ 87 apples. Consequently , Beck now has\\n90 − 87 = ≪ 90 − 87 = 3 ≫ 3 fewer apples than Boris. #### 3'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 121, 'page_label': '115'}, page_content='3.2 Advanced Prompting Methods 115\\nHere a few-shot prompt is adopted. The LLM learns from these d emonstrations of problem-\\nsolution pairs not only the way of problem-solving but also t he way of formatting the output. For\\nexample, the ﬁnal result of calculation follows the #### tok en, and ≪ ...≫ annotates the detailed\\ncalculation steps (called calculation annotation )4 .\\n3.2 Advanced Prompting Methods\\nSo far in this chapter, we have introduced the basic conceptsrelated to LLM prompting and pre-\\nsented a number of prompts for NLP tasks. W e now consider seve ral techniques for enhancing the\\neffectiveness of prompting.\\n3.2.1 Chain of Thought\\nW e have encountered the concept ofchain of thought (CoT) several times in this chapter and\\nprevious ones [\\nW ei et al. , 2022c; Chowdhery et al. , 2022]. CoT methods provide a simple way\\nto prompt LLMs to generate step-by-step reasoning for compl ex problems, thereby approaching\\ntasks in a more human-like manner. Rather than coming to a con clusion directly , the CoT methods\\ninstruct LLMs to generate reasoning steps or to learn from de monstrations of detailed reasoning\\nprocesses provided in the prompts. T o illustrate CoT , we con sider the problem of algebraic calcu-\\nlation, as commonly described in the literature. Suppose we are given a algebraic problem\\nCalculate the average of the numbers 2, 4, and 6.\\nW e can consider it as the question and prompt an LLM to answer i t.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: The answer is 6.\\nIt seems difﬁcult for the LLM to directly give a correct answe r. A simple improvement is to\\nadd demonstrations of similar problems in the prompt, and th us the LLM can learn from these\\ndemonstrations.\\nQ: Please calculate the average of the numbers 1, 3, 5, and 7.\\nA: The answer is 4.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: The answer is 7.\\nThe problem here is that, although we have shown a similar que stion-answer pair, it remains\\ndifﬁcult for the LLM to reason out the correct answer. In CoT , not only can LLMs learn from the\\n4 During prediction, a calculator is used when we see ≪ ... ≫. More speciﬁcally , once the LLM encounters “=”\\nin a ≪ ... ≫, then the calculator calculates the expression on the left- hand side of “=”. This method helps reduce the\\ncalculation errors made by LLMs.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 122, 'page_label': '116'}, page_content='116 Prompting\\ncorrespondence between questions and answers but they may g ain more from detailed problem-\\nsolving steps that used to derive the answers. T o do this, we c an incorporate some reasoning steps\\ninto the prompt to obtain a CoT prompt.\\nQ: Please calculate the mean square of the numbers 1, 3, 5, and 7.\\nA: Calculate the square of each number: 12 = 1 , 32 = 9 , 52 = 25 , and 72 = 49 .\\nSum the squares, 1 + 9 + 25 + 49 = 84 . There are 4 numbers in total. Divide\\nthe sum by the number of items, 84/4 = 21 . The answer is 21.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: Calculate 2 + 4 + 9 , which equals 15. There are three numbers. Divide the\\ntotal sum by the count, resulting in 15/3 = 5 . The answer is 5.\\nHere we highlight the reasoning steps in green. By providing a detailed reasoning process for\\na similar problem, the LLM learns to reason and thereby gener ates a problem-solving path that\\nleads to the correct answer.\\nThere are several beneﬁts of using CoT prompting. First, CoT allows LLMs to decom-\\npose complex problems into smaller, sequential reasoning s teps. This somewhat mirrors human\\nproblem-solving behaviors, making it particularly effect ive for tasks requiring detailed, multi-step\\nreasoning. Second, CoT makes the reasoning process more tra nsparent and interpretable. Since all\\nreasoning steps are visible, we can understand and interpre t how a conclusion was reached. Third,\\nif users can follow the logic behind the reasoning process, t hey will be more likely to trust the pre-\\ndictions of an LLM. This is particularly important when appl ying LLMs in ﬁelds like medicine,\\neducation, and ﬁnance. Fourth, CoT is an in-context learnin g approach, and thus, it is applicable\\nto most well-trained, off-the-shelf LLMs. Moreover, CoT pr ovides efﬁcient ways to adapt LLMs\\nto different types of problems. It can even inspire more crea tive solutions by exploring various\\nalternative reasoning paths, which might not be obvious whe n arriving at a conclusion directly .\\nThe method described above requires providing one or more ex amples of CoT reasoning, typ-\\nically called the few-shot CoT method. By contrast, the zero -shot CoT method does not require\\nsuch examples. It instead prompts LLMs to reason step-by-st ep by incorporating speciﬁc instruc-\\ntions in prompts. For example, below is a zero-shot CoT promp t.\\nQ: Please calculate the average of the numbers 2, 4, and 9.\\nA: Let’s think step-by-step.\\nW e have three numbers: 2, 4, and 9. Add these numbers together,\\n2 + 4 + 9 = 15 . Determine how many numbers there are, which in this case\\nis three. The average is calculated by dividing the total sum by the number of\\nelements. Completing the division gives 15/3 = 5 . So the answer is 5.\\nFollowing the instruction “Let’s think step by step”, the LL M is prompted to generate detailed\\nreasoning steps. As discussed in Kojima et al. [2022]’s work, prompting with such instructions\\nmay result in LLMs generating only the reasoning steps witho ut a clear conclusion. In this case,\\na second round of prompting can be used to extract the answer f rom these reasoning steps. For\\nexample, Kojima et al. [2022] create a second prompt which combines both the input and out put'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 123, 'page_label': '117'}, page_content='3.2 Advanced Prompting Methods 117\\nin the ﬁrst round of prompting. Using this combined input, th e LLM can continue its reasoning\\nprocess and then generate the correct answer. Furthermore, it is possible to prompt LLMs to\\nreason using instructions other than “Let’s think step by st ep”, such as “Let’s think logically” and\\n“Please show me your thinking steps ﬁrst”.\\nWhile we have illustrated CoT methods using an algebraic rea soning problem, these methods\\ncan be applied to a variety of different problems. T ypical pr oblem-solving scenarios for CoT\\ninclude mathematical reasoning, logical reasoning, commo nsense reasoning, symbolic reasoning,\\ncode generation, and so on. See Figure 3.1 for more examples of applying CoT in various tasks.\\nCoT today is one of the most active ﬁelds of prompt engineerin g. This has not only led to im-\\nproved performance for LLM prompting but has opened the door to a wide range of methods for\\nstudying and verifying reasoning capabilities of LLMs. Alt hough we have focused on the basic\\nidea of CoT in this section, it can be improved in several ways . For example, we can consider the\\nreasoning process as a problem of searching through many pos sible paths, each of which may con-\\nsist of multiple intermediate states (i.e., reasoning step s). In general, we wish the search space to\\nbe well-deﬁned and sufﬁciently large, so that we are more lik ely to ﬁnd the optimal result. For this\\nreason, an area of current LLM research is aimed at designing better structures for representing\\nreasoning processes, allowing LLMs to tackle more complex r easoning challenges. These struc-\\ntures include tree-based structures [ Y ao et al. , 2024], graph-based structures [ Besta et al. , 2024],\\nand so on. By using these compact representations of reasoni ng paths, LLMs can explore a wider\\nrange of decision-making paths, analogous to System 2 think ing5 . Another line of research fo-\\ncuses on prompting LLMs with multi-round interactions. Thi s involves decomposing complex\\nproblems into sub-problems, verifying and reﬁning model ou tputs, employing model ensembling,\\nand so on. Note that these methods and the issues involved are not limited to CoT . In fact, they\\nare often used as more general approaches to improving LLMs, while CoT can be seen as a way\\nto test the capabilities of LLMs. W e will see discussions of s ome of these issues in the following\\nsubsections.\\nBefore leaving our discussion of CoT , we should consider its practical limitations. One of them\\nis the need for detailed, multi-step reasoning demonstrati ons in few-shot CoT scenarios, which\\nmay be difﬁcult to obtain, either automatically or manually . Also, there is no standard method for\\nbreaking down complex problems into simpler problem-solvi ng steps. This often heavily depends\\non the user’s experience. In addition, errors in intermedia te steps can also affect the accuracy of\\nthe ﬁnal conclusion. For further discussion on the pros and c ons of CoT , the interested reader can\\nrefer to recent surveys on this topic [ Chu et al. , 2023; Y u et al. , 2023; Zhang et al. , 2023a].\\n3.2.2 Problem Decomposition\\nW e have seen that LLMs can beneﬁt from solving a complex problem by breaking it down into\\nsimpler problem-solving tasks. Such an approach can be seen as an example of a broader paradigm\\nknown as problem decomposition , which has been extensively explored and discussed in psy-\\nchology and computer science. From the psychological persp ective, complex problem-solving\\nrefers to a process of addressing a problem using knowledge t hat helps overcome the barriers of\\n5 System 1 and System 2 thinking, as described by Kahneman [2011], represent two different modes of cognitive\\nprocessing. System 1 is fast, automatic, intuitive, and emo tional. This mode of thinking operates effortlessly and\\nquickly , and is often what guides our daily decisions, judgm ents, and impressions. System 2 is slow , deliberate, and\\nanalytical. It is activated when we need to perform complex c omputations.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 124, 'page_label': '118'}, page_content='118 Prompting\\nQ: What home entertainment equipment re-\\nquires cable?\\nAnswer Choices: (a) radio shack (b) substa-\\ntion (c) television (d) cabinet\\nA:\\nThe answer must require cable. Of the\\nabove choices, only television requires cable.\\nSo the answer is (c).\\nQ: What type of person typically contracts\\nillness?\\nAnswer Choices: (a) hospital (b) head (c)\\nsick person (d) elderly person (e) doctor’s of-\\nﬁce\\nA:\\nThe answer must be a person suscepti-\\nble to illness. Of the choices, \"elderly per\\nson\" is correct because they typically have\\nweaker immune systems.\\nSo the answer is (d).\\nQ: Do hamsters provide food for any ani-\\nmals?\\nA:\\nHamsters are prey animals. Prey are food\\nfor predators. Thus, hamsters provide food\\nfor some animals. So the answer is yes.\\nQ: Can you use Microsoft Ofﬁce without in-\\nternet?\\nA:\\nY es, you can use Microsoft Ofﬁce appli-\\ncations , although features like cloud stor-\\nage and online collaboration require inter-\\nnet. So the answer is yes.\\n(a) CSQA (b) StrategyQA\\nQ: Complete the rest of the sequence, mak-\\ning sure that the parentheses are closed prop-\\nerly . Input:[ {\\nA:\\nLet’s think step by step.\\n0: empty stack\\n1:[ ; stack: [\\n2: { ; stack: [ {\\nSo the answer is } ].\\nQ: Complete the rest of the sequence, mak-\\ning sure that the parentheses are closed prop-\\nerly . Input:<[ [\\nA:\\nLet’s think step by step.\\n0: empty stack\\n1: <; stack: <\\n2: [ ; stack: <[\\n3: [ ; stack: <[ [\\nSo the answer is ] ] >.\\nQ: T ake the last letters of the words in \"Elon\\nMusk\" and concatenate them.\\nA:\\nThe last letter of \"Elon\" is \"n\". The last\\nletter of \"Musk\" is \"k\". Concatenating them\\nis \"nk\". The answer is nk.\\nQ: T ake the last letters of each words in \"Re-\\nnata Mariela Mona Kristin\" and concatenate\\nthem.\\nA:\\nThe last letter of \"Renata\" is \"a\". The\\nlast letter of \"Mariela\" is \"a\". The last let-\\nter of \"Mona\" is \"a\"\". The last letter of\\n\"Kristin\" is \"n\". Concatenating them is\\n\"aaan\". The answer is aaan.\\n(c) Dyck languages (d) Last Letter Concatenation\\nFig. 3.1: CoT in four different reasoning tasks, including CSQA, Stra tegyQA, Dyck languages, and Last Letter Con-\\ncatenation. The CoT parts are highlighted in green.\\nthe problem\\n6 . There are generally no standard or clear paths to a solution for a complex prob-\\nlem. However, it is often advantageous to employ strategies that decompose the problem, thereby\\nmaking it easier to tackle the corresponding sub-problems w ith less effort. For example, consider\\nwriting a blog about the risks of AI. If we simply prompt an LLM with the instruction “Please\\nwrite a blog about the risks of AI”, the LLM may generate a blog with arbitrary structures and\\n6 A relatively formal deﬁnition can be found in Frensch and Funke [2014]’s book: complex problem-solving occurs\\nto overcome barriers between a given state and a desired goal state by means of behavioral and/or cognitive, multi-step\\nactivities.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 125, 'page_label': '119'}, page_content='3.2 Advanced Prompting Methods 119\\nwriting styles. A better method, instead, could be to outlin e the blog and provide more detailed\\ninformation about each section. Consider the following pro mpt\\nY ou are a blog writer. Please follow the provided outline bel ow to write a blog\\nabout the risks of AI.\\n• Introduction\\nIntroduce AI, its relevance, and the importance of understa nding its risks for youth.\\n• Privacy Concerns\\nDiscuss how AI might compromise personal privacy through in teractions online.\\n• Misinformation\\nExplore AI’s role in spreading misinformation and inﬂuenci ng young people’s deci-\\nsions.\\n• Cyberbullying\\nHighlight how AI tools can be utilized in cyberbullying and t he impact on mental\\nhealth.\\n• Tips for Safe AI Use\\nOffer guidelines for responsible AI usage and promote criti cal thinking.\\n• Conclusion\\nRecap main points and encourage proactive engagement with A I ethics.\\nHere we give the title and major points for each section. Then , the LLM can use this structure to\\nbreak down the writing task by ﬁlling in content for these sec tions. Note that the way to structure\\nthe blog can be provided by humans or even generated automati cally . For example, we can use\\nthe LLM to ﬁrst generate the outline, and then ask it to follow this outline to complete the writing.\\nIn computer science, decomposing complex problems is a comm only used strategy in software\\nand hardware system design. A well-known example is the divi de-and-conquer paradigm, which\\nis often used to design algorithms for computation problems that can be reduced to simpler, more\\nmanageable problems. For example, consider a problem of det ermining whether a document\\ndiscusses the risks of AI. W e can instruct the LLM with the fol lowing prompt.\\nY ou are provided with a text. Please determine whether it dis cusses the risks of\\nAI.\\n{∗document∗}\\nIf the document is long, the computation will be expensive. A lternatively , we can divide\\nthe document into relatively short segments and perform the same task on each segment. These\\nsegments can be processed in parallel to further reduce the c omputational cost. Next, we determine'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 126, 'page_label': '120'}, page_content='120 Prompting\\nthe relevancy of each segment to the topic of AI risks. The ﬁna l output is then generated using\\nanother prompt.\\nY our task is to determine whether a text discusses the risks o f AI. This text has\\nbeen divided into segments, and you have obtained the releva ncy of each segment\\nto the topic of AI risks. Based on this, please provide your ﬁn al result.\\nSegment 1: {∗relevancy-to-the-topic1∗}\\nSegment 2: {∗relevancy-to-the-topic2∗}\\nSegment 3: {∗relevancy-to-the-topic3∗}\\n...\\nNow let us return to a more general discussion of problem deco mposition in prompting. While\\nproblem decomposition can be applied to various NLP problem s, it has been more extensively\\ndiscussed and tested in reasoning tasks recently . For compl ex reasoning tasks, we often need\\na multi-step reasoning path to reach a correct conclusion. W e can use LLMs to achieve this in\\nthree different ways. First, LLMs can directly reach the con clusion. In other words, they can\\npredict without explicit reasoning processes, and there is a hidden and uninterpretable reasoning\\nmechanism. Second, LLMs are prompted to generate a multi-st ep reasoning path that leads to the\\nconclusion, like CoT . However, we run LLMs just once, and all intermediate steps in reasoning\\nare generated in a single prediction. Third, we break down th e original problem into a number of\\nsub-problems, which are either addressed in separate runs o f LLMs or tackled using other systems.\\nHere we focus our attention on the third approach, which is cl osely related to problem decompo-\\nsition. Note, however, that a more comprehensive discussio n could cover all these approaches,\\nwhile the ﬁrst two have been discussed to some extent in this c hapter.\\nA general framework for problem decomposition involves two elements.\\n• Sub-problem Generation . This involves decomposing the input problem into a number o f\\nsub-problems.\\n• Sub-problem Solving . This involves solving each sub-problem and deriving inter mediate\\nand ﬁnal conclusions through reasoning.\\nThese two issues can be modeled in different ways, leading to various problem decomposition\\nmethods. One approach is to treat them as separate steps in a t wo-step process. For example,\\nconsider the blog writing task described at the beginning of this subsection. In the ﬁrst step, we\\ndecompose the entire problem into sub-problems all at once ( i.e., outline the blog). In the second\\nstep, we solve the sub-problems either sequentially or in an other order (i.e., ﬁll in content for\\neach section as needed). The ﬁnal output of this process comb ines the results from solving each\\nsub-problem. While this method is simple and straightforwa rd, it assumes that the problem is\\ncompositional, making it more suitable for tasks like writi ng and code generation.\\nHowever, many real-world problems require complex reasoni ng. One key characteristic of\\nthese problems is that the reasoning steps may not be ﬁxed. Th e reasoning path can vary for\\ndifferent problems, and each step of reasoning may depend on the outcomes of prior steps. In'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 127, 'page_label': '121'}, page_content='3.2 Advanced Prompting Methods 121\\nsuch cases, it is undesirable to use ﬁxed sub-problem genera tion in advance. Instead, sub-problems\\nshould be generated dynamically based on the input problem, and, if possible, generated on the\\nﬂy during the reasoning process. This makes problem decompo sition more challenging compared\\nwith designing divide-and-conquer algorithms. Ideally , w e would like to jointly design both the\\nsystems for sub-problem generation and sub-problem solvin g. But a more practical and widely\\nused approach is to adopt separate models for these tasks. A s traightforward way to achieve this\\nis to adapt an LLM for these tasks by either prompting or tunin g the model.\\nHere we consider a method based on the above idea, called least-to-most prompting [Zhou et al. ,\\n2023b]. The motivation for this method arises from the challenges of solving difﬁcult reasoning\\nproblems — those that cannot be addressed by simply generali zing from a few examples. For\\nthese problems, a more effective problem-solving strategy is to follow a progressive sequence of\\nsub-problems that systematically lead to the conclusion. M ore speciﬁcally , in the least-to-most\\nprompting method, sub-problem generation is performed by p rompting an LLM with instructions\\nand/or demonstrations. For example, below is a 2-shot promp t for sub-problem generation in\\nleast-to-most prompting.\\nT ASK Y our task is to decompose a problem into several sub-problem s. Y ou will\\nbe given a few examples to illustrate how to achieve this.\\nDEMO Q: In a community , 5% of the population are infants, 15% are ch ildren,\\n40% are adults, and 40% are seniors. Which group makes up the l argest\\nportion of the population?\\nA: T o answer the question “Which group makes up the largest portion of the\\npopulation?”, we need to know: “\\nHow many percent are infants?”, “ How\\nmany percent are children?”, “ How many percent are adults?”, “ How many\\npercent are seniors?”.\\nQ: Alice, Bob, and Charlie brought beads for their group proj ect in their\\ncraft class. Alice has twice as many beads as Bob, and Bob has ﬁ ve times\\nas many beads as Charlie. If Charlie has 6 beads, how many bead s can they\\nuse for their craft project?\\nA: T o answer the question “How many beads can they use for their craft\\nproject?”, we need to know: “\\nHow many beads does Bob have?”, “ How\\nmany beads does Alice have?”.\\nUSER Q: The environmental study conducted from 2015 to 2020 revea led that the\\naverage temperature in the region increased by 2.3 degrees C elsius. What\\nwas the duration of the environmental study?\\nA: T o answer the question “\\nWhat was the duration of the environmental\\nstudy?”, we need to know: “ When did the environmental study start?”,\\n“ When did the environmental study end?”.\\nBy learning from the examples, the LLM can generate two sub-p roblems for answering the\\nnew problem “What was the duration of the environmental stud y?” (highlighted in blue and\\norange). Given these sub-problems, we solve them sequentia lly . For each sub-problem, we take\\nall previously-generated QA pairs as context, and then prod uce the answer. For the example above,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 128, 'page_label': '122'}, page_content='122 Prompting\\nwe need to answer the ﬁrst sub-problem by prompting the LLM, l ike this\\nThe environmental study conducted from 2015 to 2020 reveale d that\\nthe average temperature in the region increased by 2.3 degre es Celsius.\\nSUB-PROB1 Q: When did the environmental study start?\\nA: The environmental study started in 2015.\\nOnce we have the answer to the ﬁrst sub-problem, we proceed to the second one. This time,\\nwe include both the ﬁrst sub-problem and its corresponding a nswer in the input.\\nThe environmental study conducted from 2015 to 2020 reveale d that\\nthe average temperature in the region increased by 2.3 degre es Celsius.\\nSUB-PROB1 Q: When did the environmental study start?\\nA: The environmental study started in 2015.\\nSUB-PROB2 Q: When did the environmental study end?\\nA: The environmental study started in 2020.\\nFinally , we use the LLM to solve the original problem given th e answers to all the sub-\\nproblems.\\nThe environmental study conducted from 2015 to 2020 reveale d that\\nthe average temperature in the region increased by 2.3 degre es Celsius.\\nSUB-PROB1 Q: When did the environmental study start?\\nA: The environmental study started in 2015.\\nSUB-PROB2 Q: When did the environmental study end?\\nA: The environmental study started in 2020.\\nFINAL Q: What was the duration of the environmental study?\\nA: The duration of the environmental study was 5 years.\\nThe least-to-most method offers a basic approach to prompti ng LLMs to generate and solve\\nsub-problems separately . W e can improve it in several ways. One simple improvement is to apply\\nvarious advanced prompting techniques, which do not requir e changes to the problem decom-\\nposition framework. For example, we can incorporate CoT int o the prompting to enhance the\\nreasoning performance of sub-problem generation and solvi ng.\\nAnother improvement is to explore methods for better decomp osing problems and organizing\\nproblem-solving paths. T o describe these approaches, we wi ll use the symbol p0 to denote the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 129, 'page_label': '123'}, page_content='3.2 Advanced Prompting Methods 123\\ninput problem, and use the symbols {p1 ,...,pn } to denote the sub-problems corresponding to p0.\\nFor least-to-most prompting, we decompose p0 into {p1 ,...,pn }, given by\\n{p1 ,...,pn } = G(p0 ) (3.2)\\nwhere G(·) denotes the function of sub-problem generation. Then, we so lve the sub-problems\\n{p1,...,pn } sequentially , resulting in a sequence of answers {a1 ,...,an }. For answering the i-th\\nsub-problem pi, we include both the original problem p0 and all previously-seen problem-answer\\npairs in the context for prediction. The answer ai is given by\\nai = Si(pi ,{p0 ,p<i,a<i}) (3.3)\\nwhere p<i = {p1 ,...,pi−1 } and a<i = {a1 ,...,ai−1 }. Si(·) denotes the function that solves the\\nsub-problem pi given the context {p0,p<i ,a<i}. The last step is to generate the answer to the\\noriginal problem p0, which can be expressed in a similar manner to Eq. ( 3.3).\\na0 = S0(p0 ,{p≤n,a≤n}) (3.4)\\nOne way to reﬁne this model is to modify the G(·) function so that the model can dynamically\\ngenerate answers. Instead of generating all sub-problems a t one time, we can generate each of\\nthem during problem-solving [ Dua et al. , 2022]. T o do this, we can replace Eq. ( 3.2) with\\npi = Gi(p0 ,{p<i,a<i }) (3.5)\\nHence we obtain a sub-problem generation model that operate s in a step-by-step manner. At each\\nstep i, we ﬁrst generate the sub-problem pi by prompting an LLM with the original problem p0\\nand the problem-solving history {p<i,a<i}. W e then generate the answer ai for this sub-problem\\nusing the same or a different LLM, based on the same contextua l information (see Eq. ( 3.3)). This\\nmethod effectively expands the reasoning capacity of LLMs b y allowing them to dynamically\\ngenerate and solve sub-problems in intermediate reasoning steps. As a result, the reasoning paths\\nare not ﬁxed in advance, and the models can choose and adapt th eir reasoning strategies during\\nproblem-solving.\\nAnother way to improve the above model is to focus on developi ng better sub-problem solvers.\\nIn our previous discussion, we restricted Si(·) to LLMs that are prompted to solve the sub-problem\\npi. In fact, we can expand this function to any system that is cap able of addressing the sub-\\nproblem. For example, Si(·) could make calls to IR systems, thereby allowing us to access a\\nbroader range of data for problem-solving. Another example is using Si(·) as a calculator to\\naccurately compute results in mathematical problem-solvi ng. If the sub-problem pi is complex\\nand requires multiple intermediate problem-solving steps , it is also possible to further decompose\\npi into smaller sub-problems. For example, Si(·) can be deﬁned as a recursive program that\\ngenerates and solves sub-problems. This incorporates recu rsion into problem-solving and allows\\nus to address problems by iteratively decomposing them. As a result, we can deﬁne a hierarchical\\nstructure for problem-solving [ Khot et al. , 2023].\\nIf we generalize the above formulation a bit further, we can c onsider it as a reinforcement\\nlearning problem. A typical method is to model a problem-sol ving process as a decision making\\nprocess. In each step of this process, an action is taken base d on the current state. These actions'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 130, 'page_label': '124'}, page_content='124 Prompting\\ncan include all functions for sub-problem generation and so lving (i.e., Gi(·) and Si(·)). Thus,\\nthe action sequence corresponds to a problem-solving path. Since the discussion of reinforcement\\nlearning problems is beyond the scope of this chapter, we ski p the precise description of this\\nlearning task. Nevertheless, developing an agent or contro ller to determine when and how to\\ngenerate and solve a sub-problem is also a natural choice.\\nIn NLP , problem decomposition is related to a long line of res earch on multi-hop question\\nanswering [ Mavi et al. , 2024]. This task requires the system to gather and combine inform ation\\nfrom multiple pieces of text to provide an accurate answer to a complex question. For example,\\nto answer the question “What is the capital of the country whe re Albert Einstein was born?”, we\\nneed to know “Where Albert Einstein was born?” and “What’s th e capital of Germany?”. Earlier\\nwork in this area and related ones has investigated the issue of problem decomposition, though the\\nmethods might not be based on LLMs. For example, a popular met hod is to develop an additional\\nneural model to generate simpler questions that address dif ferent aspects of the original question\\n[Andreas et al. , 2016; T almor and Berant , 2018; Min et al. , 2019]. This question generator can\\ncreate questions in a batch or sequential manner.\\nBroadly speaking, problem decomposition is also related to the compositionality issue in NLP\\n[Drozdov et al. , 2022; Press et al. , 2023]. For example, in semantic parsing, we map natural lan-\\nguage sentences into structured meaning representations b y breaking them down into constituent\\nparts and understanding the sentences based on the meanings of these parts and the rules used to\\ncombine them. In early studies of this ﬁeld, highly composit ional sentences were considered easier\\nfor testing systems, as it is relatively straightforward to decompose such sentences and compose\\nthe meanings of their parts. However, the task becomes much m ore difﬁcult when more gener-\\nalization is required for modeling compositionality in new data. In this case, we want systems\\nto have improved abilities of compositional generalization . In more recent research on LLMs,\\nthis issue has been frequently discussed in compositional r easoning tasks, such as SCAN 7, as it\\nis considered an important aspect of testing the language un derstanding and reasoning abilities\\nof LLMs. This also presents new tasks for developing and exam ining problem decomposition\\nmethods.\\nIn LLMs, one interesting application of problem decomposit ion is tool use. In some cases,\\nit is necessary to integrate external tools into LLMs to acce ss accurate data not available during\\ntraining or ﬁne-tuning. For example, LLMs can integrate wit h APIs to fetch real-time data such\\nas weather updates, stock market prices, or news feeds, enab ling them to provide up-to-date re-\\nsponses to user queries. When using tools, LLM predictions m ight include markers that indicate\\nwhere and how to call external APIs. This requires decomposi ng the problem into sub-problems,\\nwith some handled by the LLMs and others by external tools. Mo re detailed discussions on this\\ntopic will be presented in Section 3.2.5.\\n3.2.3 Self-reﬁnement\\nIn many cases, predictions of LLMs can be inaccurate or incorrect. Given that current LLMs can\\nperform tasks like reﬁnement and correction, it makes sense to explore methods for these models\\nto self-reﬁne their outputs. Self-reﬁnement is a common phe nomenon in human psychological\\n7 The SCAN tasks (Simpliﬁed versions of the CommAI Navigation tasks) are designed to evaluate the ability of\\nLLMs to perform compositional generalization [ Lake and Baroni , 2018]. They involve translating natural language\\ncommands into a sequence of actions. For example, a command “ jump opposite left and walk thrice” can be translated\\ninto the action sequence “L TURN L TURN JUMP W ALK W ALK W ALK”.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 131, 'page_label': '125'}, page_content='3.2 Advanced Prompting Methods 125\\nactivities and daily behavior. For example, when designing a product, a designer might ﬁrst create\\na basic prototype, then reﬁne the design after evaluation an d testing to enhance user experience\\nand functionality . The reﬁnement can be iterated several ti mes until the design is satisfactory . The\\nidea of predict-then-reﬁne can also be found in NLP . One earl y example is Brill’s tagger [ Brill,\\n1992], where an initial POS tagging result of a sentence can be ite ratively reﬁned using a rule-\\nbased system. In the era of deep learning, a good deal of work o n sequence-to-sequence problems,\\nsuch as grammar correction and text rewriting, can also be se en as examples on this theme.\\nW e can prompt LLMs to do self-reﬁnement. Consider a simple ex ample where we ﬁrst use\\nan LLM to translate a text from Chinese to English and then use the same model to reﬁne the\\ntranslation. Here is how we can prompt the model to translate :\\nTranslate the following text from Chinese to English:\\nChinese:\\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\\n现较为明显的社会分化，进入文明起源的加速阶段。\\nEnglish:\\nA series of discoveries have cemented the signiﬁcant role of the Hongshan\\nculture in studies on the origins of Chinese civilization. The latest research\\nﬁndings suggests that, starting from around 5800 years ago, many regions\\nacross China began to experience noticeable social differentiations, entering an\\naccelerated phase in the origin of civilization.\\nThen, we can prompt it to reﬁne the translation:\\nNiuTrans'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 132, 'page_label': '126'}, page_content='126 Prompting\\nY ou are provided with a Chinese text along with its English tr anslation. Please\\nreﬁne the translation to improve its accuracy and ﬂuency if n ecessary .\\nChinese:\\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。最新公\\n布的研究成果认为，大约从距今5800年开始，中华大地上各个区域相继出\\n现较为明显的社会分化，进入文明起源的加速阶段。\\nEnglish:\\nA series of discoveries have cemented the signiﬁcant role ofthe Hongshan cul-\\nture in studies on the origins of Chinese civilization. The l atest research ﬁndings\\nsuggests that, starting from around 5800 years ago, many reg ions across China\\nbegan to experience noticeable social differentiations, e ntering an accelerated\\nphase in the origin of civilization.\\nReﬁnement:\\nA series of\\narchaeological discoveries have cemented the signiﬁcant role\\nof the Hongshan culture in studies on the origins of Chinese c ivilization.\\nThe latest research ﬁndings suggest that, starting from around 5800 years\\nago, various regions across China began to experience noticeable soc ial\\ndifferentiations, entering an accelerated phase in the ori gin of civilization.\\nHere modiﬁcations to the input translation are highlighted in green. This process can be iterative:\\nat each step, we use the translation from the previous step as input, and prompt the LLM to produce\\na potentially better translation.\\nIn the above example, the reﬁnement of the initial model outp ut was achieved by prompting the\\nLLM with a simple instruction like “Please reﬁne it!”. Howev er, the reﬁnement is based solely on\\nthe instruction-following ability of the LLM, and there is n o guidance or supervision on how and\\nwhere to improve the model output. A more effective approach would be to consider feedback on\\nspeciﬁc aspects that need reﬁnement. For example, we can pro mpt the LLM with “Please correct\\nall the grammatical errors in the translation”, so that the m odel can focus more on grammatical\\nerror correction during reﬁnement.\\nA general framework of self-reﬁnement with LLMs involves th ree steps [ Madaan et al. , 2024].\\n• Prediction. W e use an LLM to produce the initial model output.\\n• Feedback Collection . W e obtain feedback on the model output.\\n• Reﬁnement . W e use the LLM to reﬁne the model output based on the feedback .\\nThe last two steps can be repeated multiple times, which lead s to an iterative self-reﬁnement\\nprocess. In this framework, receiving accurate and detaile d feedback is critical as it guides the sub-\\nsequent reﬁnement step. In general, feedback can be obtaine d in various ways [ Fernandes et al. ,\\n2023; Pan et al. , 2024]. W e can manually examine the model’s predictions to identi fy issues. Also,\\nwe can use a feedback model to automatically generate feedba ck. For instance, a reward model\\ntrained on some labeled data can help evaluate the model outp ut and return a score indicating its\\nquality .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 133, 'page_label': '127'}, page_content='3.2 Advanced Prompting Methods 127\\nSince the focus of this chapter is on LLM prompting, we consid er use LLMs to provide feed-\\nback. T o illustrate, let us consider an example of generatin g good responses to user questions:\\n• First, we prompt an LLM to answer the input question, as usua l.\\nGenerate a response to the question: “What are some major env ironmental\\nconcerns today?”\\nResponse:\\nMajor environmental concerns today include climate change, air and water\\npollution, deforestation, loss of biodiversity , and ocean degradation.\\n• Then, using the following prompt, we prompt the same LLM aga in to get feedback on the\\ngenerated response.\\nY ou will be provided with a question and its corresponding re sponse. Eval-\\nuate the accuracy and depth of this response, and identify an y factual in-\\naccuracies or areas where additional details could enhance understanding.\\nQuestion:\\nWhat are some major environmental concerns today?\\nResponse:\\nMajor environmental concerns today include climate change, air and water\\npollution, deforestation, loss of biodiversity , and ocean degradation.\\nFeedback:\\nThe response provided is accurate and highlights key environmental\\nconcerns. However, it would beneﬁt from more speciﬁc and detailed\\ndescriptions of these concerns. Here are suggestions for improvement:\\n1. Include speciﬁc examples of environmental issues.\\n2. Add information about major pollutants.\\n3. Specify the regions most affected.\\n• Given the feedback the LLM produces, we prompt it to reﬁne th e response.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 134, 'page_label': '128'}, page_content='128 Prompting\\nY ou will be provided with a question, its corresponding resp onse, and\\nsome feedback on this response.\\nQuestion:\\nWhat are some major environmental concerns today?\\nResponse:\\nMajor environmental concerns today include climate change, air and water\\npollution, deforestation, loss of biodiversity , and ocean degradation.\\nFeedback:\\nThe response provided is accurate and highlights key environmental con-\\ncerns. However, it would beneﬁt from more speciﬁc and detail ed descrip-\\ntions of these concerns. Here are suggestions for improveme nt:\\n1. Include speciﬁc examples of environmental issues.\\n2. Add information about major pollutants.\\n3. Specify the regions most affected.\\nNow reﬁne the response based on the provided feedback.\\nReﬁned Response:\\nKey environmental concerns include climate change, evidenced by rising\\nglobal temperatures and severe weather patterns; signiﬁcant air and\\nwater pollution in urban and industrial areas; rampant deforestation in\\nregions like the Amazon, causing biodiversity loss; and ocean degradation,\\nhighlighted by coral reef bleaching and widespread overﬁshing.\\nIdeally , if a strong LLM is adopted, we would like to have it pe rform all three steps without\\nextra training. On the other hand, if we have enough labeled d ata for the task of interest, we can\\nenhance the performance of the LLM using supervised learnin g. For example, we can ﬁne-tune\\nthe LLM to better adapt it to reﬁnement tasks, or alternative ly , use task-speciﬁc models, which\\nmay not necessarily be based on LLMs [ W elleck et al. , 2023; Schick et al. , 2023]. In a broader\\nsense, improving LLMs for self-reﬁnement tasks can be seen a s an alignment issue. For example,\\nit has been found that some self-correction abilities can be activated through RLHF [ Ganguli et al. ,\\n2023]. However, discussing these issues is beyond the scope of th is chapter. Further discussion\\ncan be found in Chapter 4.\\nIn LLMs, self-reﬁnement is related to several concepts that reveal the psychological aspects\\nof these models, such as the ability to self-reﬂect. A view is that if LLMs are capable of self-\\nreﬂection, their predictions can become more accurate and e ven possess self-correcting capabili-\\nties. This self-reﬂection can be activated in various ways, for example, by prompting these LLMs\\nto engage in more in-depth and careful thinking, or by provid ing examples from which the models\\ncan learn and reﬂect. T o illustrate, we consider here the deliberate-then-generate (DTG) method\\npresented in Li et al. [2023a]’s work, where LLMs are prompted to deliberate. In DTG, we ar e\\ngiven an initial model output which may contain errors. LLMs are then prompted to identify the\\nerror types of this model output and provide an improved outp ut. Below is a template of DTG\\nprompting for Chinse-to-English translation tasks.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 135, 'page_label': '129'}, page_content='3.2 Advanced Prompting Methods 129\\nGiven the Chinese sentence: {∗source∗}\\nThe English translation is: {∗target∗}\\nPlease ﬁrst detect the type of error, and then reﬁne the trans lation.\\nError T ype:\\nW e aim to ﬁrst predict the error type (red), and then produce a reﬁned translation (blue). This\\nprocess of deliberation is guided by the instruction “Pleas e ﬁrst detect the type of error, and then\\nreﬁne the translation”. It encourages LLMs to initially eng age in thoughtful analysis and then give\\nbetter results. Since error type prediction and reﬁnement a re performed in a single run of LLMs,\\nthis method incorporates both steps of feedback and reﬁneme nt into one process.\\nIn the above prompts, we assume that the LLM we use is able to re view the input translation\\nand correctly identify its error types. However, this raise s new difﬁculties as the model may not\\nbe good at ﬁnding errors in translations. This will in turn re sult in extra ﬁne-tuning or prompt-\\ning engineering efforts. So a simpler method is to reduce the burden of error identiﬁcation and\\nuse LLMs for deliberation only . T o do this, we can replace the input translation with a random\\ntranslation and assign a default error type. An example of su ch a prompt is shown below .\\nGiven the Chinese sentence:\\n一系列考古发现奠定红山文化在中华文明起源研究中的重要地位。\\nThe English translation is:\\nA variety of innovative techniques have redeﬁned the importance of modern art\\nin contemporary cultural studies.\\nPlease ﬁrst detect the type of error, and then reﬁne the trans lation.\\nError T ype: Incorrect Translation\\nIn this example, the input translation is not generated by LL Ms but is instead randomly sam-\\npled from the dataset. So it is simply an incorrect translati on for the source sentence, and we can\\nset the error type accordingly . The LLMs then generate a new t ranslation by taking both the source\\nsentence and the incorrect translation as input. The design of this prompt can also be considered as\\nactivating the learning capabilities of LLMs through “nega tive evidence” [ Marcus, 1993], thereby\\nenabling them to reﬂect and produce better outcomes through contrastive analysis. Nevertheless,\\nthis method does not rely on any feedback and can enhance the p erformance of a single LLM\\nprediction via simple prompting.\\nNote that while DTG is non-iterative, iterative learning an d reﬁnement are commonly used\\nin NLP . An advantage of these iterative approaches are that t hey mimics human learning and\\nproblem-solving, where continuous feedback and adjustmen ts lead to progressively improved out-\\ncomes. Iterative methods can be applied to a range of LLM prom pting problems. For example, in\\nproblem decomposition, one can incorporate new sub-proble ms and their solutions into the con-\\ntext at each step, and thus LLMs can progressively approach t he solution of the original problem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 136, 'page_label': '130'}, page_content='130 Prompting\\nOn the other hand, iterative methods raise several issues th at are absent in non-iterative meth-\\nods, for example, errors in earlier steps may negatively imp act subsequent problem-solving, and\\ndetermining when to stop iterating often requires addition al engineering effort.\\n3.2.4 Ensembling\\nModel ensembling for text generation has been extensively discussed in the NLP literature. The\\nidea is to combine the predictions of two or more models to gen erate a better prediction. This\\ntechnique can be directly applicable to LLMs. For example, w e can collect a set of LLMs and run\\neach of them on the same input. The ﬁnal output is a combined pr ediction from these models.\\nFor LLM prompting, it is also possible to improve performanc e by combining predictions\\nbased on different prompts. Suppose we have an LLM and a colle ction of prompts that address\\nthe same task. W e can run this LLM with each of the prompts and t hen combine the predictions.\\nFor example, below are three different prompt templates for text simpliﬁcation.\\nMake this text simpler.\\n{∗text∗}\\nCondense and simplify this text.\\n{∗text∗}\\nRewrite for easy reading.\\n{∗text∗}\\nEach of these prompts will lead to a different prediction, an d we can consider all three predictions\\nto generate the ﬁnal one.\\nFormally , let {x1 ,...,xK } be Kprompts for performing the same task. Given an LLM Pr(·|·),\\nwe can ﬁnd the best prediction for each xi using ˆyi = arg max yi Pr(yi |xi ). These predictions\\ncan be combined to form a “new” prediction:\\nˆy = Combine(ˆ y1 ,..., ˆyK ) (3.6)\\nHere Combine(·) is the combination model, which can be designed in several di fferent ways. For\\nexample, we can select the best prediction by voting or by ide ntifying the one that overlaps the\\nmost with others. Another method for model combination is to perform model averaging during\\ntoken prediction. Let ˆyj be the predicted token at the j-th step for model combination. The'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 137, 'page_label': '131'}, page_content='3.2 Advanced Prompting Methods 131\\nprobability of predicting ˆyj is given by\\nˆyj = arg max\\nyj\\nK∑\\nk=1\\nlog Pr( yj |xk ,ˆy1,..., ˆyj−1 ) (3.7)\\nIn ensembling for LLM prompting, it is generally advantageo us to use diverse prompts so that\\nthe combination can capture a broader range of potential res ponses. This practice is common in\\nensemble learning, as diversity helps average out biases an d errors that may be speciﬁc to any\\nsingle model or conﬁguration. From the Bayesian viewpoint, we can treat the prompt x as a latent\\nvariable, given the problem of interest, p. This allows the predictive distribution of y given pto\\nbe written as the distribution Pr(y|x) marginalized over all possible prompts\\nPr(y|p) =\\n∫\\nPr(y|x) Pr( x|p)dx (3.8)\\nThe integral computes the total probability of y by considering all possible values of x, weighted\\nby their likelihoods given p. Here Pr(y|x) is given by the LLM, and Pr(x|p) is the prior distri-\\nbution of prompts for the problem. This is a good model becaus e the integral effectively accounts\\nfor the uncertainty in the choice of x, ensuring that the ﬁnal predictive distribution Pr(y|p) is\\nrobust and encompasses all potential variations and biases in the prompts. However, computing\\nthis integral directly can be computationally infeasible d ue to the potentially inﬁnite space of x.\\nOne approach to addressing this issue is to employ methods li ke Monte Carlo sampling, which\\napproximate the integral using a manageable, ﬁnite number o f prompts.\\nWhile the Bayesian treatment is mathematically well-deﬁne d, it is common practice in NLP\\nto assume a non-informative or uniform prior and focus inste ad on constructing a set of diverse\\nprompts. Consequently , the output can be computed using a st raightforward combination model,\\nas described in Eq. ( 3.6). The issue of creating high-quality , diverse prompts has b een studied in\\nCoT and other in-context learning areas. Most of the researc h focuses on incorporating a variety\\nof demonstration examples across different prompts. Here, we list some of these methods.\\n• Given a problem, we manually create a number of demonstrati ons and use different ones\\nfor different prompts.\\n• Given a problem, we use LLMs to automatically generate demo nstrations and prompts.\\n• Given a prompt, we create different prompts by changing the order of demonstrations in the\\nprompt.\\n• Given a prompt, we use LLMs to generate a number of similar pr ompts.\\n• Given a prompt, we transform it into other forms, e.g., tran slating it into other languages.\\nOf course, in practice, we can combine these methods to achie ve greater diversity . An underly-\\ning assumption here is that diverse prompts can lead to diver se model outputs. This is particularly\\nthe case when the problem we deal with is relatively new and di fﬁcult. For stronger and more ro-\\nbust LLMs, the variance in output for similar prompts might n ot be large. In this case, the beneﬁt\\nof involving multiple prompts can be modest.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 138, 'page_label': '132'}, page_content='132 Prompting\\nIn addition to providing diverse prompts for LLMs, another a pproach is to make use of the\\ninherent variance in the outputs of LLMs. One simple way to ge nerate multiple outputs is to\\nsample outputs from the hypothesis space. This is straightf orward for LLMs because their search\\nalgorithms are naturally suited to producing multiple resu lts. For example, if beam search is\\nused, we can easily collect all the complete hypotheses in th e beam to generate multiple output\\ncandidates. Also, we can modify the search algorithms to imp lement more powerful sampling\\nmethods and explore a wider range of high-quality hypothese s.\\nThe next question is how to combine diverse model prediction s. This leads us back to the\\nmodel combination issue as described in Eq. ( 3.6). Here we consider the self-consistency method,\\nwhich outputs not the prediction with the highest probabili ty , but rather the one that best aligns\\nwith other predictions [ W ang et al. , 2022a; 2023b]. First, an LLM is prompted with CoT as\\nusual and generates multiple reasoning paths by sampling. T hen, the frequency of each answer is\\ncounted across these reasoning paths. The ﬁnal output is the answer that has the highest count.\\nBelow shows three different predictions by prompting an LLM with CoT .\\nPrediction 1 (Correct):\\nQ: Three friends play a game where they ﬂip a fair coin. Each fr iend ﬂips the\\ncoin once. What is the probability that exactly one of them ﬂi ps heads?\\nA: Let’s think step by step.\\nEach coin ﬂip is an independent event with a 50% chance of landing heads\\n(H) and 50% tails (T). Consider the possible outcomes for three ﬂips: HHH,\\nHHT , HTH, HTT , THH, THT , TTH, TTT . Out of these, only HHH, HHT , HTH,\\nand THH are relevant. W e are only interested in scenarios with exactly one\\nhead: HTT , THT , and TTH. There are 3 such outcomes out of 8 total, giving a\\nprobability of 3/8.\\nPrediction 2 (Correct):\\nQ: Three friends play a game where they ﬂip a fair coin. Each fr iend ﬂips the\\ncoin once. What is the probability that exactly one of them ﬂi ps heads?\\nA: Let’s think step by step.\\nUsing a binomial probability formula, where p(success or heads) is 0.5, and\\neach of the 3 trials (ﬂips) is independent, we look for exactly one success\\n(head). The formula for exactly k successes in n trials is given by the\\nbinomial coefﬁcient formula\\n( n\\nk\\n)\\npk (1 − p)n−k . Plugging in the values, we get( 3\\n1\\n)\\n(0.5)1 (0.5)3−1 = 3 · 0.5 · 0.25 = 0 .375.\\nPrediction 3 (Wrong):'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 139, 'page_label': '133'}, page_content='3.2 Advanced Prompting Methods 133\\nQ: Three friends play a game where they ﬂip a fair coin. Each fr iend ﬂips the\\ncoin once. What is the probability that exactly one of them ﬂi ps heads?\\nA: Let’s think step by step.\\nSince each coin ﬂip has a 50% chance of landing heads and a 50% chance of\\nlanding tails, and we are looking for the case where only one of the three ﬂips\\nresults in heads, we might initially think this is simply a matter of picking which\\nﬂip is the heads. There are three ﬂips, so one might incorrectly assume that the\\nchance of picking one speciﬁc outcome like this would be 1 out of 3. Thus, they\\nmight conclude that the probability of exactly one head is 1/3 = 33 .3%.\\nPredictions 1 and 2 correctly identify the three cases where exactly one head is ﬂipped, both\\nobtaining a probability of 37.5%. The reasoning in Predicti on 3 fails to account for the total num-\\nber of outcomes possible with three coin ﬂips, thus giving a w rong answer of 33.3%. Therefore,\\nwe select 37.5% as the ﬁnal answer because it is the consensus .\\nSelf-consistency provides a criterion for determining the best prediction in a pool of can-\\ndidates. Since the prompt and the model are ﬁxed in this metho d, it is not strictly a prompt\\nensembling method. Instead, it can be seen as an instance of o utput ensembling methods, also\\nknown as hypothesis selection methods, which have long been explored in NLP , particularly for\\ntext generation problems [ Xiao et al. , 2013]. In these methods, multiple outputs are generated by\\nvarying model architectures or parameters. Each output is t hen assigned a score by some criterion,\\nand the outputs are re-ranked based on these scores. There ar e various ways to deﬁne the scoring\\nfunction, such as measuring the agreement between an output and others, and using a stronger\\nmodel to rescore each output 8 . Figure 3.2 shows a comparison of different ensembling methods\\nfor LLMs.\\nNow , let us brieﬂy review the methods we have discussed so far in this section, such as problem\\ndecomposition and self-reﬁnement. It is apparent that thes e methods enhance decision-making by\\nintroducing more “choices” into the reasoning process. T o s ome extent, they all involve evalu-\\nating and providing feedback on the results of LLMs. For exam ple, in self-reﬁnement, we need\\nto offer suggestions for improving the prediction of LLMs, a nd in output ensembling, we select\\nthe optimal output from a pool of candidates. In this sense, t hese methods fall under the broader\\ncategory of predict-then-verify approaches, where predic tions are initially made, then veriﬁed and\\nreﬁned. The fundamental problem here involves verifying an d evaluating the reasoning results\\nor intermediate steps. This issue is somewhat related to the problem of training reward models\\nin RLHF , although RLHF addresses a different aspect. In fact , the development of veriﬁers has\\nbeen explored and implemented in reasoning with LLMs. Most w ork, rather than developing\\nheuristic-based inference-time algorithms, focuses on le arning veriﬁers in a supervised manner. A\\nstraightforward method is to train veriﬁers as binary class iﬁers, such as classifying an answer as\\n8 An interpretation of self-consistency is to view it as a mini mum Bayes risk search process. It searches for the best\\noutput by minimizing the Bayes risk. More speciﬁcally , a ris k function R(y, yr ) is deﬁned on each pair of outputs\\n(denoted by (y, yr )), representing the cost of replacing y with yr . Given a set of outputs Ω , the risk of an output\\ny ∈ Ω is given by\\nRisk(y) = Eyr ∼Pr(yr |x) R(y, yr )\\n=\\n∑\\nyr ∈Ω\\nR(y, yr ) · Pr(yr |x) (3.9)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 140, 'page_label': '134'}, page_content='134 Prompting\\nLLM2\\nLLM1\\nLLM2\\nPrompt Prediction2\\nPrediction1\\nPrediction3\\nCombine/Select\\nFinal\\nPrediction\\n(a) Model Ensembling\\nLLMPrompt2\\nPrompt1\\nPrompt3\\nPrediction2\\nPrediction1\\nPrediction3\\nCombine/Select\\nFinal\\nPrediction\\n(b) Prompt Ensembling\\nLLMPrompt Prediction2\\nPrediction1\\nPrediction3\\nCombine/Select\\nFinal\\nPrediction\\nSample\\n(c) Output Ensembling\\nFig. 3.2: Ensembling methods for LLMs. In standard model ensembling ( a), multiple LLMs varying in architectures or\\nparameters are used. Each LLM receives the same prompt and pr oduces a prediction. These predictions are combined\\nto generate the ﬁnal prediction. In prompt ensembling (b), w e have one LLM and multiple prompts. The LLM produces\\na prediction for each prompt, and these predictions are comb ined as usual. In output ensembling (c), the LLM samples\\nmultiple predictions over the prediction space given a prom pt. It can be seen as a method to boost the performance\\nof the LLM itself. Note that these ensembling methods can be c ombined to increase the diversity of predictions. For\\nexample, we can use both prompt ensembling and output ensemb ling to obtain more diverse predictions.\\ncorrect or incorrect, although these veriﬁers are typicall y used as scoring models. Given a reason-\\ning path for a problem, the veriﬁers can be used to score eithe r the entire path (called outcome-\\nbased approaches) [ Cobbe et al. , 2021], or each individual reasoning step (called process-based\\napproaches) [ Uesato et al. , 2022; Lightman et al. , 2024].\\n3.2.5 RAG and T ool Use\\nRAG is generally employed when standard LLMs, which rely solely on pre-trained knowledge,\\nlack accuracy and depth in the generated text. By drawing fro m external databases and documents,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 141, 'page_label': '135'}, page_content='3.2 Advanced Prompting Methods 135\\nRAG can signiﬁcantly improve the quality of responses, ensu ring they are both contextually rel-\\nevant and factually correct. Such an approach is particular ly useful in scenarios that require high\\nfactual accuracy and up-to-date information, such as compl ex question answering.\\nThe concept of RAG has been mentioned several times in the pre vious sections and chapters.\\nFor completeness, we outline the key steps involved in RAG he re.\\n• W e prepare a collection of texts which are treated as an addi tional source of knowledge we\\ncan access.\\n• W e retrieve relevant texts for a given query .\\n• W e input both the retrieved texts and the query into an LLM, which is then prompted to\\nproduce the ﬁnal prediction.\\nSteps 1 and 2 can be implemented by using an external informat ion retrieval system. For\\nexample, we can store the collection of texts in a vector data base and then retrieve the most similar\\ntexts through vector-based search techniques. Since infor mation retrieval is not the focus of this\\nchapter, we will assume that such systems are available off- the-shelf and use them directly .\\nHere we present how to prompt LLMs to make use of retrieved tex ts. T o illustrate, consider\\nan example of using LLMs to answer the following question.\\nWhere will the 2028 Olympics be held?\\nW e can simply input this question into an online search engin e. It will then return the relevant\\npieces of text found on the internet, for example,\\n(Wikipedia)\\nThe 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Olympiad and com-\\nmonly known as Los Angeles 2028 or LA28, is an upcoming intern ational multi-sport\\nevent scheduled to take place from July 14-30, 2028, in the Un ited States. ...\\n(The Sporting News)\\nIn 2028, Los Angeles will become the third city, following London and P aris respectively,\\nto host three Olympics after hosting the Summer Games in 1932 and 1984. It will also\\nbe the ﬁrst time the United States has hosted an Olympic Games since the 2002 Winter\\nGames in Salt Lake City. ...\\n...\\nW e can use these retrieved texts as additional context, and p rompt an LLM to generate a\\nresponse based on these texts. Below is an example RAG prompt .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 142, 'page_label': '136'}, page_content='136 Prompting\\nY our task is to answer the following question. T o help you wit h this, relevant\\ntexts are provided. Please base your answer on these texts.\\nQuestion:\\nWhere will the 2028 Olympics be held?\\nRelevant T ext 1:\\nThe 2028 Summer Olympics, ofﬁcially the Games of the XXXIV Olympiad and\\ncommonly known as Los Angeles 2028 or LA28 ...\\nRelevant T ext 2:\\nIn 2028, Los Angeles will become the third city , following London and Paris\\nrespectively , to host three Olympics after ...\\n...\\nThe 2028 Olympics will be held in Los Angeles.\\nThis prompt assumes that the provided texts are relevant to t he question and expects the LLM\\nto generate a faithful response using these texts. However, the information retrieval system may\\nsometimes provide irrelevant or incorrect texts, which may lead the LLM to produce an incorrect\\nanswer. One straightforward way to address this issue is to i mprove the accuracy of the informa-\\ntion retrieval system. Nevertheless, as with most AI system s, errors may still occur. Therefore, it\\nis also necessary to enhance the robustness of the LLM, so tha t it can make reasonable predictions\\neven when the input is inaccurate. Below is a new prompt that e nables the LLM to be more faith-\\nful to the facts, and allows it to choose not to answer questio ns when the information provided is\\ninaccurate.\\nY our task is to answer the following question. T o help you wit h this, relevant\\ntexts are provided. Please base your answer on these texts.\\nPlease note that your answers need to be as accurate as possible and faithful to\\nthe facts. If the information provided is insufﬁ cient for an accurate response, you\\nmay simply output \"No answer!\".\\nQuestion:\\nWhere will the 2028 Olympics be held?\\nRelevant T ext 1:\\nThe 2024 Summer Olympics, ofﬁcially the Games of the XXXIII Olympiad and\\nbranded as Paris 2024, were an international multi-sport ev ent ...\\n...\\nNo answer!\\nIn this example, the LLM refuses to answer because the provid ed information is insufﬁcient and\\nirrelevant to the question.\\nBoth RAG and ﬁne-tuning are common methods for adapting LLMs using task-speciﬁc data.\\nStandard RAG is training-free and can be directly applied to LLMs. T o further improve RAG, it'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 143, 'page_label': '137'}, page_content='3.2 Advanced Prompting Methods 137\\nis also possible to ﬁne-tune LLMs, though this will require s ome training effort. For example, we\\ncan ﬁne-tune LLMs using human-labelled data to supervise th em in learning to refuse to answer.\\nNote that, while the examples shown above seem simple, RAG is not trivial. From the prompt\\nengineering perspective, different use cases may require d ifferent prompts, though our somewhat\\n“greedy” goal is to develop a universal prompting strategy t hat can adapt to different tasks. In\\nmany cases, we need to control how much we depend on the retrie ved context to make predictions.\\nSometimes, LLMs must derive responses strictly from the pro vided texts, while at other times,\\nthey may need to generate responses using their pre-trained knowledge if the provided texts are\\ninsufﬁcient. There are many aspects of RAG, such as improvem ents to the retrieval systems, that\\ncannot be covered in this chapter. Interested readers can re fer to surveys of RAG techniques for\\nmore information [ Li et al. , 2022; Gao et al. , 2023c].\\nOne reason we discuss RAG here is that it can be broadly regard ed as an instance of the\\ngeneral problem decomposition framework (see Section 3.2.2). RAG divides problem-solving\\ninto two steps. In the ﬁrst step, we collect relevant and supp orting information for a given query\\nfrom various knowledge sources. In the second step, we use LL Ms to generate responses based\\non the collected information. If we extend the concept of pro blem decomposition further, we\\nwill ﬁnd that many tasks requiring the use of external system s or tools can be treated as similar\\nproblems. One such example is tool use in LLMs. In many applic ations, LLMs need to employ\\nexternal databases, APIs, and even simulation tools to gene rate accurate responses. For example,\\nLLMs can access real-time data from ﬁnancial markets to prov ide up-to-date investment advice or\\nintegrate with healthcare databases to offer personalized medical insights. This integration extends\\nthe capabilities of LLMs by allowing them to interact with, a nd in some contexts, inﬂuence or\\ncontrol external systems. Consequently , LLMs function mor e as autonomous agents rather than\\nmere text generators [ Franklin and Graesser , 1996].\\nThe issue of tool use is broad and vast. Here we narrow our disc ussion to tasks that can be fa-\\ncilitated by calling external APIs to solve some of the sub-p roblems [ Parisi et al. , 2022; Gao et al. ,\\n2023b]. Consider again the example of asking an LLM to answer “Wher e will the 2028 Olympics\\nbe held?”. Suppose the LLM can access a web search tool. W e can then prompt the LLM to\\nanswer the question with web search, like this\\nY our task is to answer the following question. Y ou may use ext ernal tools, such\\nas web search, to assist you.\\nQuestion:\\nWhere will the 2028 Olympics be held?\\nThe information regarding this question is given as follows:\\n{tool: web-search, query: \"2028 Olympics\"}\\nSo the answer is: Los Angeles\\nHere {tool: web-search, query: \"2028 Olympics\"} indicates a request to the web search\\nsystem using the query “2028 Olympics”. When the LLM sees thi s string, it executes a web search\\nand uses the result to replace the string. Then, in subsequen t steps of prediction, the LLM uses\\nthis web search result as context to produce the correct answ er.\\nConsider another example where we ask the LLM to solve a mathe matical problem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 144, 'page_label': '138'}, page_content='138 Prompting\\nProblem:\\nA swimming pool needs to be ﬁlled with water. The pool measures 10 meters\\nin length, 4 meters in width, and 2 meters in depth. Calculate the volume of the\\npool in cubic meters and then determine how many liters of wat er are needed to\\nﬁll it (considering 1 cubic meter equals 1000 liters).\\nSolution:\\nT o solve this problem, the LLM needs to ﬁrst calculate the volume of the\\npool by using the formula for the volume of a rectangular pris m: Length ×\\nWidth × Depth.Therefore, The volume is 10 m × 4 m × 2 m = {tool:\\ncalculator, expression: 10 * 4 * 2} m3 . Next, to ﬁnd out how\\nmany liters of water are needed, the LLM multiplies the volum e in cubic\\nmeters by 1000 (since 1 cubic meter equals 1000 liters). Thus , 80 ×1000\\n= {tool: calculator, expression: 80*1000} liters.\\nHere the string {tool: calculator, expression: 10 * 4 * 2} triggers the invocation\\nof a mathematical interpreter to calculate the result of the expression. Note that the result (i.e.,\\n80) will replace {tool: calculator, expression: 10 * 4 * 2} and can be referred to\\nin the following token predictions. For example, in the last step of problem-solving, 80 is used\\ninstead of {tool: calculator, expression: 10 * 4 * 2} .\\nA key difference between the tool use examples here and the pr eviously discussed RAG ex-\\namples is that in tool use, external functions can be called d uring inference. In contrast, in RAG,\\nthe retrieved texts are provided before the prediction proc ess begins. However, from the language\\nmodeling perspective, they are actually doing the same thin g: before generating the ﬁnal result,\\nwe use external tools, either manually or automatically , to obtain sufﬁcient and relevant context. A\\nhigh-level interpretation of these approaches is that they both rely on an “agent” that can determine\\nwhere and how to call external functions to generate the cont ext necessary for prediction.\\nAn issue with tool use is that the original LLMs are not traine d to generate the necessary\\nmarkers for tool use. Therefore, we need to ﬁne-tune the LLMs to adapt them for these tasks\\n[Schick et al. , 2024]. As this chapter focuses on prompting, we will not present t he details of this\\nﬁne-tuning process. T o put it simply , we ﬁrst need to annotat e data. For each ﬁne-tuning example,\\nwe replace parts of the output that require the use of externa l tools with predeﬁned commands\\nor markers. Then, we use this labeled data to ﬁne-tune the par ameters of the LLM as usual. As\\na result, the LLM can gain the ability to generate commands fo r calling external tools. During\\ninference, we can execute these tool use commands in the mode l outputs to get assistance from\\nexternal tools.\\n3.3 Learning to Prompt\\nSo far in this chapter, we have considered several basic prompting strategies and various reﬁne-\\nments to them. However, all the prompts we have discussed wer e designed manually . This leads\\nto a number of problems: First, designing high-quality prom pts is inherently difﬁcult and requires\\nsubstantial manual effort. For example, extensive experim entation with different prompts is often\\nneeded to identify the most effective ones. Since different LLMs may respond better to certain'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 145, 'page_label': '139'}, page_content='3.3 Learning to Prompt 139\\ntypes of prompts, developing universally effective prompt s can be even more resource-intensive.\\nSecond, manual prompt design relies heavily on human expert ise, which can limit the diversity\\nof approaches and overlook potentially effective prompts t hat are not immediately obvious to hu-\\nmans. Third, prompts created by humans can be complex and red undant, leading to longer inputs\\nfor LLMs and higher computational costs.\\nIn this section, we discuss techniques for automated prompt ing. These methods aim to auto-\\nmatically create, optimize, and represent prompts so that t he downstream tasks can be addressed\\nmore effectively and efﬁciently . In particular, we conside r three issues here.\\n• How can we automate the process of designing and optimizing prompts for LLMs?\\n• Are there other forms of representing prompts beyond strin gs, and how can we learn such\\nrepresentations?\\n• How can we make prompts more concise and compact, thereby re ducing their complexity\\nand length?\\nNote that there are many settings in which we can investigate these issues. For example, we\\nmight specify that prompts are developed speciﬁcally for a p articular LLM, or that the develop-\\nment is independent of the LLM used. These settings can lead t o different methods and application\\nscenarios, but these methods may overlap in some ways. In the following discussion, we will cover\\nseveral different scenarios and discuss the connections be tween various methods.\\n3.3.1 Prompt Optimization\\nGiven that prompt design is difﬁcult and labor-intensive, it is desirable to use machine learning\\nmodels to discover the optimal prompt for a speciﬁc task (cal l it automatic prompt design or\\nprompt optimization ). This approach can broadly be regarded as an instance of automated ma-\\nchine learning (AutoML), which aims to reduce or eliminate the need for expert-driv en manual\\ndesign of machine learning models. Although our focus here i s on the design of prompts, prompts\\nthemselves are discrete structures. Therefore, designing prompts is very similar to designing ma-\\nchine learning models, such as discrete model architecture s. Perhaps one of the most related ﬁelds\\nis neural architecture search (NAS), where the most optimal neural networks are identiﬁed by\\nexploring a space of possible neural networks [\\nZoph and Le , 2016; Elsken et al. , 2019]. If we con-\\nsider prompt optimization as a search process, then we can de scribe a general prompt optimization\\nframework involving the following components:\\n• Prompt Search Space . This deﬁnes all possible prompts that the algorithms can ex plore.\\nFor example, one can edit some seed prompts to generate a set o f diverse candidate prompts.\\n• Performance Estimation . Once an prompt is chosen, it needs to be evaluated. For examp le,\\na straightforward way is to input it to an LLM and measure its p erformance on a validation\\nset.\\n• Search Strategy . The search process is generally the same as that used in many AI sys-\\ntems. At each step, the system explores a set of promising pro mpts in the search space and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 146, 'page_label': '140'}, page_content='140 Prompting\\nevaluates them. This process continues as more prompts are e xplored. The outcome of the\\nsearch is the best-performing prompt observed until the sea rch stops.\\nThis is a very general framework, and different prompt optim ization systems can vary in their\\ndesign of each component. A widely-used approach is to use LL Ms as the basis to develop these\\ncomponents. Initially , a few prompts are provided. Then, th e following process is iterated until\\na stopping criterion is met: 1) the prompts are evaluated on a validation set; 2) a candidate pool\\nis maintained by keeping only the most promising prompts; an d 3) new prompts are created by\\nemploying LLMs to infer similar prompts from this candidate pool. One beneﬁt of this approach is\\nthat it allows us to use off-the-shelf LLMs to perform the tas ks mentioned above without the need\\nfor substantial system development. T o achieve this, we can prompt or ﬁne-tune LLMs to adapt\\nthem to these tasks. Here we consider Zhou et al. [2023c]’s method for illustrating LLM-based\\nprompt optimization. It involves the following steps.\\n• Initialization. Let Crepresent the pool of the candidate prompts we intend to expl ore. The\\nﬁrst step is to add initial prompts into C. W e can do this in several ways. A simple method is\\nto create such prompts by hand for a given task. However, in ma ny cases where humans have\\nlimited knowledge about how to write effective prompts for t he task, developing prompts\\nbecomes challenging. In these cases, it is desirable to use L LMs to generate prompts.\\nFor example, we can directly instruct LLMs to produce prompt s, providing them with a\\ndescription of the task.\\nY ou are given a task to complete using LLMs. Please write a pro mpt to\\nguide the LLMs.\\n{∗task-descripion∗}\\nThis method is straightforward, but it still requires a huma n-provided description of the task.\\nAn alternative method is to use LLMs to generate prompts give n examples of the input and\\noutput of the task. Here is a prompt template.\\nY ou are provided with several input-output pairs for a task. Please write\\nan instruction for performing this task.\\nInput:{∗input1∗} Output: {∗output1∗}\\nInput: {∗input2∗} Output: {∗output2∗}\\n...\\nAs such, LLMs can infer the corresponding instruction for th e task from the provided inputs\\nand outputs.\\n• Evaluation. Once we obtain the candidate pool C, we need to evaluate the prompts in C.\\nOne method is to feed each prompt into an LLM and assess the res ults on the downstream'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 147, 'page_label': '141'}, page_content='3.3 Learning to Prompt 141\\ntask. For example, we can evaluate the output of the LLM given an input using a pre-deﬁned\\nmetric, or alternatively , use the log-likelihood of the out put as a measure of the quality of\\nthe prompt.\\n• Pruning. If Ccontains a large number of prompts, it is reasonable to prune the unpromising\\nprompts within it, thus reducing the computational burden i n subsequent steps. This is a\\nstandard pruning problem. Given the evaluation score for ea ch prompt, a simple method is\\nto keep only a certain percentage of the prompts and discard t he rest.\\n• Expansion. Expansion is a key operation in search algorithms used to ex plore different\\nstates in the search space. The expansion operation here can be deﬁned as a function\\nC′ = Expand( C,f) (3.10)\\nwhere C′ is the set of new prompts generated from C using the model f. If we consider f\\nas an LLM, we can perform the expansion operation by instruct ing f to generate new and\\nrelevant prompts based on C. Below is an example.\\nBelow is a prompt for an LLM. Please provide some new prompts t o per-\\nform the same task.\\nInput:{∗prompt∗}\\nThen, we replace C with C′. The steps of evaluation, pruning and expansion can be re-\\npeated, and so we can gradually explore a wider range of promp ts.\\nIn prompt optimization, the expansion step plays a key role, as it deﬁnes how we explore\\nthe search space, and our goal is to ﬁnd optimal results with m inimal effort. One improvement\\nto this step is to treat the problem as a paraphrasing task. A s imple method is to apply off-the-\\nshelf paraphrasing systems, either based on LLMs or other mo dels, to transform input prompts\\ninto semantically equivalent forms [ Jiang et al. , 2020]. Alternatively , we can deﬁne speciﬁc edit\\noperations, such as insertions and modiﬁcations, for each t oken. A given prompt can be edited\\ninto new prompts by applying these operations [ Prasad et al. , 2023]. Also, further evaluation and\\npruning can be applied to ﬁlter out low-quality prompts. In a ddition to framing prompt generation\\nas a paraphrasing problem, we can improve the quality of prom pts during expansion by learning\\nfrom feedback [ Pryzant et al. , 2023]. This approach is somewhat related to the self-reﬁnement\\nissue discussed in Section 3.2.3. An LLM can be used to generate feedback on an input prompt,\\nwhich is then revised based on this feedback. This feedback- and-revision cycle can be repeated\\nmultiple times until the result converges or the desired out come is achieved.\\nAnother approach to prompt optimization is to apply classic optimization techniques. For\\nexample, the problem can be framed as an evolutionary comput ation problem, where prompts\\nare treated as candidates that evolve generation by generat ion as the optimization progresses\\n[Guo et al. , 2024]. Since many powerful optimization algorithms have been de veloped in related\\nﬁelds, they can be directly applied to this problem.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 148, 'page_label': '142'}, page_content='142 Prompting\\nIn practice, we might be tempted to use existing LLM APIs to im plement the steps described\\nabove. Such an approach, however, would be strongly depende nt on the inference and in-context\\nlearning abilities of the LLMs. If these LLMs are not strong a nd lack adaptation to the tasks, they\\nmay introduce errors into search, for example, generating i ncorrect prompts during expansion. In\\nsuch cases, it is preferable to train models that are better s uited to the tasks. One approach in\\nthis research direction appeals to reinforcement learning , which has been widely used in solving\\ndiscrete decision making and optimization problems. For ex ample, Deng et al. [2022] developed\\na prompt generator by integrating an FFN-based adaptor into an LLM. The prompt generator is\\ntrained as a typical policy network, but only the parameters of the adaptor are updated while the\\nremaining parameters of the model are kept unchanged. Durin g training, the reward is obtained by\\ntesting the generated prompts using another LLM, similar to the evaluation method as discussed\\nabove. Once the training is complete, the prompt generator i s then employed to generate new\\nprompts.\\nNote that, in our discussion here, prompts are simply seen as sequences of tokens, and the out-\\nput of prompt optimization is such a sequence. However, in a s trict sense, prompts have complex\\nstructures and include different ﬁelds such as user input, i nstruction, and demonstration. While\\nour discussed approaches are mostly general, much work in pr ompt optimization has focused on\\nlearning better instructions for prompting. Speciﬁcally , the goal is to generate instructions that\\neffectively guide LLMs based on a given task. Of course, the c oncept of prompt optimization\\ncan also be extended to learning other parts of prompts. For e xample, there has been substan-\\ntial research interest in learning to select or generate dem onstrations in CoT [ Liu et al. , 2022;\\nRubin et al. , 2022; Zhang et al. , 2023b]. One of the differences between learning instructions\\nand learning demonstrations is that generating high-quali ty demonstrations using LLMs is rela-\\ntively easy and the focus of learning demonstrations is typi cally on how to sample appropriate\\ndemonstrations from a pool of candidates. In contrast, the d ifﬁculty in learning instructions is\\npartly because pre-trained LLMs are not suited to predict th e quality of instructions, and testing\\nthese instructions on downstream tasks is computationally expensive. This makes the optimization\\nmethods costly to apply , and exploring a wide variety of inst ructions poses signiﬁcant challenges.\\n3.3.2 Soft Prompts\\nAlthough developing natural language prompts, either manually or automatically , is a straight-\\nforward and widely applied approach, it presents some probl ems. One problem is that natural\\nlanguage prompts can be complex and lengthy , resulting in si gniﬁcant computational burdens\\nwhen processed via LLMs. In many applications, users may nee d to perform a task repeatedly ,\\nand inputting the same long prompt into the LLMs a large numbe r of times is clearly inefﬁcient.\\nAnother problem is that while prompts are typically represe nted as discrete token sequences (call\\nthem hard prompts ) in regular LLM input, the LLMs encode them as low-dimension al real-\\nvalued vectors. This raises the question of whether there ar e more compact and efﬁcient ways to\\nrepresent prompts.\\nIn this subsection, we introduce the concept of soft prompts , which can be viewed as hidden,\\ndistributed representations of prompts. When prompting LL Ms, we are concerned with commu-\\nnicating tasks or questions to elicit the desired responses . W e can deﬁne hard prompts as explicit,\\npredeﬁned text sequences that users input directly into LLM s to guide the responses. In contrast,\\nwe can think of soft prompts as implicit, adaptable promptin g patterns embedded within LLMs.\\nUnlike hard prompts, which are expressed in natural languag e and should be understandable for'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 149, 'page_label': '143'}, page_content='3.3 Learning to Prompt 143\\n... Translate this into Chinese . I have a cat . ...\\nTransformer\\n... hj hj+1 hj+2 hj+3 hj+4 hj+5 hj+6 hj+7 hj+8 hj+9 ...\\n... ... ... ... ... ... ... ... ... ... ... ...\\nHard Prompt (Instruction)\\nSoft Prompt\\nFig. 3.3: Illustration of hard and soft prompts. Here the hard prompt i s the instruction we input to the LLM for\\nperforming the task. The LLM encodes this instruction as usu al, and the intermediate representations corresponding to\\nthe instruction can be viewed as some sort of soft prompt.\\nhumans, soft prompts are encoded in a format that is more comprehensible to the model rather\\nthan to humans. T o illustrate, consider a simple prompt\\nTranslate the sentence into Chinese.\\nConsider it done!\\nHere, the instruction “Translate the sentence into Chinese ” can be seen as a hard prompt, denoted\\nby the token sequence c1 ...c5 . By feeding these tokens into an LLM, they are transformed in to a\\nsequence of real-valued vectors h1...h5 , each corresponding to a token. W e can roughly think of\\nh1...h5 as a soft prompt, as illustrated in Figure 3.3.\\nWhile the above example shows that soft prompts can be genera ted by transforming hard\\nprompts, there is not necessarily a direct correspondence b etween them. In fact, we do not even\\nneed to interpret soft prompts using meaningful text. They a re instead simply hidden states in\\nLLMs and can be learned as standard parameters of the models t hrough continuous optimiza-\\ntion. Such a treatment allows us to explore prompting method s beyond text. As another beneﬁt,\\nsoft prompts provide dense, low-dimensional, and learnabl e representations for encoding how\\nwe guide LLMs to generate speciﬁc outputs. The training and a pplication of these representa-\\ntions require signiﬁcantly lower computational costs than those required for processing long hard\\nprompts. This approach would be of great practical value in L LM inference applications where\\nthe same prompt is repeatedly used.\\n3.3.2.1 Adapting LLMs with Less Prompting\\nOne obvious way to adapt an LLM for a particular task is to simply ﬁne-tune the model using\\nlabeled data. This leads to a variety of LLM alignment method s, such as supervised ﬁne-tuning,\\nwhich update the model parameters by aligning the responses to given prompts with supervision\\nsignals. Fine-tuned LLMs embed task-related information i n model parameters, and thus these'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 150, 'page_label': '144'}, page_content='144 Prompting\\nmodels can response correctly when dealing with similar pro mpts with those in ﬁne-tuning.\\nIf we take this idea further, we can expect LLMs to absorb the k nowledge about prompting\\nof a task as much as possible during ﬁne-tuning. Consequentl y , the prompting information is\\npartially captured in the model parameters, and the ﬁne-tun ed LLMs can perform the task with\\nless prompting. Here we consider a simple form of prompt, whe re only an instruction (denoted by\\nc) and an user input (denoted by z) are included. A prompt can be expressed using the following\\ntuple\\nx = ( c,z) (3.11)\\nGiven a set of prompt-response pairs D = {(x,y)}, the objective of ﬁne-tuning is to minimize\\nthe total loss incurred over this set. A popular method is to m inimize the negative log-likelihood\\n(i.e., maximize the log-likelihood) with respect to the mod el parameters θ:\\nˆθ = arg max\\nθ\\n∑\\n(x, y)∈D\\nlog Pr θ (y|x)\\n= arg max\\nθ\\n∑\\n(x, y)∈D\\nlog Pr θ (y|c,z) (3.12)\\nwhere Prθ (·|·) is the probability predicted by an LLM with the parameters θ9.\\nIn general, the instruction in each ﬁne-tuning example shou ld follow the guideline of prompt\\ndesign, for example, a good instruction should be as clear as possible and provide a detailed\\ndescription of the task. However, the method described in th e above equation does not restrict the\\ninstruction to any particular form. This ﬂexibility allows us to instruct LLMs in any way that we\\nwant. Consider an example where we intend to instruct LLMs to translate an English sentence\\ninto Chinese. Of course, as mentioned earlier in this chapte r, we can prompt LLMs using the\\ninstruction\\nTranslate the following sentence from English to Chinese.\\nIf we want the instruction to be simpler, we may rephrase it in to a simpler form\\nTranslate this into Chinese.\\nEven, we can deﬁne the instruction as a single phrase\\nTranslate!\\nWith certain ﬁne-tuning effort, we can adapt LLMs to follow a ny of these instructions. From\\na efﬁcient prompting perspective, there are computational advantages in simplifying instructions\\nin prompting. For example, we can use simple instructions li ke “Translate!” to perform tasks\\nthat would typically require more complex and detailed inst ructions. This can make subsequent\\n9 In practice, we initialize θ with the parameters obtained from pre-training, and then ad just θ moderately to ensure\\nthat the results after ﬁne-tuning do not deviate too much fro m the pre-trained results.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 151, 'page_label': '145'}, page_content='3.3 Learning to Prompt 145\\nFull Context User Input+ Prt(y|c,z)\\nc z y\\nT eacher Model:\\nSimpliﬁed Context User Input+ Prs(y|c′ ,z)\\nc′ z y\\nStudent Model:\\nLoss\\nFig. 3.4: Illustration of context distillation [ Snell et al. , 2022]. The teacher model is a standard LLM, which takes both\\nthe context and the user input as model input and produces a pr ediction as model output. Then, we simplify the context\\n(e.g., simplifying the instruction in prompting) and use th e student model to make predictions based on the simpliﬁed\\ncontext and the user input. The student model is trained by mi nimizing the loss between the predictions produced by\\nthe two models.\\nprompting during inference much easier. On the other hand, ﬁne-tuning LLMs with overly simpli-\\nﬁed instructions may be harmful to the generalization of the models. Since simpliﬁed instructions\\ncan lead to a loss of information, it is more likely that the LL Ms will overﬁt the ﬁne-tuning data\\nand fail to generalize beyond those instructions. In scenar ios involving both complex and simpli-\\nﬁed instructions for ﬁne-tuning, this problem is more sever e because the labeled data available for\\nﬁne-tuning is usually limited, and accommodating a variety of instructions is costly .\\nAn alternative way to adapt LLMs for simpliﬁed instructions is through knowledge distillation.\\nAs an example, we consider the context distillation method [\\nSnell et al. , 2022]. The goal of this\\nmethod is to learn a student model that can make use of simpliﬁ ed instructions from a well-trained\\ninstruction-following teacher model. Figure 3.4 shows an illustration of this approach. Building\\nthe teacher model follows a standard ﬁne-tuning process: we ﬁrst collect a certain amount of\\ndata that includes instructions, user inputs, and correct r esponses, and then we continue to train a\\npre-trained model with this dataset. For building the stude nt model, we need to construct a new\\ndataset D′ where each sample is a tuple consisting of an instruction, a c orresponding simpliﬁed\\ninstruction, and a user input, denoted by x′ = ( c,c′ ,z). Knowledge distillation is performed by\\nminimizing a loss function deﬁned on the outputs of the teach er and student models\\nˆθ = arg min\\nθ\\n∑\\nx′ ∈D′\\nLoss(Prt(·|·), Prs\\nθ(·|·), x′) (3.13)\\nwhere Prt(·|·) denotes the pre-trained teacher model, and Prs\\nθ(·|·) denotes the student model with\\nthe parameters θ. T o keep the notation simple we will write Loss(Prt(·|·), Prs\\nθ(·|·), x) as Loss\\nfor short. A commonly-used loss is the sequence-level loss, which has the basic form:\\nLoss =\\n∑\\ny\\nPrt(y|c,z) log Pr s\\nθ(y|c′ ,z) (3.14)\\nBut this function is computationally infeasible because it requires summing over an exponen-\\ntially large number of outputs. A variant of this method is to train the student model using outputs\\ngenerated by the teacher model. For each sample, we use the te acher model to produce an output'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 152, 'page_label': '146'}, page_content='146 Prompting\\nˆy = arg max y log Pr t(y|c,z). Then we consider ˆy as the target for learning, and the loss function\\nis given by\\nLoss = log Pr s\\nθ(ˆy|c′ ,z) (3.15)\\nAlternatively , we can minimize the distances between the pr obability distributions outputted\\nby the two models [\\nAskell et al. , 2021]. For example, the loss function can be deﬁned as the KL\\ndivergence between the two output distributions\\nLoss = KL(P t || Ps\\nθ) (3.16)\\nwhere\\nPt = Pr t(·|c,z) (3.17)\\nPs\\nθ = Pr s\\nθ(·|c′ ,z) (3.18)\\nAlthough we have restricted ourselves to knowledge distill ation for instructions, the approaches\\ndiscussed here are general. By learning from the outputs of t he teacher model, the knowledge in\\nprompting can be distilled into the parameters of the studen t model. Therefore, the distilled model\\ncan be considered as encoding some sort of soft prompt. This m ethod can be applied to many\\nother problems in prompt learning, such as compressing long contexts and learning soft prompts\\nas speciﬁc components of LLMs.\\n3.3.2.2 Learning Soft Prompts for Parameter-efﬁcient Fine-tuning\\nUpdating all parameters is a common method for adapting LLMs to tasks of interest. Although\\nﬁne-tuning is considered computationally cheaper than pre -training, it is still costly to apply in\\npractice. This issue motivates the development of paramete r-efﬁcient ﬁne-tuning methods, which\\naim to minimize the number of parameters that need to be updat ed.\\nOne approach, known as preﬁx ﬁne-tuning , is to append a series of trainable vectors, or\\npreﬁxes, at the beginning of the input of each Transformer la yer [\\nLi and Liang , 2021]. These\\npreﬁxes can be thought of as soft prompts that serve as additi onal context to guide the behavior\\nof the model under speciﬁc tasks. During ﬁne-tuning, we need only to learn the preﬁxes for\\nembedding task-speciﬁc knowledge. Thus, this method is efﬁ cient because it only modiﬁes a\\nsmall part of the model rather than adjusting the entire set o f model parameters.\\nSpeciﬁcally , let the input of a layer at depth lbe denoted by Hl = hl\\n0hl\\n1...hl\\nm. The output of\\nthe layer can be expressed as\\nHl+1 = Layer( Hl ) (3.19)\\nIn preﬁx ﬁne-tuning, we extend the sequence hl\\n0hl\\n1...hl\\nmby adding a few vectors at beginning,\\nwhich we denote as pl\\n0pl\\n1...pl\\nn. Hence Hl can be written in the form\\nHl = pl\\n0 pl\\n1 ...pl\\nn\\ued19\\n\\ued18\\ued17 \\ued1a\\ntrainable\\nhl\\n0 hl\\n1 ...hl\\nm\\ued19\\n\\ued18\\ued17 \\ued1a\\nprevious layer output\\n(3.20)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 153, 'page_label': '147'}, page_content='3.3 Learning to Prompt 147\\nThe output of the layer is the last m+ 1 representations.\\nHl+1 = Layer( Hl )[−m− 1 :]\\n= hl+1\\n0 hl+1\\n1 ...hl+1\\nm (3.21)\\nwhere [−m− 1 :] denotes the slicing operation that extracts the last m+ 1 elements of a sequence.\\nGiven Hl+1, the input of the next layer can be expressed in the same form o f Eq. ( 3.20):\\nHl+1 = pl+1\\n0 pl+1\\n1 ...pl+1\\nn Hl+1\\n= pl+1\\n0 pl+1\\n1 ...pl+1\\nn hl+1\\n0 hl+1\\n1 ...hl+1\\nm (3.22)\\nHere each pi ∈ Rd can be seen as a learnable parameter. During training, pl\\n0pl\\n1...pl\\nn are trained\\nas usual, and the parameters of the original Transformer mod el are kept ﬁxed.\\nFigure\\n3.5 shows an illustration of preﬁx ﬁne-tuning for a translation task. Here, only the preﬁx\\nvectors pl\\n0 and pl\\n1 are updated by receiving the error gradients from the output (i.e., the Chinese\\ntranslation). By adjusting these vectors for the translati on task, the model adapts accordingly . This\\nmakes pl\\n0 and pl\\n1 serve as prompts which activate the LLM to perform the task wi thout needing\\nexplicit input prompts like “Translate the following sente nce from English to Chinese”. At test\\ntime, we prepend the optimized pl\\n0 and pl\\n1 to the layer, and the LLM will then translate the input\\nsentence. Note that preﬁx ﬁne-tuning introduces additiona l L× n× dparameters, where Lis the\\nnumber of layers, nis the number of preﬁxes, and dis the dimensionality of each preﬁx. However,\\nthis number is much smaller compared to the total number of pa rameters in the LLM, making the\\nﬁne-tuning process highly efﬁcient.\\nWhile preﬁx ﬁne-tuning is simple, it still requires modiﬁca tions to LLMs. Alternatively , sep-\\narating soft prompts from the LLMs allows us to preserve the o riginal model architecture, making\\nit more efﬁcient for deployment across different tasks with out the need to adjust the core model.\\nOne such method is prompt tuning [\\nLester et al. , 2021]. Like preﬁx ﬁne-tuning, prompt tuning\\nincorporates trainable vectors so that LLMs can adapt to giv en tasks by adjusting these vectors.\\nHowever, prompt tuning differs in that it modiﬁes only the em bedding layer.\\nRecall that in LLMs each input token zi is represented by an embedding ei . These embeddings\\nare generally learned through a token embedding model and ar e then used as the real inputs to the\\nLLMs, replacing the symbolically represented tokens. In pr ompt tuning, a number of pseudo\\nembeddings p0 ...pn are added at the beginning of the token embedding sequence. S o the actual\\ninput to the LLMs can be expressed as\\np0 p1 ...pn\\ued19 \\ued18\\ued17 \\ued1a\\ntrainable\\ne0 e1 ...em\\ued19 \\ued18\\ued17 \\ued1a\\ntoken embeddings\\nNote that a pseudo embedding needs not to correspond to any to ken in natural language. Instead\\nthese embeddings can be seen as “soft prompt embeddings” tha t serve to condition the LLMs.\\nBy training soft prompt embeddings on task-speciﬁc data, th ey learn to interact adaptively with\\nthe token embeddings e0 ...em and guide the behavior of LLMs. Since prompt tuning does not\\nchange the underlying parameters of pre-trained LLMs, it is considered a lightweight and efﬁcient\\nmethod of ﬁne-tuning, improving task-speciﬁc performance while maintaining their generalization\\ncapabilities. See Figure 3.6 for an illustration of prompt tuning.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 154, 'page_label': '148'}, page_content='148 Prompting\\npl\\n0 pl\\n1\\nhl\\n0 hl\\n1 hl\\n2 hl\\n3 hl\\n4\\nLayer l\\npl−1\\n0 pl−1\\n1 hl−1\\n0 hl−1\\n1 hl−1\\n3 hl−1\\n4 hl−1\\n5\\nLayer l − 1\\npl+1\\n0 pl+1\\n1 hl+1\\n0 hl+1\\n1 hl+1\\n3 hl+1\\n4 hl+1\\n5\\nLayer l + 1\\n· · · · · · · · · · · · · · ·\\nLoss Loss\\n· · · · · · · · · · · · · · ·\\nLook out ! 小心 !\\ntrainable preﬁxes\\nUser Input LLM PredictionSoft Prompt\\nFig. 3.5: Illustration of preﬁx ﬁne-tuning for a translation task (Lo ok out! → 小心!). For each layer, we add two\\npreﬁxes pl\\n0 and pl\\n1 at the beginning. The LLM is trained to minimize the loss on th e predictions given the input.\\nDuring this process, only the preﬁxes are optimized while th e rest of the parameters remain ﬁxed. Therefore, the model\\ncan adapt to the given task in a very efﬁcient manner. At infer ence time, the LLM works with optimized preﬁxes, and\\ncan perform the task without the need of explicit hard prompt s.\\nSince p0 p1 ...pn is itself a sequence, we can employ sequence models to better represent\\nit. For example, a Transformer model can encode this sequenc e, and the resulting representation\\ncan then be used as the input to the LLM. In other words, we can d evelop an additional model\\nfor encoding soft prompts. Another way to improve prompting is by combining soft and hard\\nprompts, thereby taking advantage of both types [\\nLiu et al. , 2023b]. In the embedding sequence,\\nwe can arrange or intersperse these prompts. This would resu lt in different prompt patterns. For\\nexample, a simple pattern that uses both two types of prompt i s\\np0 p1 · · · pn q0 q1 · · · qm′ e0 e1 · · · em\\nc0 c1 · · · cm′ z0 z1 · · · zm\\nSoft Prompt Hard Prompt User Input and Response\\nwhere c0 ...cm′ denotes the hard prompt and q0 ...qm′ denotes the corresponding embedding se-\\nquence.\\nHere we have considered methods for inserting soft prompts i n LLMs. But we skip the details\\nof training these soft prompts and assume that the reader is f amiliar with the standard supervised\\nlearning process, that is, maximizing the likelihood of the correct model output given the model'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 155, 'page_label': '149'}, page_content='3.3 Learning to Prompt 149\\nLayer l− 1\\nLayer l\\nLayer l+ 1\\n· · · · · · · · · · · · · · · · · · · · ·\\np0 p1 e0 e1 e2 e3 e4\\n· · · · · · · · · · · · · · · · · · · · ·\\nLoss Loss\\nLook out ! 小心 !\\ntrainable prompt\\nembeddings\\nUser Input LLM PredictionSoft Prompt\\nFig. 3.6: Illustration of prompt tuning for a translation task (Look o ut! → 小心!). Instead of using ﬁxed textual\\nprompts, soft prompts are learnable embeddings that are add ed at the beginning of the embedding sequence. During\\nﬁne-tuning, only these prompt embeddings are optimized to e fﬁciently adapt the LLM to the given task. Once opti-\\nmized, the prompt embeddings are used to instruct the LLM to p erform the task as new data arrives.\\ninput. In fact, learning soft prompts can be related to many i ssues in LLM ﬁne-tuning. For exam-\\nple, if we consider it as a context compression problem, we ca n apply the knowledge distillation\\nmethods described previously . In Mu et al. [2024]’s work, prompts are compressed and repre-\\nsented as a few pseudo tokens, which are appended to each inpu t sequence. The embeddings of\\nthese pseudo tokens are optimized to mimic the predictions o f a standard-prompted model. In\\nother words, the prompting knowledge is distilled from a tea cher model into the pseudo tokens.\\nBroadly speaking, many parameter-efﬁcient ﬁne-tuning met hods can be thought of as learning\\nsome sort of soft prompt [ Lialin et al. , 2023]. When we ﬁne-tune a part of an LLM for a task, this\\nprocess can essentially be seen as injecting task-related p rompting information into that speciﬁc\\npart of the model. Another widely-used approach to paramete r-efﬁcient ﬁne-tuning is to add an\\nadaptor layer between the existing model layers. This appro ach allows us to ﬁne-tune only the\\nadaptor layer on speciﬁc tasks without altering the underly ing architecture or retraining the entire\\nmodel. In this sense, adaptor layers can be viewed as soft pro mpts that encode prompting and task-\\nrelated information and interact with the original LLM to he lp it adapt. T o summarize, Figure 3.7\\nshows a comparison of different methods of using soft prompt s in LLMs.\\n3.3.2.3 Learning Soft Prompts with Compression\\nAnother approach to learning soft prompts is from the perspective of compression. As a simple\\nexample, consider the problem of approximating a long conte xt using a continuous representation\\n[\\nWingate et al. , 2022]. Suppose we have a user input z and its context c (such as long instructions\\nand demonstrations). Now we want to develop a compressed rep resentation of the context, denoted'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 156, 'page_label': '150'}, page_content='150 Prompting\\nLLM\\n(a) Soft Prompts as Preﬁxes\\nLLM\\n(b) Soft Prompts as Inputs (Embeddings)\\nLLM\\nLayer\\n(c) Fine-tuning Parts of the Model\\nLLM\\nAdaptor\\n(d) Fine-tuning the Adaptor\\nFig. 3.7: Illustrations of using soft prompts in LLMs. Here tunable so ft prompts are shown in blue, and components\\nwhose parameters are ﬁxed during ﬁne-tuning are shown in gra y . In sub-ﬁgure (a), soft prompts are preﬁxes appended\\nto each layer of the LLM. In sub-ﬁgure (b), soft prompts are us ed as input embeddings for the LLM. In sub-ﬁgures (c)\\nand (d), soft prompts are broadly treated as components of th e model that are ﬁne-tuned for task adaptation.\\nby σ, such that the prediction based on z and σis as close as possible to the prediction based on z\\nand c. This goal can be expressed in the form\\nˆσ = arg min\\nσ\\ns(ˆy,ˆyσ ) (3.23)\\nwhere ˆy = arg max y Pr(y|c,z) and ˆyσ = arg max yσ Pr(y|σ,z) are the LLM predictions given\\nthe full context and the compressed context, respectively . The function s(·,·) typically represents\\na loss or similarity measure, aiming to minimize the differe nce in predictions between the two\\ncontext representations.\\nOne general framework for achieving this is knowledge disti llation, where ˆy and ˆyσ can be\\nseen as the predictions of the teacher model and the student m odel, respectively . This formal-\\nization links our discussion to the context distillation pr oblem discussed earlier. The training\\nobjective can be obtained by analogy with Eqs. ( 3.15) and ( 3.16). For example, a simple training\\nobjective is given by\\nˆσ = arg max\\nσ\\nlog Pr(ˆy|σ,z) (3.24)\\nAlternatively , we can minimize the KL divergence between th e output distributions, giving\\nˆσ = arg min\\nσ\\nKL(Pr(·|c,z) || Pr(·|σ,z)) (3.25)\\nThe difference with the models in Eqs. ( 3.15) and ( 3.16) is that here the compressed context is\\nrepresented as real-valued vectors (call them prompt embeddings ), rather than as normal tokens.\\nBy applying the above methods, we distill the context from th e token sequence c into the embed-\\ndings σ. Note that the teacher model Pr(·|c,z) and the student model Pr(·|σ,z) may not share\\nthe same architecture or model settings. In practice, we gen erally wish for the teacher model to be'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 157, 'page_label': '151'}, page_content='3.3 Learning to Prompt 151\\nσ <i\\n1 σ <i\\n2 ei\\n1\\nei\\n2\\nei\\n3\\nei\\n4\\n⟨ e1 ⟩ ⟨ e2 ⟩\\nzi\\n1 zi\\n2 zi\\n3 zi\\n4 ⟨ g1 ⟩ ⟨ g2 ⟩\\nTransformer Layers\\nh<i\\n1\\nh<i\\n1\\nh1 h2 h3 h4 σ <i+1\\n1 σ <i+1\\n2\\nSoft Prompts\\nat Stepi− 1\\nSoft Prompts\\nat the Current Step\\nFig. 3.8: Illustration of compressing a context segment into soft pro mpts ( κ = 2 and mi = 4 ). The input to the\\nLLM includes the soft prompts from the previous step ( σ <i\\n1 and σ <i\\n2 ), the tokens of the segment ( z1 ,z 2 ,z 3 , and z4 ),\\nand the summary tokens ( ⟨ g1 ⟩ and ⟨ g2 ⟩ ). Given these, the LLM operates as usual. W e then extract the outputs at the\\nlast Transformer layer that correspond to the summary token s. These outputs can be viewed as the soft prompts that\\naccumulated up to this segment.\\nstronger, while the student model should be smaller and moreefﬁcient.\\nWhile compressing full context into continuous representa tions is a straightforward approach\\nto learning soft prompts, it requires a teacher model that ca n deal with long input sequences. In\\nmany cases, however, the context is so long that applying an L LM is too costly or infeasible.\\nModeling long input sequences can fall under the broad famil y of efﬁcient methods for long-\\ncontext LLMs. Many techniques have been developed to addres s this issue. For example, one can\\nuse a ﬁxed-size KV cache to store the past information at each step during inference. Efﬁcient\\nTransformer architectures and long-context LLMs have been intensively discussed in this book.\\nFor more detailed discussions of these topics, interested r eaders can refer to Chapter 2.\\nThere are also methods speciﬁcally designed to compress lon g context into soft prompts. Here\\nwe consider\\nChevalier et al. [2023]’s method as an example. The basic idea is that we learn\\nsoft prompts gradually by accumulating the ﬁxed-size conte xt representation over the context\\nsequence. Given a long context, we ﬁrst divide it into a numbe r of segments z1 ,...,zK . W e\\nthen process these segments in sequence, each time generati ng a representation of the context we\\nhave processed so far, denoted by σ<i+1 . T o do this, a few summary tokens ⟨ g1 ⟩ ,...,⟨ gκ ⟩ are\\nintroduced. At each step, we take a segment zi = zi\\n1...zi\\nmi , along with the previous context rep-\\nresentation σ<i and the summary tokens ⟨ g1 ⟩ ,...,⟨ gκ ⟩ as input, and use an LLM to produce the\\ncorresponding hidden representation sequence at the last T ransformer layer. An example of this\\nprocess is illustrated in Figure 3.8.\\nHere σ<i is essentially a memory . The model operates in an RNN fashion . Each time we take\\na segment and update this memory by encoding both the previou s memory state and the segment.\\nTherefore, the σ<i produced at the last segment is a representation of the entir e context sequence.\\nThe Transformer model for learning these representations c an be a standard LLM but we need to\\nﬁne-tune it to adapt to this context representation task.\\nNote that here we simply consider prompt and context as similar terms, even though they are\\nnot the same. Although we are somewhat “misusing” the concep t prompt, we can often view it as\\na type of context. From this perspective, the methods discus sed here can be applied to general text\\ncompression problems.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 158, 'page_label': '152'}, page_content='152 Prompting\\n3.3.3 Prompt Length Reduction\\nWhile soft prompts provide dense, hidden representations,they are not directly interpretable. The\\nlack of interpretability can be a signiﬁcant barrier for use rs trying to understand how their inputs\\ninﬂuence LLM outputs. Moreover, although soft prompts are e fﬁcient for ﬁne-tuning and de-\\nployment, they are inﬂexible and do not allow for easy adjust ments without extensive ﬁne-tuning\\nor modiﬁcation. This inﬂexibility can limit their utility i n dynamic environments where prompt\\nchanges are frequently needed.\\nOne alternative way to develop efﬁcient prompts is to simpli fy the text used for prompting.\\nFor example, below is a prompt for answering questions on hea lthcare and ﬁnance.\\nThe task involves developing a language model capable of und erstanding and\\nresponding to user inquiries across various domains, with a particular emphasis\\non healthcare and ﬁnance. Considering the broad range of pot ential queries,\\nfrom the speciﬁcs of medical diagnoses to the nuances of ﬁnan cial regulations,\\nthe model must ensure a comprehensive understanding and acc urate responses.\\nQuestion:\\nWhat are the best practices for using artiﬁcial intelligence in diagnosing cardio-\\nvascular diseases?\\nW e can simplify the task description by deleting the unimpor tant parts.\\nThe task involves developing a language model capable of und erstanding\\nand responding to user inquiries across various domains, with a particular\\nemphasis on healthcare and ﬁnance. Considering the broad range of potential\\nqueries, from the speciﬁcs of medical diagnoses to the nuances of ﬁnancial\\nregulations, The model must ensure a comprehensive understanding and ac-\\ncurate responses.\\nW e can also paraphrase it as a shorter text.\\nThe task involves developing a language model focused on hea lthcare and\\nﬁnance, capable of understanding and accurately respondin g to a wide range\\nof user inquiries.\\nThis problem can be viewed as a classic NLP issue — text simpli ﬁcation. So the methods\\nused can be general and not restricted to the problem of simpl ifying prompts. There are many\\nways to achieve this. One simple method is to deﬁne some heuri stics and identify redundant\\nwords that can be eliminated without losing essential infor mation. For example, we can examine\\neach token in a sequence in terms of its contribution to the ov erall meaning and remove those that\\nprovide minimal value [ Li et al. , 2023c; Jiang et al. , 2023b]. Another method involves framing\\nthe problem as a sequence-to-sequence task. With labeled da ta for text simpliﬁcation, we can\\ntrain an encoder-decoder model to transform each input text into its simpliﬁed form. In addition,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 159, 'page_label': '153'}, page_content='3.4 Summary 153\\ngiven that many LLMs have been ﬁne-tuned and aligned to perfo rm text simpliﬁcation tasks, it is\\nstraightforward to use these models to simplify prompts. Fo r example, we can prompt an LLM to\\nsimplify a text under certain constraints, such as limiting the length of the simpliﬁed text.\\n3.4 Summary\\nIn this chapter, we have discussed a variety of issues related to LLM prompting. Our discussion\\nhas focused mainly on two aspects:\\n• How to design basic prompts to guide the predictions of LLMs and reﬁne these prompts for\\nmore effective and efﬁcient problem-solving?\\n• How to automate the design and representation of prompts?\\nSolutions to these issues involve both general prompt desig ns and more advanced techniques, such\\nas CoT and prompt learning, which have been explored extensi vely in recent research.\\nIn NLP , prompting can be viewed as a technology that has evolv ed along with LLMs, and\\nin a sense, it has opened the door to the practical applicatio n of these models in an impressive\\nrange of problem domains. In fact, if we expand the concept of prompts to some extent, it can be\\ntraced back to the early days of machine learning and NLP . For example, many NLP systems use\\nhand-crafted features and templates to “prompt” speciﬁc ta sks. Imagine developing a feature to\\nindicate whether a text is formal or informal. W e can feed thi s feature into a machine translation\\nsystem to condition the translation on the type of the input t ext.\\nThe widespread use of the modern concept of prompts began wit h the rise of large pre-trained\\nmodels in the ﬁeld of NLP . Initially , these models, such as BE RT , were adapted to speciﬁc down-\\nstream tasks mainly through ﬁne-tuning. However, research ers soon discovered that by designing\\nspeciﬁc \"prompts\" — adding certain words or sentences to the input — the models could be\\ntriggered to respond to speciﬁc tasks without extensive ﬁne -tuning. This motivated the NLP com-\\nmunity to develop and apply universal foundation models tha t can be prompted to address various\\ntasks without changing the underlying architecture and the pre-training procedure.\\nPrompting approaches were ﬁrst experimented with smaller m odels and later demonstrated\\nimpressive capabilities with large models like GPT -3, whic h could generate high-quality text in\\nresponse to simple prompts across various tasks. As prompti ng technology evolved, prompt en-\\ngineering emerged as a critical area of research. As discuss ed in this chapter, it broadly involves\\ndesigning effective prompts to maximize model performance , encompassing both hand-crafted\\nand automatically generated prompts. More recent research has explored how to enhance the\\neffectiveness of prompting through techniques like few-sh ot learning, zero-shot learning, and\\nCoT reasoning, enabling LLMs to work effectively across a wi de range of scenarios. A gen-\\neral discussion of prompting can be very broad, and we cannot cover all details in this chapter.\\nFor more advanced techniques of prompting, the reader can re fer to recent surveys. T opics in-\\nclude in-context learning [\\nLi, 2023; Dong et al. , 2022], CoT [ Chu et al. , 2023; Y u et al. , 2023;\\nZhang et al. , 2023a], efﬁcient prompting [ Chang et al. , 2024], and general prompt engineering\\n[Liu et al. , 2023c; Chen et al. , 2023a].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 160, 'page_label': '154'}, page_content='154 Prompting\\nNote that although we would ideally like to develop general p rompting methods without ad-\\njusting model architectures and parameters, the results of prompting generally depend heavily\\non the quality and size of the given LLMs. For stronger models , such as commercialized online\\nLLMs, simple prompts may be sufﬁcient to instruct these mode ls to perform tasks correctly . In\\nthis case, prompt engineering is relatively easy , though we still need certain efforts to make LLMs\\nwork properly . By contrast, if the LLMs are not powerful enou gh, we may need to carefully design\\nthe prompts to achieve the desired results. In many cases, ﬁn e-tuning is still necessary to adapt\\nthe models to sophisticated prompting strategies.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 161, 'page_label': '155'}, page_content='CH A P TER 4\\nAlignment\\nAlignment is not a new concept in NLP , but its meaning varies a cross different domains and over\\ntime. In traditional NLP , the term alignment typically refers to the tasks that link corresponding\\nelements in two sets, such as aligning words between a Chines e sentence and an English sentence.\\nAs LLMs become increasingly important in NLP research, this term is more broadly used to refer\\nto aligning model outputs with human expectations. The prob lem that alignment addresses is\\nthat the output of a model may not align with the speciﬁc goals or contexts intended by users.\\nFor example, pre-trained LLMs may not be able to follow user i nstructions because they were\\nnot trained to do so. Another example is that LLMs may generat e harmful content or perpetuate\\nbiases inherent in their training data. This poses new chall enges in ensuring that LLM outputs are\\nnot only accurate and relevant, but also ethically sound and non-discriminatory .\\nSimply pre-training LLMs can result in a variety of alignmen t problems. Our ultimate goal\\nis to resolve or mitigate all these problems to ensure LLMs ar e both accurate and safe. There\\nis an interesting issue here: since large language models ar e trained on vast amounts of data,\\nwe have reason to believe that if we have sufﬁcient data cover ing a variety of tasks and aligned\\nwith human preferences, pre-training could make LLMs accur ate and safe enough, perhaps even\\neliminating the need for alignment. However, the reality is that it is nearly impossible to gather\\ndata that encompasses all tasks or adequately represents hu man preferences. This makes it difﬁcult\\nto achieve model alignment through pre-training alone, or a t least, at this stage, alignment remains\\na very necessary and critical step in the development of LLMs .\\nIn this chapter, we will focus on alignment methods for LLMs. W e will begin by discussing the\\ngeneral alignment tasks. Then we will consider two widely-u sed approaches, known as instruc-\\ntion alignment and human preference alignment , respectively . The former resorts to supervised\\nﬁne-tuning techniques and guides the LLMs to generate outpu ts that adhere closely to user instruc-\\ntions. On the other hand, the latter typically relies on rein forcement learning techniques, where\\nthe LLMs are trained based on feedback from humans. While the se methods are motivated by\\ndifferent goals, they are commonly used together to develop well-aligned LLMs.\\n4.1 An Overview of LLM Alignment\\nAlignment can be achieved in several different ways. W e needdifferent methods for LLM align-\\nment because this problem is itself complicated and multifa ceted, requiring a blend of technical\\nconsiderations. Here we consider three widely-used approa ches to aligning LLMs.\\nThe ﬁrst approach is to ﬁne-tune LLMs with labeled data. This approach is straightforward\\nas it simply extends the pre-existing training of a pre-trai ned LLM to adapt it to speciﬁc tasks.\\nAn example of this is supervised ﬁne-tuning (SFT), in which the LLM is further trained on a\\ndataset comprising task-speciﬁc instructions paired with their expected outputs. The SFT dataset\\nis generally much smaller compared to the original training set, but this data is highly specialized.\\nThe result of SFT is that the LLM can learn to execute tasks bas ed on user instructions. For\\nexample, by ﬁne-tuning the LLM with a set of question-answer pairs, the model can respond to\\nspeciﬁc questions, even if not directly covered in the SFT da taset. This method proves particularly\\n155'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 162, 'page_label': '156'}, page_content='156 Alignment\\nPre-training Prompting\\nInstruction\\nAlignment\\n(e.g., SFT)\\nHuman Preference\\nAlignment\\n(e.g., RLHF)\\nPre-training\\nStage\\nAlignment\\nStage\\nTraining & Fine-tuning Inference\\nFig. 4.1: Schematic illustration of the pre-train-then-align metho d for developing LLMs. In the pre-training stage, we\\ntrain an LLM on vast amounts of data using next token predicti on. Then, in the alignment stage, we align the LLM\\nto user instructions, intents, and preferences. This inclu des instruction alignment, human preference alignment, an d\\nprompting.\\nuseful when it is relatively easy to describe the input-output relationships and straightforward to\\nannotate the data.\\nThe second approach is to ﬁne-tune LLMs using reward models. One difﬁculty in alignment\\nis that human values and expectations are complex and hard to describe. In many cases, even for\\nhumans themselves, articulating what is ethically correct or culturally appropriate can be challeng-\\ning. As a result, collecting or annotating ﬁne-tuning data i s not as straightforward as it is with SFT .\\nMoreover, aligning LLMs is not just a task of ﬁtting data, or i n other words, the limited samples\\nannotated by humans are often insufﬁcient to comprehensive ly describe these behaviors. What we\\nreally need here is to teach the model how to determine which o utputs are more in line with human\\npreferences, for example, we not only want the outputs to be t echnically accurate but also to align\\nwith human expectations and values. One idea is to develop a r eward model analogous to a human\\nexpert. This reward model would work by rewarding the LLM whe never it generates responses\\nthat align more closely with human preferences, much like ho w a teacher provides feedback to a\\nstudent. T o obtain such a reward model, we can train a scoring function from human preference\\ndata. The trained reward model is then used as a guide to adjus t and reﬁne the LLM. This frames\\nthe LLM alignment task as a reinforcement learning task. The resulting methods, such as rein-\\nforcement learning from human feedback (RLHF), have been demonstrated to be particularly\\nsuccessful in adapting LLMs to follow the subtleties of huma n behavior and social norms.\\nThe third approach is to perform alignment during inference rather than during training or\\nﬁne-tuning. From this perspective, prompting in LLMs can al so be seen as a form of alignment,\\nbut it does not involve training or ﬁne-funing. So we can dyna mically adapt an LLM to various\\ntasks at minimal cost. Another method to do alignment at infe rence time is to rescore the outputs\\nof an LLM. For example, we could develop a scoring system to si mulate human feedback on the\\noutputs of the LLM (like a reward model) and prioritize those that receive more positive feedback.\\nThe three methods mentioned above are typically used in sequ ence once the pre-training is\\ncomplete: we ﬁrst perform SFT , then RLHF , and then prompt the LLM in some way during\\ninference. This roughly divides the development of LLMs int o two stages — the pre-training stage\\nand the alignment stage. Figure\\n4.1 shows an illustration of this. Since prompting techniques h ave\\nbeen intensively discussed in the previous chapter, we will focus on ﬁne-tuning-based alignment\\nmethods in the rest of this chapter.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 163, 'page_label': '157'}, page_content='4.2 Instruction Alignment 157\\n4.2 Instruction Alignment\\nOne feature of LLMs is that they can follow the prompts provided by users to perform various\\ntasks. In many applications, a prompt consists of a simple in struction and user input, and we want\\nthe LLM to follow this instruction to perform the task correc tly . This ability of LLMs is also\\ncalled the instruction-following ability . For example, be low is a prompt where we want the LLM\\nto extract key points and provide a concise summary for a leng thy article.\\nInstruction Summarize this text in three sentences.\\nInput Daylight Savings Time (DST) - the process of moving clocks fo rward\\nby one hour in the summer - was started in Germany in 1916 ...\\nOutput\\nThis task requires the LLM to understand the instruction “Su mmarize this text in three sentences”\\nand perform the summarization accordingly . However, LLMs a re typically trained for next-token\\nprediction rather than for generating outputs that follow i nstructions. Applying a pre-trained LLM\\nto the above example would likely result in the model continu ing to write the input article instead\\nof summarizing the main points. The goal of instruction alig nment (or instruction ﬁne-tuning ) is\\nto tune the LLM to accurately respond to user instructions an d intentions. The rest of this section\\nwill discuss some issues related to instruction alignment, including ﬁne-tuning LLMs to follow\\ninstructions, generating or collecting instruction data, and generalizing instruction alignment.\\n4.2.1 Supervised Fine-tuning\\nOne straightforward approach to adapting LLMs to follow instructions is to ﬁne-tune these models\\nusing annotated input-output pairs [\\nOuyang et al. , 2022; W ei et al. , 2022a]. Unlike standard lan-\\nguage model training, here we do not wish to maximize the prob ability of generating a complete\\nsequence, but rather maximizing the probability of generat ing the rest of the sequence given its\\npreﬁx. This approach makes instruction ﬁne-tuning a bit dif ferent from pre-training. The SFT\\ndata is a collection of such input-output pairs (denoted by S), where each output is the correct\\nresponse for the corresponding input instruction. For exam ple, below is an SFT dataset\\nx (instruction + user input) y (output)\\nSummarize the following article. {∗summary∗}\\nArticle: In recent years, solar energy has seen\\nunprecedented growth, becoming the fastest-growing ...\\nExtract the main ﬁnancial ﬁgures from the following Revenue: $10 million,\\nearnings report. Proﬁt Margin: 15%\\nReport: The company reported a revenue of $10 million\\nin the ﬁrst quarter with a proﬁt margin of 15% ...\\nClassify the following email as spam or not spam. Spam\\nT ext: Congratulations! Y ou’ve won a $500 gift card.\\nClick here to claim now .\\nProvide a solution to the following technical issue. First, check for ...\\nIssue: my computer is running slow and often freezes.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 164, 'page_label': '158'}, page_content='158 Alignment\\nwhere the instructions are highlighted. This dataset conta ins instructions and the corresponding\\noutputs for several different NLP problems, and so we can ﬁne -tune an LLM to handle multiple\\ntasks simultaneously .\\nLet x = x0...xm be an input sequence (e.g., instruction + user input) and y = y1...yn be the\\ncorresponding output sequence. In SFT , we aim to maximize th e probability of the output y given\\nthe input x. Consider an LLM with pre-trained parameters ˆθ. The ﬁne-tuning objective can then\\nbe formulated as:\\n˜θ = arg max\\nˆθ +\\n∑\\n(x, y)∈D\\nlog Pr ˆθ + (y|x) (4.1)\\nwhere ˜θdenotes the parameters optimized via ﬁne-tuning, and ˆθ+ represents an adjustment to ˆθ.\\nHere we will omit the superscript + and use θto represent ˆθ+ to keep the notation uncluttered. But\\nthe reader should keep in mind that the ﬁne-tuning starts fro m the pre-trained parameters rather\\nthan randomly initialized parameters.\\nThe objective function log Pr θ (yi|x,y<i) is computed by summing the log-probabilities of\\nthe tokens in y, conditional on the input x and all the previous tokens y<i:\\nlog Pr θ (y|x) =\\nn∑\\ni=1\\nlog Pr θ (yi|x,y<i) (4.2)\\nThis formulation is equivalent to minimizing the cross-ent ropy loss.\\nNote that minimizing the conditional log-probability log Pr θ (y|x) is not a standard language\\nmodel training problem. If we concatenate x and y as a single sequence, a more general form of\\nlanguage modeling is based on the joint log-probability log Pr θ (x,y), that is, we minimize the\\nloss over all tokens of the sequence seqx, y = [ x,y]. W e can write the probability of this sequence\\nusing the chain rule\\nlog Pr θ (seqx, y) = log Pr θ (x,y)\\n= log Pr θ (x)\\ued19 \\ued18\\ued17 \\ued1a\\nset to 0\\n+ log Pr θ (y|x)\\ued19 \\ued18\\ued17 \\ued1a\\nloss computation\\n(4.3)\\nThere are two terms on the right-hand side of the equation. W e can simply set the ﬁrst term\\nlog Pr θ (x) to 0, focusing solely on the second term log Pr θ (y|x) for loss computation. As a\\nresult, the training can be implemented using standard LLMs . For the sequence seqx, y, we ﬁrst\\nrun the forward pass as usual. Then, during the backward pass , we force the loss corresponding\\nto x to be zero. Figure 4.2 shows an illustration of this process.\\nBy taking log Pr θ (seqx, y) as the objective function, we can describe SFT using a regula r form\\nof language model training:\\n˜θ = arg max\\nθ\\n∑\\n(x, y)∈D\\nlog Pr θ (seqx, y) (4.4)\\nThe problem we considered above is fundamentally a single-round prediction problem, where\\nthe LLM generates a response based on a single input without a ny further interaction or feedback\\nfrom the user. The input is processed, and the output is gener ated in one go. This is typical in'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 165, 'page_label': '159'}, page_content='4.2 Instruction Alignment 159\\nx0 x1 x2 x3 y1\\nx1 x2 x3 y1 y2\\nInput Output\\n(a) Forward Pass\\nx0 x1 x2 x3 y1\\nx1 x2 x3 y1 y2\\nLoss = 0 Loss ̸= 0\\n(b) Backward Pass\\nFig. 4.2: Illustration of supervised ﬁne-tuning for LLMs. W e concate nate the input and the output into a single se-\\nquence. During the forward pass, we run the LLM as usual. Duri ng the backward pass, we compute the loss only for\\nthe output part and simply set the loss for the input part to 0.\\nscenarios where a single question is asked, and a single answer is provided, with no follow-up\\nquestions or clariﬁcations. However, in practice, we somet imes have to handle multi-round pre-\\ndiction problems, for example, an LLM engages in a dialogue o ver multiple turns. In this setting,\\nthe LLM not only generates responses based on the initial inp ut but also incorporates subsequent\\ninputs that might reﬁne or expand on earlier interactions. F or example, we can use the LLM to act\\nas a healthcare assistance chatbot and have a conversation w ith the user, like this\\nUser I’ve been feeling very tired lately .\\nChatbot I’m sorry to hear that. Besides feeling tired, have you noticed any\\nother symptoms?\\nUser Y es, I’m also experiencing headaches frequently .\\nChatbot How long have these symptoms been going on?\\nUser About a week now .\\nChatbot It might be good to check in with a healthcare professional. W ould\\nyou like help setting up an appointment?\\nUser Y es, please. Can it be after work hours?\\nChatbot Sure, I can arrange that. There are slots available next W ednesday\\nand Thursday after 5 PM. Which day works better for you?\\n...\\nIn this task, there are several rounds of conversation, each involving the generation of a re-\\nsponse based on the user’s request or question and the conver sational history . Suppose we have\\nK rounds of conversation, denoted by {x1 ,y1 ,x2,y2 ,...,xK ,yK }. Here xk and yk denote the\\nuser request and the response, respectively , for each round k. The log-probability of generating\\nthe response can be written as log Pr θ (yk |x1 ,y1 ,...,xk ). Our goal is then to maximize the sum\\nof these log-probabilities\\n˜θ = arg max\\nθ\\nK∑\\nk=1\\nlog Pr θ (yk |x1 ,y1 ,...,xk ) (4.5)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 166, 'page_label': '160'}, page_content='160 Alignment\\nA straightforward implementation of this involves calcula ting the conditional probability for\\neach k. However, it requires running the LLM K times, each time with an increased conversa-\\ntional history to make predictions. A more efﬁcient method i s to perform loss computation of all\\nresponses in a single run of the LLM. T o do this, we represent t he conversation as a sequence\\nseqx1 , y1 ,..., xK , yK = [ x1 ,y1 ,...,xK ,yK ] (or seq for short). The log-probability of this sequence\\nis given by\\nlog Pr θ (seq) = log Pr θ (x1 ,y1 ,...,xK ,yK )\\n= log Pr θ (x1 )\\ued19 \\ued18\\ued17 \\ued1a\\nset to 0\\n+ log Pr θ (y1 |x1)\\ued19 \\ued18\\ued17 \\ued1a\\nloss computation\\n+ · · · +\\nlog Pr θ (xK |x1 ,y1 ,...,yK −1 )\\ued19 \\ued18\\ued17 \\ued1a\\nset to 0\\n+\\nlog Pr θ (yK |x1,y1 ,...,xK )\\ued19 \\ued18\\ued17 \\ued1a\\nloss computation\\n(4.6)\\nThe trick here is that we ignore the loss for generating user i nputs, as illustrated in Figure 4.3.\\nHence we only compute the probabilities of generating the re sponses given their conversational\\nhistories, in other words, the value on the right-hand side o f Eq. ( 4.6) is actually equal to the value\\non the right-hand side of Eq. ( 4.5). As with Eq. ( 4.4), the training of this multi-round prediction\\nmodel can be achieved by maximizing the log likelihood over a training dataset D:\\n˜θ = arg max\\nθ\\n∑\\nseq∈D\\nlog Pr θ (seq) (4.7)\\nWhile implementing the SFT methods introduced above seems t rivial as they are fundamen-\\ntally the same as regular language model training, there are still issues that need to be considered\\nin practice. For example,\\n• SFT requires labeled data. This makes SFT quite different f rom pre-training, where raw text\\nis used as training data and is readily available. As in other supervised machine learning\\nproblems, data annotation and selection in SFT are not simpl e tasks. In general, we wish\\nto develop SFT data that is both substantial in quantity and h igh in quality , and this data\\nshould be highly relevant to the tasks the LLM will perform. O n the other hand, there is\\na need to ﬁne-tune LLMs with less data to minimize computatio nal and data construction\\ncosts. Often, the quality of LLMs is highly dependent on the d ata used in SFT . Thus, such\\ndata must be carefully developed and examined. As we will see in later subsections, SFT\\ncan be more efﬁcient and effective through more advanced tec hniques for data construction.\\n• SFT is still computationally expensive for LLMs due to thei r big size. As a result, main-\\ntaining and updating such models is resource-intensive. Fo r example, applying gradient up-\\ndates to billions of parameters within an LLM requires signi ﬁcant computational power and\\nmemory . This often requires high-performance computing en vironments, which are costly\\nto operate. T o address these challenges, various optimizat ion strategies, such as pruning,\\nquantization, and the use of more efﬁcient training algorit hms, have been explored. In par-\\nticular, there has been signiﬁcant interest in parameter-e fﬁcient ﬁne-tuning methods which\\nare designed to maintain state-of-the-art performance wit hout the need for extensive compu-\\ntational resources. W e have seen in Chapter 3 that applying t echniques like soft prompts can'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 167, 'page_label': '161'}, page_content='4.2 Instruction Alignment 161\\nUser: I’ve been feeling very tired lately .\\nChatbot: I’m sorry to hear that. Besides feeling tired,\\nhave you noticed any other symptoms?\\nUser: Y es, I’m also experiencing headaches frequently .\\nChatbot: How long have these symptoms been going on?\\n.\\n.\\n.\\nx1 y1 x2 y2 · · ·\\nPrθ (x1 ) Prθ (y1 |x1 ) Prθ (x2 |x1 ,y1 ) Prθ (y2 |x1 ,y1 ,x2 )\\nLoss = 0 Loss ̸= 0 Loss = 0 Loss ̸= 0\\nFig. 4.3: Illustration of supervised ﬁne-tuning for conversational models. Here the LLM acts as a chatbot to respond to\\neach request based on the conversational history . The conve rsation progresses by alternating between the user and the\\nchatbot. In SFT , we treat the entire conversation as a sequen ce, just like in standard LLMs, but compute the loss only\\nfor the responses of the LLM.\\nmake the ﬁne-tuning process more efﬁcient. For further disc ussion on parameter-efﬁcient\\nmethods, the reader can refer to related papers on this issue [Houlsby et al. , 2019; Hu et al. ,\\n2022; Han et al. , 2024].\\n• SFT can be regarded as a post-training step following pre-t raining. It is a separate training\\nphase designed to preserve the advantages of the initial pre -training while incorporating new\\nadjustments. This may seem paradoxical because updating a p re-trained LLM with further\\ndata potentially causes the model to forget some of its prior knowledge. Imagine a scenario\\nwhere we have a large amount of SFT data and extensively ﬁne-t une the LLM. In this\\ncase, the LLM could overﬁt the data, which in turn may reduce g eneralization performance\\nor cause catastrophic forgetting. A common strategy to miti gate this issue is to employ\\nregularization and early stopping techniques. Another pra ctical approach is to use a smaller\\nlearning rate to gently adjust the weights of the LLM. In addi tion, ﬁne-tuning with data from\\ndiverse sources and problem domains can also be beneﬁcial. N evertheless, in practice, the\\nSFT step is often carefully examined and requires substanti al engineering and experimental\\nefforts to optimize.\\n4.2.2 Fine-tuning Data Acquisition\\nFine-tuning data is so important that much recent work in LLMhas focused on developing various\\ndatasets for instruction ﬁne-tuning. As with most work in ma chine learning, there are generally\\ntwo approaches to data acquisition — manual data generation and automatic data generation.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 168, 'page_label': '162'}, page_content='162 Alignment\\n4.2.2.1 Manually Generated Data\\nOne straightforward method is to recruit human annotators to create input-output pairs for the\\ntasks of interest. Unlike data annotation in conventional N LP , such as text classiﬁcation, where\\nannotators simply assign labels to collected texts accordi ng to guidelines, creating ﬁne-tuning data\\nfor LLMs requires more steps and effort, making it thus more c hallenging. Suppose we want to\\nobtain ﬁne-tuning data for the English-to-Chinese machine translation task. The ﬁrst step is to\\nwrite a prompt template to describe the task and format the pr oblem clearly . For example,\\nInstruction Translate the text from English to Chinese.\\nUser Input {∗text∗}\\nOutput {∗translation∗}\\nThen, we collect pairs of source and target texts (i.e., Chin ese texts and the corresponding\\ntranslations), and replace the variables {∗text∗} and {∗translation∗} to generate the ﬁne-tuning\\nsamples. For example, given a pair of English and Chinese sen tences\\nHow’s the weather today? → 今天天气怎么样？\\n{∗text∗} {∗ translation∗}\\nwe can generate a ﬁne-tuning sample using the prompt templat e, like this\\nInstruction Translate the text from English to Chinese.\\nUser Input How’s the weather today?\\nOutput 今天天气怎么样？\\nThat is,\\nx = Translate the text from English to Chinese. \\\\n How’s the weather today?\\ny = 今天天气怎么样？\\nW e can use this (x,y) pair to ﬁne-tune the LLM, as described in the previous subsec tion.\\nOne difﬁculty here is that there are many , many different way s to write prompt templates\\nfor the same task, and different people may produce prompt te mplates with varying qualities\\nand complexities. Sometimes, we may write prompt templates with overly complex or verbose\\ninstructions. Sometimes, we may not even know exactly what t he target task is and how to de-\\nscribe it. A widely-adopted strategy is to create prompt tem plates for existing NLP tasks, given\\nthat there have been so many well-established NLP problems a nd benchmarks [ Bach et al. , 2022;\\nW ang et al. , 2022b; Mishra et al. , 2022]. In this case, annotators can be given the original task\\ndescription and many examples. Then, they can use their own w ays to express how to prompt the\\nLLM to perform the tasks. Note that, while such a method can ea se the process of creating and\\nwriting prompts, we still need annotation frameworks and cr owdsourcing systems to manage the\\nwork and conduct quality control. For example, we generally need to design annotation guidelines'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 169, 'page_label': '163'}, page_content='4.2 Instruction Alignment 163\\nand a uniﬁed format for writing prompt templates, especiall y when many annotators are contribut-\\ning to the same task. One advantage of inducing prompts from e xisting NLP tasks is that, once the\\nprompt templates have been developed, it is easy to generate prompts using the annotated samples\\nin the original tasks. For example, given a bilingual datase t for English-to-Chinese translation, we\\ncan easily create a number of ﬁne-tuning examples by ﬁlling t he slots in the above template with\\nthe sentence pairs in this dataset.\\nAnother approach is to directly use the naturally existing d ata available on the internet. A\\ncommon example is by collecting question-and-answer pairs from QA websites to ﬁne-tune LLMs\\nfor open-domain QA tasks [ Joshi et al. , 2017]. Many benchmarks in QA are built in this way\\nbecause there are so many types of questions that it is imposs ible to think of them all by a small\\ngroup of people. Instead, using data from those websites can ensure that the LLM ﬁne-tuning data\\nis at a good or acceptable level in terms of quantity and quali ty .\\nIn addition to employing existing resources, another strai ghtforward way to develop a ﬁne-\\ntuning dataset is to crowdsource the data. A simple approach is to allow users to input any ques-\\ntion, after which responses are either manually given or aut omatically generated by an LLM and\\nthen manually annotated and corrected. It is thus possible t o capture real user behavior and conse-\\nquently gather inputs and outputs for a large number of “new” problems that traditional NLP tasks\\ndo not cover.\\nAn issue related to the construction of the ﬁne-tuning datas ets is that we usually want the\\ndata to be as diverse as possible. Many studies have found tha t increasing the diversity of ﬁne-\\ntuning data can improve the robustness and generalization a bility of LLMs. For this reason, there\\nhas been considerable interest in involving more diverse pr ompts and tasks in LLM ﬁne-tunining\\ndatasets. W e will give further discussions on the generaliz ation of ﬁne-tuning in Section 4.2.4.\\n4.2.2.2 Automatically Generated Data\\nOne limitation of manual data generation is that the qualityand diversity largely depend on human\\nexperience and creativity . Therefore, if we want LLMs to han dle a broad range of tasks, that\\nis, to effectively execute any instruction, relying on huma n-annotated data for LLM ﬁne-tuning\\nis often inefﬁcient. Moreover, the coverage of such data can be limited, and the data may even\\ncontain biases introduced by the annotators themselves. An alternative approach is to generate data\\nautomatically . For example, we can collect a number of quest ions through crowdsourcing, and\\nemploy a well-tuned LLM to generate answers to the questions . These question-answer pairs are\\nthen used as ﬁne-tuning samples as usual. This method, thoug h very simple, has been extensively\\napplied to generate large-scale ﬁne-tuning data for LLMs.\\nThe above way of producing synthetic ﬁne-tuning data is simi lar to those used in data aug-\\nmentation for NLP . If we have an LLM, we can produce a predicti on in response to any input.\\nRepeating this process for different inputs allows us to cre ate a sufﬁcient number of ﬁne-tuning\\nsamples. Such a method is particularly useful for ﬁne-tunin g new LLMs using a well-tuned LLM.\\nHowever, one disadvantage of this approach is that it relies on human-crafted or collected inputs\\nfor data generation, which may turn out to be inappropriate f or generalizing LLMs. In many LLM\\napplications, a signiﬁcant challenge arises from the broad range of users’ questions and requests,\\nmany of which are not covered in existing NLP tasks and datase ts. In these cases, it becomes\\nnecessary to generate not only the predictions but also the i nputs themselves.\\nHere we consider self-instruct as an example to illustrate how to generate LLM ﬁne-tuning'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 170, 'page_label': '164'}, page_content='164 Alignment\\nInitialization\\nInitialize the task pool with a number of instructions\\nand corresponding input-output samples.\\nSample 1:\\n(Instruction, User-input, Output)\\nSample 2: (Instruction, User-input, Output)\\n· · ·\\nT ask Pool\\nSampling\\nDraw a few instructions from the pool\\nInstructiona\\nInstructionb\\nInstructionc\\nT ask Pool\\nsampling\\nInstruction\\nGeneration\\nPrompt the LLM to generate a new instruction based on\\nthe drawn instructions.\\nYou are provided several different instructions for perfor ming\\nsome tasks. Please generate an instruction based on these.\\nTask 1:\\nInstructiona\\nTask 2: Instructionb\\nTask 3: Instructionc\\nNew Task: Instructionnew\\nSample\\nGeneration\\nGiven the newly-generated instruction and a few\\ninput-output samples, generate a new sample.\\nYou are provided with a set of input-output samples tasks,\\neach composed of an instruction, a user input, and an output.\\nPlease generate a new sample based on these.\\nSample 1:\\nSamplea\\nSample 2: Sampleb\\nNew Sample: Instructionnew User-inputnew Outputnew\\nFiltering\\nFilter out invalid and low-quality samples.\\nAdd the remaining samples into the pool.\\nFig. 4.4: Illustration of self-instruct [ W ang et al. , 2023b]. This method maintains a pool of instructions and correspo nd-\\ning input-output samples. Initially , the pool contains a nu mber of hand-crafted instructions and samples. Each time,\\nwe draw a few instructions from the pool. An LLM is then prompt ed to generate new instructions and samples based\\non those drawn. Finally , the newly-generated instructions and samples are ﬁltered and added to the pool.\\nsamples [ W ang et al. , 2023d; Honovich et al. , 2023]. The idea is that we can prompt an LLM to\\ncreate a new instruction by learning from other instruction s. Given this instruction, the LLM can\\nthen ﬁll in other ﬁelds (such as the user input) and produce th e predictions. Figure 4.4 shows a\\nschematic illustration of self-instruct. Here we give a bri ef outline of the key steps involved.\\n• The self-instruct algorithm maintains a pool of tasks. Ini tially it contains a number of seed\\nhand-crafted tasks, each with an instruction and input-out put sample. As the algorithm\\nproceeds, LLM-generated instructions and samples will be a dded to this pool.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 171, 'page_label': '165'}, page_content='4.2 Instruction Alignment 165\\n• At each step, a small number of instructions are drawn from t he instruction pool. For ex-\\nample, we can randomly select a few human-written instructi ons and a few LLM-generated\\ninstructions to ensure diversity .\\n• The selected instructions are then used as demonstration e xamples. Thus, the LLM can\\nin-context learn from these examples and produce a new instr uction. Below is an example\\ntemplate for prompting the LLM.\\nY ou are provided several different instructions for perfor ming some tasks.\\nPlease generate an instruction based on these.\\nT ask 1:{instruction1}\\nT ask 2: {instruction2}\\nT ask 3: {instruction3}\\nT ask 4: {instruction4}\\nNew T ask:\\n• Given the generated instruction, the LLM is then prompted t o complete the sample by ﬁlling\\nin the remaining input ﬁelds and generating the correspondi ng output. Below is a prompt\\ntemplate.\\nY ou are provided with a set of input-output samples, each com posed of\\nan instruction, a user input, and an output. Please generate a new sample\\nbased on these.\\nSample 1:{instruction1}\\nInput: {user-input1}\\nOutput: {output1}\\nSample 2: {instruction2}\\nInput: {user-input2}\\nOutput: {output2}\\nNew Sample: {new-instruction}\\n• This newly-generated sample is examined by some heuristic rules (such as ﬁltering out\\nsamples or instructions that are similar to those already in the pool). If it passes, the sample\\nand instruction are added to the pool.\\nThis generation process can be repeated many times to obtain a sufﬁcient number of ﬁne-\\ntuning samples. Note that, above, we just show simple prompt templates for generating instruction\\nand ﬁne-tuning samples. Of course, we can develop better tem plates to generate more diverse and\\naccurate instruction and ﬁne-tuning samples. For example, for certain tasks like text classiﬁcation,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 172, 'page_label': '166'}, page_content='166 Alignment\\nthe LLM may tend to produce biased predictions, for example, most generated samples belong to\\na single class. In such cases, we can adjust the order of gener ation of different ﬁelds. More\\nspeciﬁcally , we can specify the output (i.e., the class) wit h some prior, and prompt the LLM\\nto generate user input given both the instruction and the out put. This method resembles input\\ninversion, where the LLM generates the input based on the speciﬁed outp ut [ Longpre et al. , 2023].\\nUsing LLM-generated instructions and ﬁne-tuning samples h as been a common method for\\ndeveloping LLMs, especially given that manually developin g such data is so expensive that most\\nresearch groups cannot afford it. In several well-tuned LLM s, their ﬁne-tuning datasets include\\na certain amount of synthetic data, which has proved useful [ Ouyang et al. , 2022; T aori et al. ,\\n2023; Chiang et al. , 2023]. There have been further studies on synthetic data generat ion for LLM\\nﬁne-tuning. For example, one can generate more diverse inst ructions by introducing evolutionary\\nalgorithms [ Xu et al. , 2024], or use synthetic data as supervision signals in a more adva nced ﬁne-\\ntuning process [ Chen et al. , 2024b]. More recently , there has also been considerable interest in\\nusing synthetic data in the pre-training stage [ Gunasekar et al. , 2023; Allal et al. , 2024].\\nIn many applications, a real-world scenario is that, given a task, we can collect or annotate a\\nrelatively small amount of ﬁne-tuning data, for example, we can recruit experts to create questions\\nfor QA tasks in a speciﬁc domain. But the quantity and diversi ty of this data are in general not\\nsufﬁcient. In this case, we can use self-instruct technique s to generate more diverse question-\\nanswer pairs, and thus augment the ﬁne-tuning data. This pro vides a way of bootstrapping the\\nLLM starting from a seed set of ﬁne-tuning samples. Note that using self-generated data is a com-\\nmon practice and has long been applied in NLP . For example, th is approach has been successfully\\nused in parsing and machine translation [ Charniak, 1997; Sennrich et al. , 2016].\\n4.2.3 Fine-tuning with Less Data\\nWith the increasing prominence of instruction ﬁne-tuning,there has been a surge in demand for\\nlarge-scale, high-quality ﬁne-tuning data. For example, t he FLAN ﬁne-tuning dataset, which\\nis compiled from 1,836 tasks, contains 15 million samples [\\nLongpre et al. , 2023]. Fine-tuning\\nLLMs with such large datasets is typically a computationall y expensive task, especially given\\nthat updating the large number of parameters in LLMs is resou rce-intensive. One approach for\\nmitigating this issue is to explore efﬁcient model training methods, for example, one can use\\nparameter-efﬁcient methods to update only a small portion o f the model. However, many ﬁne-\\ntuning datasets contain a large amount of synthetic data, wh ere errors and biases are still inevitable.\\nAnother approach to efﬁcient ﬁne-tuning is to consider only the most relevant and impactful\\nexamples for ﬁne-tuning. W e can thus reduce the amount of dat a that needs to be processed while\\nstill maintaining the quality of the model updates. There ar e several methods to achieve this. For\\nexample, Zhou et al. [2023a] built an instruction-following dataset containing only 1 ,000 sam-\\nples by carefully crafting the prompts and collecting sampl es from a variety of NLP tasks. They\\nshowed that the LLaMa 65B model ﬁne-tuned with this dataset c ould be competitive with or even\\nbetter than models with much more ﬁne-tuning effort. This su ggests that LLMs can be adapted\\nto respond to diverse tasks without necessarily needing ﬁne -tuning on all types of instruction-\\nfollowing data. Chen et al. [2024a] developed a system based on the GPT -3.5 model to assess\\nthe quality of each instruction-following sample. Therefo re, they could select high-quality sam-\\nples from existing datasets, showing better ﬁne-tuning per formance with fewer ﬁne-tuning sam-\\nples. Researchers have also developed methods to either sel ect or ﬁlter out data using heuristics'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 173, 'page_label': '167'}, page_content='4.2 Instruction Alignment 167\\n[Zhao et al. , 2024; Ge et al. , 2024], or to prioritize data that more signiﬁcantly inﬂuences th e ﬁne-\\ntuning process [ Xia et al. , 2024]. In fact, most of these methods can be seen as instances of la rger\\nfamilies of data selection and ﬁltering methods. And it is of ten the case that using higher quality\\n(but maybe less) data is beneﬁcial for training NLP models.\\nThe discoveries in instruction ﬁne-tuning somewhat differ from traditional views in NLP: the\\nability of models to handle complex problems can be activate d with a small amount of annotated\\ndata, rather than requiring massive amounts of supervised d ata for extensive training. One pos-\\nsible explanation is that the ability of generating correct responses given instructions has been\\nlearned during pre-training, but such instruction-respon se mappings are not with high probabil-\\nities during inference. Fine-tuning can slightly adjust th e models to get them to follow instruc-\\ntions, requiring signiﬁcantly less training effort than pr e-training. This is closely related to what\\nis known as the superﬁcial alignment hypothesis , which suggests that learning primarily occurs\\nduring pre-training, and the subsequent ﬁne-tuning or alig nment phase does not signiﬁcantly con-\\ntribute to the underlying knowledge base of an LLM [ Zhou et al. , 2023a]. Since the core abilities\\nand knowledge of the model are already established from pre- training, effective ﬁne-tuning for\\nalignment with user needs can be achieved with relatively sm all training ﬁne-tuning effort. This\\nimplies the possibility of ﬁne-tuning LLMs with very little data. In another direction, it may not\\nbe necessary to restrict ﬁne-tuning to paired instruction- response data. For example, Hewitt et al.\\n[2024] found that instruction-following can be implicitly achie ved by ﬁne-tuning LLMs only on\\nresponses, without corresponding instructions.\\nA concept related to the discussion here is sample efﬁciency . A machine learning method is\\ncalled sample efﬁcient if it can learn effectively from a small number of training ex amples. In this\\nsense, instruction ﬁne-tuning is sample efﬁcient compared with pre-training. From the perspective\\nof machine learning, sample-efﬁcient methods can be seen as efﬁcient ways to sample the space\\nof data, and are advantageous as they make optimal use of scar ce data. Therefore, sampling-based\\nlearning techniques, such as many reinforcement learning a lgorithms, can beneﬁt from these sam-\\nple efﬁcient approaches. For example, in human preference a lignment, we can either efﬁciently\\nsample preference data via reward models [ Liu et al. , 2024b] or improve sampling efﬁciency in\\npolicy learning [ W ang et al. , 2024].\\n4.2.4 Instruction Generalization\\nIn many machine learning and NLP problems, training a model to generalize is a fundamental\\ngoal. For example, in text classiﬁcation, we expect our mode l to correctly classify new texts that\\nwere not seen during training. However, generalization pos es additional challenges in instruction\\nﬁne-tuning. W e expect instruction-ﬁne-tuned LLMs to not on ly generate appropriate responses for\\ndifferent inputs within a task but also to accurately perfor m various tasks as described by different\\ninstructions. T o illustrate this issue, consider an LLM Pr(y|c,z), where c is an instruction, z\\nis a user input, and y is the corresponding model output (i.e., the response). Sup pose that the\\nperformance of this model is evaluated in terms of a metric, w ritten as Performance(Pr(y|c,z))\\nor P(c,z,y) for short. Informally , when we say this model can generalize within a given task\\n(indicated by the instruction c∗ ), we mean that there may be a value ǫ such that the average\\nperformance on new inputs is above this value:\\n1\\n|Z |\\n∑\\nz′ ∈Z\\nP(c∗ ,z′ ,y′ ) >ǫ (4.8)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 174, 'page_label': '168'}, page_content='168 Alignment\\nwhere Z is the set of new inputs, and z′ and y′ are an input in this set and the corresponding\\noutput, respectively .\\nLikewise, we can say that this model can generalize across ta sks if the average performance\\nover all instruction-input pairs is above some ǫ:\\n1\\n|D|\\n∑\\n(c′ , z′ )∈D\\nP(c′ ,z′ ,y′ ) >ǫ (4.9)\\nwhere D is the set of new instruction-input pairs.\\nHere, we need to deal with variations in two dimensions: inst ruction and user input. This\\nmakes the generalization problem very complex, because, in tuitively , a model needs to learn from\\na vast number of tasks and different input-output pairs asso ciated with each task to achieve good\\ngeneralization. As we have discussed several times in this b ook, achieving such generalization\\nincurs much lower cost than pre-training. In general, ﬁne-t uning LLMs with instruction-response\\ndata to some extent can lead to models yielding instruction f ollowing on new tasks. Nevertheless,\\nit is typically believed that certain efforts are still need ed to adapt LLMs to make them understand\\nand execute instructions broadly .\\nOne way to generalize instruction ﬁne-tuning is to increase the diversity of the ﬁne-tuning\\ndata. In earlier studies on instruction ﬁne-tuning, resear chers developed many datasets, covering a\\nwide variety of NLP tasks and different instructions for eac h task [ W ang et al. , 2022b; Sanh et al. ,\\n2022; Longpre et al. , 2023]. By transforming these tasks into a uniﬁed format, one can ﬁ ne-\\ntune an LLM with a sufﬁciently large number of samples, for ex ample, there have been several\\ninstruction ﬁne-tuning datasets that involve over 100 NLP t asks and 1M samples. However, these\\nearly datasets mostly focus on existing academic problems, but not those that users want to deal\\nwith in real-world applications. Much recent work has shift ed focus to addressing new and more\\npractical problems. For example, there has been considerab le interest in constructing datasets\\nthat contain large and complicated demonstrations and resp onses from SOT A models to real user\\nqueries [ W ang et al. , 2023c; T eknium, 2023].\\nPerhaps the use of large and diverse ﬁne-tuning datasets has its origins in attempts to scale\\nLLMs in different dimensions. Indeed, scaling laws have bee n used broadly to motivate the de-\\nvelopment of a wide range of different instruction-ﬁne-tun ed LLMs. And it is reasonable to scale\\ninstruction ﬁne-tuning to make an LLM follow broad instruct ions. From the perspective of LLM\\nalignment, however, scaling instruction ﬁne-tuning might not be efﬁcient to achieve generaliza-\\ntion.\\nOne problem is that instruction ﬁne-tuning relies on superv ised learning that learns to gener-\\nalize and perform tasks based on instruction-response mapp ings. However, such an approach does\\nnot capture subtle or complex human preferences (e.g., tone , style, or subjective quality) because\\nthese are hard to encode as explicit instruction-response d ata. Moreover, the generalization per-\\nformance is bounded by the diversity and quality of the instr uction-response dataset. Given these\\nlimitations, we would instead like to employ preference mod els as an additional ﬁne-tuning step\\nfollowing instruction ﬁne-tuning, so the LLMs can generali ze further (see Section 4.3).\\nAnother view is that some instruction-response mappings ma y already be learned during pre-\\ntraining, and so the pre-trained LLMs have encoded such mapp ings. However, since we often do\\nnot know exactly what data is used in the pre-training, it is h ard to judge whether we need to learn\\nsuch mappings in the ﬁne-tuning. A related question is wheth er out-of-distribution generalization'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 175, 'page_label': '169'}, page_content='4.2 Instruction Alignment 169\\nis primarily achieved during pre-training or ﬁne-tuning. W hile directly answering this question is\\nbeyond the scope of this chapter, it has been shown that pre-t raining on large and diverse datasets\\nis effective in improving out-of-distribution performanc e [ Hendrycks et al. , 2020; Radford et al. ,\\n2021; Gunasekar et al. , 2023]. This raises an interesting problem: if an LLM has been well pre-\\ntrained at scale, ﬁne-tuning may not be as essential for out- of-distribution generalization, since the\\nmodel may have already encountered sufﬁcient distribution al variation. This prompts researchers\\nto ﬁne-tune LLMs with modest effort or to explore new methods to achieve instruction-following.\\nAs discussed in the previous sub-section, for example, inst ruction following can be yielded by\\nﬁne-tuning on a small number of carefully selected instruct ion-response pairs [ Zhou et al. , 2023a],\\nor even by using methods that are not explicitly designed to d o so [ Kung and Peng , 2023].\\nThe above discussion provides two different strategies: on e requires scaling up ﬁne-tuning\\ndatasets for larger diversity , the other requires small but necessary ﬁne-tuning datasets for efﬁcient\\nLLM adaptation. However, in practice, involving diverse in structions often helps. In many cases,\\nwe need to adapt our LLM for speciﬁc purposes. But the LLM, whi ch has possibly encoded broad\\ninstruction-following mappings during pre-training, mig ht tend to behave as a general-purpose\\ninstruction executor even with modest ﬁne-tuning. An inter esting phenomenon is that when ﬁne-\\ntuning on math data, the resulting LLM might not specialize i n math outputs. Instead, this model\\nmight respond normally to general instructions, for exampl e, it could generate poetry if instructed\\nto do so [ Hewitt, 2024]. This is not a bad thing, but it shows that LLMs may not easily change their\\nnature of following general instructions. In this case, add itional adaptations with more diverse\\ndata may help adjust the way the LLM follows instructions, pa rticularly for those tasks we aim to\\naddress.\\n4.2.5 Using Weak Models to Improve Strong Models\\nSo far we have explored a variety of instruction ﬁne-tuning methods based on labeled data. One\\nof the limitations of many such methods is that they require t he data to be annotated by humans or\\ngenerated by strong LLMs, which can provide accurate superv ision signals in ﬁne-tuning. How-\\never, in many cases, the LLM we have in hand is already strong ( or at least is advantageous in\\nspeciﬁc aspects of problem solving), and thus it is not easy t o ﬁnd a superior model for supervi-\\nsion. Even for human experts, when the problem becomes compl ex, providing correct and detailed\\nanswers might be difﬁcult, or sometimes infeasible. For exa mple, when faced with an extremely\\nlong document, the experts would ﬁnd it challenging to ident ify any inconsistencies, subtle biases,\\nor missing key points without conducting an exhaustive and t ime-consuming review .\\nOne may ask at this point: can we use weak LLMs to supervise str ong LLMs? This seems\\nto be a signiﬁcant challenge, but it may reﬂect a future scena rio where we need to supervise AI\\nsystems that are smarter than humans or any other AI systems [\\nBurns et al. , 2023b]. The problem\\nof using smaller, less complex models to improve the trainin g of larger, more complex models\\nis also called the weak-to-strong generalization problem. While there have not been mature\\napproaches to weak-to-strong generalization, using small er models to assist stronger models has\\nindeed proven useful in several areas of LLMs.\\nFor instruction ﬁne-tuning, one of the simplest ways of appl ying weak LLMs is to use these\\nmodels to generate synthetic ﬁne-tuning data. Suppose we ha ve a collection of inputs X, where\\neach input includes an instruction and a user input if necess ary . For each x ∈ X, we use a weak\\nLLM Prw (·) to generate a prediction ˆy = arg max y Prw (y|x). Then, the strong LLM Prs\\nθ(·) can'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 176, 'page_label': '170'}, page_content='170 Alignment\\nbe trained on these generated predictions (see Eq. ( 4.1)):\\n˜θ = arg max\\nθ\\n∑\\nx∈X\\nlog Pr s\\nθ(ˆy|x) (4.10)\\nwhere θis the model parameters.\\nThe above form transforms the ﬁne-tuning problem into a know ledge distillation problem, in\\nother words, we distill knowledge from the weak model to the s trong model. Consequently , we\\ncan employ various knowledge distillation methods to achie ve this goal. However, explaining\\nweak-to-strong ﬁne-tuning from the perspective of knowled ge distillation is not straightforward.\\nA major concern is that the strong model may merely imitate or overﬁt the errors of the weak\\nmodel and fail to generalize. For example, the ﬁne-tuned str ong model still cannot solve difﬁcult\\nproblems that the weak model cannot accurately predict. For tunately , preliminary experiments in\\nthis line of research have shown positive and promising resu lts. For example,\\nBurns et al. [2023a]\\nfound that ﬁne-tuning the strong pre-trained GPT -4 model wi th GPT -2-level supervision could\\nimprove generalization across several NLP tasks. T o measur e how the weak model improves the\\ngeneralization of the strong model, we deﬁne the following t erms:\\n• W eak Performance (Pweak). This is the test-set performance of the weak model, which\\ncan be regarded as the baseline performance.\\n• W eak-to-strong Performance(Pweak→strong). This is the test-set performance of the strong\\nmodel that is ﬁne-tuned with the weak model.\\n• Strong Ceiling Performance (Pceiling ). This is the test-set performance of the strong model\\nthat is ﬁne-tuned with ground truth data. For example, we ﬁne -tune the strong model with\\nhuman-annotated predictions and take the resulting model a s a ceiling.\\nThen, the performance gap recovered (PGR) can be deﬁned as\\nPGR = max\\n{\\n0,Pweak→strong − Pweak\\nPceiling − Pweak\\n}\\n(4.11)\\nThis metric measures how much of the performance gap between the ceiling model and the\\nweak model can be recovered by the weak-to-strong model. A PG R of 1 indicates that the weak-\\nto-strong ﬁne-tuning can completely closes the performanc e gap, whereas a PGR of 0 indicates\\nno improvement. In Burns et al. [2023a]’s work, it is shown that PGR can be around 0.8 on 22\\nNLP classiﬁcation tasks. It should be noted that, while the p otential of weak-to-strong ﬁne-tuning\\nis promising, achieving substantial weak-to-strong gener alization remains a challenging goal that\\nneeds further investigation [ Aschenbrenner, 2024].\\nFine-tuning LLMs with weak supervision is just one choice fo r using small models to improve\\nlarge models. Although this section primarily focuses on ﬁn e-tuning LLMs, we also mention\\nother methods here to give a more complete discussion (see Fi gure 4.5 for illustrations of these\\nmethods).\\n• Instead of using small models to generate synthetic data, i t is also straightforward to in-\\ncorporate knowledge distillation loss based on these model s. For example, a simple loss'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 177, 'page_label': '171'}, page_content='4.2 Instruction Alignment 171\\nLarge Model\\nx\\nˆy\\nCompute Loss & Train\\nDataset\\n=⇒x ˆy\\nSmall Model\\nInput Predict\\n(a) Fine-tuning on data generated by a small model\\n(weak-to-strong generalization)\\nLarge Model\\nx\\ny\\nCompute Loss & Train\\nDataset\\n=⇒x y\\nSmall Model\\nKD Loss LM Loss\\n(b) Fine-tuning with KD Loss from a small model\\n(weak-to-strong generalization)\\nLarge Model\\nx\\ny\\nCompute Loss & Train\\nDataset\\n=⇒x y\\nDataset\\nSmall Model Data\\nSelection\\n(c) Data selection with a small model\\nSmall Model 2Small Model 1 Small Model 3\\nx x x\\nCombination Model\\ny\\n(d) Ensemble of multiple small models\\nLarge Model\\nx\\ny2\\nSmall Model\\n(e) Cascading (at inference time)\\nx\\ny1\\nStep 2\\n(expensive)\\nStep 1\\n(cheap)\\nIf Step 1 is not satisfactory , go to Step 2\\nFig. 4.5: Illustrations of using small models to improve large models in LLMs. One approach involves using smaller\\nmodels for the ﬁne-tuning or pre-training of larger models. This includes generating synthetic data (a), incorporatin g\\nauxiliary loss (b), and selecting appropriate data (c). Ano ther approach involves combining small models and large\\nmodels. This includes learning a strong model by aggregatin g multiple small models (d), and cascading small models\\nwith large models (e).\\nfunction that measures the difference between the small and large models can be deﬁned as:\\nLosskd = KL(Pr w (·|x) || Prs\\nθ(·|x)) (4.12)\\nThen, we can add this loss to the original loss of language mod eling, and yield the following\\ntraining objective\\n˜θ = arg max\\nθ\\n∑\\n(x, y)∈D\\nlog Pr s\\nθ(y|x) − λ· Losskd (4.13)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 178, 'page_label': '172'}, page_content='172 Alignment\\nwhere D is the set of input and output pairs, and λis the coefﬁcient of the interpolation. This\\nmethod can be employed in either the pre-training or ﬁne-tun ing phase. W e can adjust λto\\ncontrol how much the small model inﬂuences the training. For example, we can gradually\\ndecrease λ to make the training rely more on the original language model ing loss as the\\nlarge model becomes more capable.\\n• Another approach to involving small models in LLM pre-trai ning and ﬁne-tuning is to use\\nthem to do data selection or ﬁltering. Given a sequence, we ca n compute the likelihood\\nor cross-entropy using a small model. These quantities can t hen be used as criteria for\\nselecting or ﬁltering data. For example, sequences with low likelihood or high cross-entropy\\nmight be excluded from the training set, as they are less alig ned with the small model’s\\nlearned distribution. Conversely , sequences with high lik elihood or low cross-entropy can\\nbe prioritized, ensuring that the training focuses on more r elevant or high-quality data.\\n• Ensemble learning is a simple and effective way to build a st rong model by combining mul-\\ntiple weak models. Applying this technique to LLMs is straig htforward. W e can aggregate\\ndistributions predicted by multiple small models or specia lized submodels, and derive the\\nﬁnal prediction from the aggregated results. This aggregat ion can be done using methods\\nsuch as majority voting, weighted averaging, or stacking.\\n• Small models can also be employed at inference time to impro ve overall efﬁciency . Suppose\\nwe have a large model that is slow but more accurate, and a smal l model that is fast but\\nless accurate. In model cascading, the small model ﬁrst proc esses the input data, quickly\\ngenerating preliminary results. If these results meet cert ain pre-deﬁned criteria, they can be\\ndirectly used. However, if the initial results are not sufﬁc iently good, the input is then passed\\nto the larger, more accurate model to produce a better result . This approach signiﬁcantly\\nreduces computational costs and latency , as the small model can effectively handle many\\ninputs without access to the large model.\\n4.3 Human Preference Alignment: RLHF\\nSo far in this chapter, we have focused on ﬁne-tuning LLMs using input-output paired labeled data.\\nThis approach allows us to adapt LLMs for instruction-follo wing via supervised learning. In many\\napplications, however, LLMs are required not only to follow instructions but also to act in ways\\nthat are more aligned with human values and preferences. Con sider a scenario where a user asks an\\nLLM how to hack into a computer system. If the LLM is not approp riately aligned, it may respond\\nby providing details on how to perform this illegal activity . Instead, a more desirable response\\nmight be to advise the user against engaging in illegal activ ities and offer a general overview of\\nthe consequences of such actions. The difﬁculty in achievin g this is that the ethical nuances and\\ncontextual considerations required for an LLM to respond ap propriately in such scenarios are not\\nalways straightforward to encode into a ﬁne-tuning dataset . What’s even more challenging is that,\\noften, humans themselves cannot precisely express their ow n preferences.\\nIn this section, we discuss an alternative LLM ﬁne-tuning me thod, called reinforcement learn-\\ning from human feedback or RLHF for short [\\nChristiano et al. , 2017; Stiennon et al. , 2020]. The\\nbasic idea behind RLHF is that LLMs can learn from comparison s of model outputs using reward'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 179, 'page_label': '173'}, page_content='4.3 Human Preference Alignment: RLHF 173\\nLLM\\nx\\ny\\nPredicted T oken Distributions\\n(n token distributions)\\nGold-standard Predictions\\n(n one-hot distributions)\\nObjective (MLE):\\nmax Pr( y|x)\\nwhere\\nx: input\\ny: gold-standard output\\n(a) Supervised ﬁne-tuning (maximizing the prediction prob ability given the input)\\nLLM\\nx\\ny1y2\\nGenerate multiple\\noutputs via sampling\\nPrediction y2\\nPrediction y1\\nObjective (RL Loss Minimization):\\nmin L(x,{y1,y2 },r)\\nReward ModelHuman preference data train\\nwhere\\nL(·): loss function\\nr(·): reward model\\n(b) Reinforcement Learning from Human Feedback\\nFig. 4.6: Supervised ﬁne-tuning vs. reinforcement learning from hum an feedback. In supervised ﬁne-tuning, we\\noptimize the LLM by maximizing the probability of the predic tion given the input. In reinforcement learning from\\nhuman feedback, we ﬁrst train a reward model on human prefere nce data (on each pair of predictions, evaluators are\\nasked to choose which one they prefer). Then, we use this rewa rd model to supervise the LLM during ﬁne-tuning.\\nmodels (see Figure 4.6). T o do this, we can recruit human experts who indicate their preferences\\nbetween pairs of outputs generated by the LLM. This preferen ce data is used to train a reward\\nmodel that can predict the perceived quality of LLM outputs. Once trained, the reward model\\nprovides feedback by assigning scores to new outputs that th e LLM generates in response to the\\ninputs. The LLM uses these scores to update its parameters th rough reinforcement learning algo-\\nrithms. In the rest of this section, we will ﬁrst introduce th e basic knowledge of reinforcement\\nlearning to facilitate the discussion, and then discuss met hods for training reward models and\\naligning LLMs with these models.\\n4.3.1 Basics of Reinforcement Learning\\nW e begin by looking at some basic concepts of reinforcement learning. Note that the notation used\\nhere slightly differs from that used in the previous section s and chapters because we want to make\\nour description more consistent with those in the reinforce ment learning literature. Nevertheless,\\nwe will show how this notation corresponds to the language mo deling notation. The reader who\\nis already familiar with reinforcement learning technique s may skip or skim this subsection.\\nA general reinforcement learning framework describes how a n agent interacts with a dynamic\\nenvironment. This interaction is modeled as a sequence of ac tions taken by the agent in response\\nto the state of the environment. At each time step, the agent o bserves the current state, chooses an\\naction based on its policy , performs the action, and then rec eives feedback from the environment\\nin the form of a reward and a new state. This sequence of observ e-act-receive feedback is repeated'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 180, 'page_label': '174'}, page_content='174 Alignment\\nuntil the agent achieves its goal.\\nA reinforcement learning system involves several componen ts\\n• Agent. This is the learner or decision-maker in reinforcement lea rning. In the context of\\nLLMs, it can be seen as the LLM itself.\\n• Environment. This includes everything external to the agent with which t he agent interacts.\\nBut the environment in LLMs is less about a physical or virtua l space and more about the\\nframework within which the agent (e.g., an LLM) receives fee dback and learns.\\n• State (s). A state represents the current situation of the environme nt. Given a sequence of\\ntokens for language modeling, a state at a time step can be vie wed as the tokens we observed\\nso far, that is, the context tokens we take to predict the next token. For example, we can\\ndeﬁne (x,y<t) as the state when predicting the next token at the time step t.\\n• Action (a). Actions represent possible decisions the agent can make. W e can see them as\\npossible predicted tokens in the vocabulary .\\n• Reward (R). The reward is the feedback from the environment that evalu ates the success\\nof an action. For example, r(s,a,s ′ ) denotes the reward the agent receives for taking the\\naction aat the state sand moving to the next state s′. If the state-action sequence is given,\\nwe can denote the reward at the time step t as rt = r(st,at,st+1 ). Also note that if the\\ndecision-making process is deterministic, we can omit st+1 because it is can be determined\\nby st and at. In such cases, we can use r(st,at) as shorthand for r(st,at ,st+1).\\n• Policy (π). For an LLM, a policy is deﬁned as the probability distribut ion over the tokens\\nthat the LLM predicts, given the preceding context tokens. F ormally , this can be expressed\\nas\\nπ(a|s) = Pr( yt|x,y<t) (4.14)\\nwhere acorresponds to the token y, and scorresponds to the context (x,y<t ). Figure 4.7\\nillustrates how an LLM can be treated as a policy in the reinfo rcement learning framework.\\n• V alue Function (V and Q). A state-value function (or value function, for short) assesses\\nthe expected discounted return (i.e., accumulated rewards ) for an agent starting from a par-\\nticular state sand following a speciﬁc policy π. It is deﬁned as:\\nV(s) = E\\n[\\nr(s0,a0 ,s1) + γr(s1,a1 ,s2) + γ2r(s2,a2 ,s3 ) + · · ·\\n⏐\\n⏐s0 = s,π\\n]\\n= E\\n[\\nr0 + γr1 + γ2 r2 + · · ·\\n⏐\\n⏐s0 = s,π\\n]\\n= E\\n[ ∞∑\\nt=0\\nγtrt\\n⏐\\n⏐s0 = s,π\\n]\\n(4.15)\\nwhere γ ∈ [0,1] is the discount factor that adjusts the importance of future rewards, s0 = s\\nindicates that the agent starts with the state s, and the expectation E is performed over all\\npossible trajectories (i.e., state-action sequences). Si milarly , an action-value function (or'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 181, 'page_label': '175'}, page_content='4.3 Human Preference Alignment: RLHF 175\\nx0 x1 ... xm y1 ... yt−1\\nPolicy (LLM)\\ny1 y2 ... yt\\nState st (x and y<t )\\nAction at\\nReward Model\\nR( st, at )\\nV alue Functions\\nV( st ) and Q( st, at )\\nFeedback\\nFig. 4.7: LLM as policy in reinforcement learning. At each step t, the LLM predicts a token yt given the model\\ninput x and the previously-generated tokens y<t. This process can be framed as a reinforcement learning prob lem,\\nwhere yt serves as the action, (x, y<t) as the state, and the predicted distribution Pr(yt |x, y<t ) as the policy . Once\\nyt is predicted, the LLM inputs both (x, y<t ) and yt to the reward model, which evaluates how effectively the cho sen\\ntoken contributes to achieving the desired textual outcome . This evaluation generates reward scores which are used to\\ncompute the value functions V(st) and Q(st,a t ). These functions then provide feedback to the LLM and guide t he\\npolicy training.\\nQ-value function ) measures the expected return starting from a state staking an action a\\nand thereafter following a policy π, given by\\nQ(s,a) = E\\n[ ∞∑\\nt=0\\nγtrt\\n⏐\\n⏐s0 = s,a0 = a,π\\n]\\n(4.16)\\nwhere a0 = aindicates that the action taken at the initial state is a.\\nThe goal of reinforcement learning is to learn a policy that m aximizes the cumulative re-\\nward (or return) the agent receives over the long run. Given a state-action s equence τ =\\n{(s1,a1 ),...,(sT ,aT )}\\n1 , the cumulative reward over this sequence can be written as\\nR(τ) =\\nT∑\\nt=1\\nrt (4.17)\\nThe expectation of this cumulative reward over a space of sta te-action sequences is given in\\nthe form\\nJ(θ) = E\\nτ ∼D\\n[\\nR(τ)\\n⏐\\n⏐πθ\\n]\\n=\\n∑\\nτ ∈D\\nPrθ (τ)R(τ)\\n=\\n∑\\nτ ∈D\\nPrθ (τ)\\nT∑\\nt=1\\nrt (4.18)\\n1 W e assume the state-action sequence begins with s1 and a1 , rather than s0 and a0 , to align with the notation\\ncommonly used in this chapter, where the prediction y typically starts from y1 . Of course, it is also common to denote\\na state-action sequence as {(s0 ,a 0 ),..., (sT ,a T )} or {(s0 ,a 0 ),..., (sT −1 ,a T −1 )} in the literature. But this variation\\nin notation does not affect the discussion of the models pres ented here.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 182, 'page_label': '176'}, page_content='176 Alignment\\nwhere τ ∼ D indicates that τ is drawn from the state-action sequence space D, and the subscript\\nθindicates the parameters of the policy . J(θ) is also called the performance function .\\nThen the training objective is to maximize J(θ):\\n˜θ = arg max\\nθ\\nJ(θ) (4.19)\\nNow , we have a simple reinforcement learning approach: 1) we sample a number of state-\\naction sequences; then, 2) we evaluate each sequence using t he performance function; then, 3) we\\nupdate the model to maximize this performance function. If w e take Eq. ( 4.18) and use gradient\\ndescent to optimize the policy , this approach would constit utes a form of policy gradient methods\\n[Williams, 1992].\\nNote that in many NLP problems, such as machine translation, rewards are typically sparse.\\nFor instance, a reward is only received at the end of a complet e sentence. This means that rt = 0\\nfor all t < T, and rt is non-zero only when t = T. Ideally , one might prefer feedback to\\nbe immediate and frequent (dense), and thus the training of t he policy can be easier and more\\nefﬁcient. While several methods have been proposed to addre ss sparse rewards, such as reward\\nshaping, we will continue in our discussion to assume a spars e reward setup, where the reward is\\navailable only upon completing the prediction.\\nThe model described in Eqs. ( 4.17-4.19) establishes a basic form of reinforcement learning,\\nand many variants and improvements of this model have been de veloped. Before showing those\\nmore sophisticated models, let us take a moment to interpret the objective function J(θ) from the\\nperspective of policy gradient. In gradient descent, we nee d to compute the gradient of J(θ) with\\nrespect to θ:\\n∂J(θ)\\n∂θ = ∂∑\\nτ ∈D Prθ (τ)R(τ)\\n∂θ\\n=\\n∑\\nτ ∈D\\n∂Prθ (τ)\\n∂θ R(τ)\\n=\\n∑\\nτ ∈D\\nPrθ (τ) ∂Prθ (τ)/∂θ\\nPrθ (τ) R(τ)\\n=\\n∑\\nτ ∈D\\nPrθ (τ) ∂log Pr θ (τ)\\n∂θ R(τ) (4.20)\\nIn some cases, we will assume that every sequence in D is equally probable (i.e., Prθ (τ) =\\n1/|D|). In this case we can simplify Eq. ( 4.20) and need only consider the terms ∂ log Pr θ (τ )\\n∂θ and\\nR(τ):\\n∂J(θ)\\n∂θ = 1\\nm\\n∑\\nτ ∈D\\n∂log Pr θ (τ)\\n∂θ R(τ) (4.21)\\nOne advantage of this result is that R(τ) does not need to be differentiable, which means that we\\ncan use any type of reward function in reinforcement learnin g.\\nBy treating the generation of the sequence τ as a Markov decision process, we can further'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 183, 'page_label': '177'}, page_content='4.3 Human Preference Alignment: RLHF 177\\nderive ∂ log Pr θ (τ )\\n∂θ , and obtain:\\n∂log Pr θ (τ)\\n∂θ = ∂\\n∂θ log\\nT∏\\nt=1\\nπθ (at |st) Pr( st+1 |st,at)\\n= ∂\\n∂θ\\nT∑\\nt=1\\nlog πθ (at|st)\\ued19 \\ued18\\ued17 \\ued1a\\npolicy\\n+ ∂\\n∂θ\\nT∑\\nt=1\\nlog Pr( st+1 |st,at)\\ued19 \\ued18\\ued17 \\ued1a\\ndynamics\\n(4.22)\\nwhere the gradient is decomposed into two parts: the policy g radient and the dynamics gradient.\\nThe policy component, log πθ (at|st), determines the log-probability of taking action at given\\nstate st, and it is parameterized by θ. The dynamics component, log Pr( st+1|st,at), represents\\nthe log-probability of transitioning to state st+1 from state st after taking action at. In typical\\nreinforcement learning settings, the dynamics are not dire ctly inﬂuenced by the policy parameters\\nθ, and thus, their derivatives are often zero. In this case, th erefore, Eq. ( 4.22) can be simpliﬁed to:\\n∂log Pr θ (τ)\\n∂θ = ∂\\n∂θ\\nT∑\\nt=1\\nlog πθ (at|st) (4.23)\\nIn other words, we only concentrate on optimizing the policy without concerning ourselves with\\nthe underlying dynamics.\\nSubstituting Eq. ( 4.23) into Eq. ( 4.21), and expanding R(τ), we then obtain\\n∂J(θ)\\n∂θ = 1\\n|D|\\n∑\\nτ ∈D\\n∂\\n∂θ\\n( T∑\\nt=1\\nlog πθ (at|st)\\nT∑\\nt=1\\nrt\\n)\\n(4.24)\\nWhile this policy gradient approach is straightforward, it suffers from the problem that the\\nvariance of the estimated gradients can be very high, making the learning process noisy and inef-\\nﬁcient. One reason for this high variance problem is that rew ards can vary greatly across different\\nsteps or scenarios. Imagine that in a sequence of action deci sions, the reward model tends to assign\\nsmall rewards to good actions (e.g., Rt = 2 ) and large penalties to poor actions (e.g., Rt = −50).\\nSuch varying reward scales for good and poor actions can resu lt in a very low total reward for the\\nentire sequence, even if it includes good actions.\\nOne simple method for reducing the variance of the gradient i s to set a baseline band subtract\\nit from ∑ T\\nt=1 rt, resulting in ∑ T\\nt=1 rt − b.\\n2 Here, the baseline can be interpreted as a reference\\npoint. By centering the rewards around this baseline, we rem ove systematic biases in the reward\\nsignal, making the updates more stable and less sensitive to extreme ﬂuctuations in individual\\nrewards.\\n2 In fact, the use of a baseline bdoes not change the variance of the total rewards\\n∑ T\\nt=1 rt. However, it is important\\nto note that while introducing a baseline does not alter the o verall variance of the rewards, it helps reduce the variance\\nof the gradient estimates. This is because subtracting the b aseline from the total rewards effectively reduces ﬂuctuat ions\\naround their mean, which makes the gradient estimates more s table. In general, the operation\\n∑ T\\nt=1 rt − bcenters the\\nrewards around zero (e.g., bis deﬁned as the expected value of\\n∑ T\\nt=1 rt), which can lead to reduced variance in the\\nproduct\\n∑ T\\nt=1 log πθ (at|st )(\\n∑ T\\nt=1 rt − b).'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 184, 'page_label': '178'}, page_content='178 Alignment\\nThis policy gradient model with a baseline can be given by\\n∂J(θ)\\n∂θ = 1\\n|D|\\n∑\\nτ ∈D\\n∂\\n∂θ\\n( T∑\\nt=1\\nlog πθ (at|st)\\n)( T∑\\nt=1\\nrt − b\\n)\\n= 1\\n|D|\\n∑\\nτ ∈D\\n∂\\n∂θ\\n[ T∑\\nt=1\\nlog πθ (at |st)\\n( T∑\\nk=1\\nrk − b\\n)]\\n= 1\\n|D|\\n∑\\nτ ∈D\\n∂\\n∂θ\\n[ T∑\\nt=1\\nlog πθ (at |st)\\n( t−1∑\\nk=1\\nrk +\\nT∑\\nk=t\\nrk − b\\n)]\\n(4.25)\\nHere we write ∑ T\\nk=1 rk as the sum of two terms ∑ t−1\\nk=1 rk and ∑ T\\nk=t rk to distinguish between the\\nrewards accrued before and after the action at time step t. Note that in Markov decision processes,\\nthe future is independent of the past given the present. Ther efore, the action taken at time step t\\ncannot inﬂuence the rewards received before t, or in other words, the rewards prior to tare already\\n“ﬁxed” by the time the action at tis chosen. The term ∑ t−1\\nk=1 rk does not contribute to the gradient\\nand can be omitted, leading to a simpliﬁed version of Eq. (\\n4.25)\\n∂J(θ)\\n∂θ = 1\\n|D|\\n∑\\nτ ∈D\\n∂\\n∂θ\\n[ T∑\\nt=1\\nlog πθ (at|st)\\n( T∑\\nk=t\\nrk − b\\n)]\\n(4.26)\\nAlso note that removing ∑ T\\nk=t rk can further reduce the variance of the gradient.\\nThere are many ways to deﬁne the baseline b. Here we consider the value function of the state\\nst, that is, the estimated value of being in state st: V(st) = E(rt + rt+1 + · · · + rT ). Hence we\\nhave\\nA(st,at) =\\nT∑\\nk=t\\nrk − b\\n=\\nT∑\\nk=t\\nrk − V(st) (4.27)\\nwhere ∑ T\\nk=t rk represents the actual return received, and V(st) represents the expected return.\\nA(st,at) (or At for short) is called the advantage at time step t, which quantiﬁes the relative\\nbeneﬁt of the action at compared to the expected value of following the policy from t he state st\\nonward.\\nBy using the advantage function A(st,at ), the gradient of J(θ) can be written in the form\\n∂J(θ)\\n∂θ = 1\\n|D|\\n∑\\nτ ∈D\\n∂\\n∂θ\\n( T∑\\nt=1\\nlog πθ (at|st)A(st,at )\\n)\\n(4.28)\\nThis optimization objective corresponds to the advantage actor-critic (A2C) method in re-\\ninforcement learning [ Mnih et al. , 2016]. In this method, the actor aims at learning a policy . It\\nupdates the policy parameters using Eq. ( 4.28) to help focus more on actions that are likely to\\nimprove performance. The critic, on the other hand, updates its estimation of the value function,\\nwhich is used to calculate the advantage function A(st,at), thus serving as the evaluator of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 185, 'page_label': '179'}, page_content='4.3 Human Preference Alignment: RLHF 179\\npolicy being learned by the actor.\\nIn the A2C method, A(st,at) is typically expressed as the difference of the action-valu e func-\\ntion Q(st,at) and the state-value function V(st)\\nA(st,at) = Q(st,at) − V(st) (4.29)\\nAt ﬁrst glance, this model may seem challenging to develop be cause it requires two separate sub-\\nmodels to calculate Q(st,at) and V(st) respectively . Fortunately , considering that Q(st,at) can\\nbe deﬁned as the return rt + V(st+1 ), we can rewrite Eq. ( 4.29) as\\nA(st,at) = rt + V(st+1 ) − V(st) (4.30)\\nor alternatively , introduce the discount factor γto obtain a more general form\\nA(st,at) = rt + γV(st+1 ) − V(st) (4.31)\\nA(st,at) = rt + γV(st+1 ) − V(st) is also called the temporal difference (TD) error. What\\nwe need is to train a critic network for the value function V(st), and then use it to compute the\\nadvantage function 3 .\\nUp to this point, we have spent considerable space discussin g the basics of reinforcement\\nlearning, especially on how to derive the optimization obje ctive for the A2C method. However,\\nreinforcement learning is a vast ﬁeld, and many technical de tails cannot be covered here. The in-\\nterested reader can refer to reinforcement learning books f or more details [ Sutton and Barto , 2018;\\nSzepesvári, 2010]. Nevertheless, we now have the necessary knowledge to furt her discuss RLHF .\\nIn the subsequent subsections, we will return to the discuss ion on LLM alignment, demonstrating\\nhow to use the A2C method for aligning with human preferences .\\n4.3.2 T raining Reward Models\\nW e have shown that reward models play a very important role inthe general reinforcement learn-\\ning framework and form the basis for computing value functio ns. W e now consider the problem\\nof training these reward models.\\nIn RLHF , a reward model is a neural network that maps a pair of i nput and output token\\nsequences to a scalar. Given an input x and an output y, the reward can be expressed as\\nr = Reward( x,y) (4.33)\\nwhere Reward(·) is the reward model. rcan be interpreted as a measure of how well the output y\\naligns with the desired behavior given the input x. As discussed in the previous subsection, both x\\n3 The training loss for the value network (or critic network) i n A2C is generally formulated as the mean squared\\nerror between the computed return rt + γV (st+1 ) and the predicted state value V(st). Suppose that the value network\\nis parameterized by ω . The loss function is given by\\nLv (ω ) = 1\\nM\\n∑ (\\nrt + γVω (st+1) − Vω (st)\\n) 2\\n(4.32)\\nwhere M is the number of training samples, for example, for a sequenc e of T tokens, we can set M = T.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 186, 'page_label': '180'}, page_content='180 Alignment\\nx0 x1 x2 · · · xm y1 y2 · · · yn\\n(Last T oken ⟨ EOS⟩ )\\nhx0 hx1 hx2 · · · hxm hy1 hy2 · · · hlast\\nTransformer Decoder (LLM)\\nRepresentation\\nat Each Position\\nReward (Scalar)\\nWr Linear Map\\nFig. 4.8: Architecture of the reward model based on Transformer. The m ain component of this model is still an LLM.\\nW e use the Transformer decoder as the sequence representati on model. W e extract the representation of the last position\\nof the decoder as the representation of the entire sequence [x, y]. W e then map this representation to a scalar through a\\nlinear transformation, which serves as the reward score for [x, y].\\nand y are assumed to complete texts. This means that the reward mod el evaluates the relationship\\nbetween inputs and outputs that provide full semantic conte nt. For example, when applying the\\nreward model, it assigns a value of 0 (or another predetermin ed value) at each position tin the\\noutput sequence y = y1...yn. Only at the ﬁnal position, when t = n, does the reward model\\ngenerate the actual reward score. T o keep the notation unclu ttered, we will use r(x,y) to denote\\nthe reward model Reward(x,y) from here on.\\nThere are many ways to implement the reward model. One simple approach is to build the\\nreward model based on a pre-trained LLM. More speciﬁcally , w e can concatenate x and y to form\\na single token sequence seqx, y = [ x,y]. W e run a pre-trained LLM on this sequence, as usual,\\nand at each position, we obtain a representation from the top -most Transformer layer. Then, we\\ntake the representation at the last position (denoted by hlast ) and map it to a scalar via linear\\ntransformation:\\nr(x,y) = hlastWr (4.34)\\nwhere hlast is a d-dimensional vector, and Wris a d× 1 linear mapping matrix. This architecture\\nof the reward model is illustrated in Figure 4.8.\\nT o train the reward model, the ﬁrst step is to collect human fe edback on a set of generated\\noutputs. Given an input x, we use the LLM to produce multiple candidate outputs {y1 ,...,yN }.\\nHuman feedback can be obtained in several ways:\\n• Pairwise Comparison (Pairwise Ranking ). Given two different outputs, human experts\\nselect which one is better.\\n• Rating. Human experts provide a score or rating to each output. This score is often a\\ncontinuous or discrete numerical value, such as a score on a s cale (e.g., 1-5 stars, or 1-10\\npoints). In some cases, the rating might be binary , indicati ng a “yes/no” or “positive/nega-\\ntive” preference.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 187, 'page_label': '181'}, page_content='4.3 Human Preference Alignment: RLHF 181\\n• Listwise Ranking . Human experts are asked to rank or order the given set of poss ible\\noutputs.\\nHere we consider pairwise comparison feedback as it is one of the simplest and most com-\\nmon forms of human feedback used in RLHF . In this setting, eac h time, two outputs (ya ,yb) are\\nrandonly drawn from the candidate pool {y1 ,...,yN }. Human experts are then presented with\\nthese pairs and asked to decide which output they prefer base d on speciﬁc criteria, such as clarity ,\\nrelevance, and accuracy . The human feedback can be encoded a s a binary label, ya ≻ yb for a\\npreference for ya, and yb ≻ ya for a preference for yb.\\nOne simple and widely used model for describing such pairwis e comparisons is the Bradley-\\nT erry model [Bradley and T erry , 1952]. It is a probabilistic model that estimates the probabilit y\\nthat one item is preferred over another. Adapting this model to the notation used here, we can\\nwrite the probability that ya is preferred over yb in the form\\nPr(ya ≻ yb |x) = er(x, ya)\\ner(x, ya ) + er(x, yb )\\n= er(x, ya )−r(x, yb )\\ner(x, ya )−r(x, yb ) + 1\\n= Sigmoid( r(x,ya) − r(x,yb)) (4.35)\\nWhen training the reward model, we want to maximize this pref erence probability . A loss\\nfunction based on the Bradley-T erry model is given by\\nLr (φ) = −E(x, ya, yb )∼Dr\\n[\\nlog Pr φ (ya ≻ yb |x)\\n]\\n(4.36)\\nwhere (x,ya,yb ) is drawn from a human-annotated dataset Dr consisting of preference pairs of\\noutputs and their corresponding inputs. φrepresents the parameters of the reward model, which\\nincludes both the parameters of the Transformer decoder and the linear mapping matrix Wr . In\\npractice, assuming (x,ya,yb ) is uniformly sampled from Dr , we can replace the expectation with\\na summation\\nLr (φ) = − 1\\n|Dr |\\n∑\\n(x, ya, yb )∈Dr\\nlog Pr φ (ya ≻ yb|x) (4.37)\\nThe goal of training the reward model is to ﬁnd the optimal par ameters ˆφthat minimize this\\nloss function, given by\\nˆφ = arg min\\nφ\\nLr (φ) (4.38)\\nSince the reward model itself is also an LLM, we can directly r euse the Transformer training\\nprocedure to optimize the reward model. The difference from training a standard LLM is that we\\nonly need to replace the cross-entropy loss with the pairwis e comparison loss as described in Eq.\\n(4.37). After the training of the reward model, we can apply the tra ined reward model rˆφ (·) to\\nsupervise the target LLM for alignment.\\nIt is worth noting that although we train the reward model to p erform pairwise ranking, we\\napply it to score each input-output pair independently duri ng the alignment process. The pairwise'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 188, 'page_label': '182'}, page_content='182 Alignment\\nranking objective ensures that the reward model is sensitiv e to subtle differences between outputs,\\nbut we rely on the continuous scores produced by the reward mo del to guide the optimization of\\nthe LLM. An advantage of this approach is that we can choose fr om or combine various ranking\\nloss functions, and still apply the resulting reward models in the same way as we have done in this\\nsubsection. This consistency ensures a uniﬁed framework fo r aligning the LLM, regardless of the\\nspeciﬁc ranking loss used during reward model training.\\n4.3.3 T raining LLMs\\nHaving obtained the reward model, we then train the policy (i.e., the LLM) via the A2C method.\\nRecall from Section\\n4.3.1 that a state-action sequence or trajectory τcan be evaluated by the utility\\nfunction\\nU(τ; θ) =\\nT∑\\nt=1\\nlog πθ (at|st)A(st,at) (4.39)\\nwhere A(st,at) is the advantage of taking the action at given the state st. An estimate of A(st,at)\\nis deﬁned as the TD error rt + γV(st+1 ) − V(st), where the value function V(st) is trained with\\nthe reward model.\\nGiven this utility function, the A2C-based loss function ca n be written in the form\\nL(θ) = −Eτ ∼D\\n[\\nU(τ; θ)\\n]\\n= −Eτ ∼D\\n[ T∑\\nt=1\\nlog πθ (at|st)A(st,at)\\n]\\n(4.40)\\nwhere D is a space of state-action sequences. As usual, the goal of tr aining the policy is to\\nminimize this loss function\\n˜θ = arg min\\nθ\\nL(θ) (4.41)\\nIf we map the problem back to the language modeling problem an d adopt the notation from\\nLLMs, the loss function can be written as:\\nL(θ) = −E(x, y)∼D\\n[\\nU(x,y; θ)\\n]\\n(4.42)\\nwhere\\nU(x,y; θ) =\\nT∑\\nt=1\\nlog πθ (yt|x,y<t)A(x,y<t ,yt) (4.43)\\nHere πθ (yt|x,y<t) = Pr θ (yt|x,y<t) is the LLM parameterized by θ.\\nIn general, we do not have a human annotated input-output dat aset D in RLHF , but a dataset\\ncontaining inputs only . The outputs, in this case, are typic ally the predictions made by the LLM.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 189, 'page_label': '183'}, page_content='4.3 Human Preference Alignment: RLHF 183\\nThe loss function is then deﬁned as\\nL(θ) = −Ex∼DEy∼π θ (·|x)\\n[\\nU(x,y; θ)\\n]\\n(4.44)\\nwhere D denotes the input-only dataset, and y ∼ πθ (·|x) denotes that the output y is sampled by\\nthe policy πθ (·|x).\\nThe above formulation provides a basic form of the A2C method for LLMs. Improved versions\\nof this model are more commonly used in RLHF . In the following discussion, we will still use\\nthe reinforcement learning notation to simplify the presen tation and will get back the language\\nmodeling notation later.\\nOne common improvement of policy gradient methods is to use importance sampling to\\nreﬁne the estimation of U(τ; θ). This can be written as\\nU(τ; θ) =\\nT∑\\nt=1\\nπθ (at|st)\\nπθ ref (at|st) A(st,at) (4.45)\\nHere we replace the log-probability log πθ (at|st) with the ratio π θ (at |st)\\nπ θ ref (at |st) . θref denotes the pa-\\nrameters of the previous policy (such as an initial model fro m which we start the training). So\\nπ θ (at |st)\\nπ θ ref (at |st) , also called the ratio function , can be interpreted as the log-probability ratio between\\nthe current policy πθ and the previous policy πθ ref (call it the reference policy). By using the\\nratio function we reweight the observed rewards based on the likelihood of the actions under the\\ncurrent policy versus the reference policy . When π θ (at |st )\\nπ θ ref (at |st) >1, the action at is more favored by\\nthe current policy compared to the reference policy . By cont rast, when π θ (at |st)\\nπ θ ref (at |st) <1, the action\\nat is less favored by the current policy 4 .\\n4 Consider a more general case where we wish to evaluate the pol icy using its expected reward (also see Eq. ( 4.18))\\nJ(θ) = Eτ ∼π θ\\n[\\nR(τ)\\n]\\n(4.46)\\nHere τ ∼ πθ means that the sequence τ is generated by the policy πθ . Alternatively , we can write J(θ) in another form\\nJ(θ) = Eτ ∼π θ ref\\n[ Prθ (τ)\\nPrθ ref (τ) R(τ)\\n]\\n(4.47)\\nIt is not difﬁcult to ﬁnd that the right-hand sides of these eq uations are essentially the same since\\nEτ ∼π θ ref\\n[\\nPrθ (τ )\\nPrθ ref (τ ) R(τ)\\n]\\n=\\n∑\\nτ Prθ ref (τ) Prθ (τ )\\nPrθ ref (τ ) R(τ) =\\n∑\\nτ Prθ (τ)R(τ) = Eτ ∼π θ\\n[\\nR(τ)\\n]\\nNote that this equivalence holds only when the expectation i s performed over the entire sequence space. In practice,\\nhowever, we often only sample a relatively small number of se quences using a policy in policy learning. As a result,\\nthe sampling method itself matters. Eq. ( 4.47) offers an interesting manner to separate the sampling and r eward\\ncomputation processes: we ﬁrst use a baseline policy (with θref ) to sample a number of sequences, and then use the\\ntarget policy (with θ) to compute the expected reward. In this way , we separate the policy used for collecting the data,\\nand the policy used for computing the gradient. This approac h avoids the need to directly sample from the policy we are\\nevaluating, which can be beneﬁcial in cases where generatin g sequences from the target policy is expensive or difﬁcult.\\nIn reinforcement learning, Eτ ∼π θ ref\\n[\\nPrθ (τ )\\nPrθ ref (τ ) R(τ)\\n]\\nis often called a surrogate objective .\\nEq. ( 4.47) can also be interpreted from a policy gradient perspective . For Eτ ∼π θ ref\\n[\\nPrθ (τ )\\nPrθ ref (τ ) R(τ)\\n]\\n, the gradient at\\nθ = θref is given by\\n∂\\n∂θ Eτ ∼π θ ref\\n[ Prθ (τ)\\nPrθ ref (τ) R(τ)\\n] ⏐\\n⏐\\n⏐\\nθ =θ ref\\n= Eτ ∼π θ ref\\n[ ∂Prθ (τ)|θ =θ ref\\n∂θ R(τ)\\n]\\n(4.48)\\nThe right-hand side is a standard form used in policy gradien t methods, meaning that we compute the direction of'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 190, 'page_label': '184'}, page_content='184 Alignment\\nA problem with the model presented in Eq. ( 4.47) (as well as in Eq. ( 4.39)) is that the\\nvariance in the gradient estimates is often high, making the learning process unstable. T o mitigate\\nthis issue, techniques such as clipping are often employed t o bound the importance weights and\\nprevent large updates. A clipped version of the utility func tion (also called the clipped surrogate\\nobjective function) is given by\\nUclip(τ; θ) =\\nT∑\\nt=1\\nClip\\n( πθ (at|st)\\nπθ ref (at |st)\\n)\\nA(st,at) (4.49)\\nClip\\n( πθ (at|st)\\nπθ ref (at|st)\\n)\\n= min\\n( πθ (at|st)\\nπθ ref (at|st) ,bound\\n( πθ (at|st)\\nπθ ref (at|st) ,1 − ǫ,1 + ǫ\\n) )\\n(4.50)\\nHere the function bound( π θ (at |st)\\nπ θ ref (at |st) ,1 − ǫ,1 + ǫ) constrains the ratio function to the range [1 −\\nǫ,1 + ǫ].\\nA further improvement to the above model is to consider trust regions in optimization [ Schulman et al. ,\\n2015]. In reinforcement learning, a large update to the policy ca n lead to instability , where the\\nagent may start performing worse after an update. A reasonab le idea is to optimize the model in\\nthe trust region, which refers to a region around the current parameter estimate where the model\\nis well-behaved. One approach to incorporating trust regio ns is to impose a constraint on the size\\nof the policy update, ensuring that the current policy does n ot deviate too signiﬁcantly from the\\nreference policy . This can be achieved by adding a penalty ba sed on some form of divergence\\nbetween the current and reference policies to the objective function. A simple form of such a\\npenalty is given by the difference in the log-probability of the sequence τ under the current policy\\nversus the reference policy:\\nPanalty = log πθ (τ) − log πθ ref (τ) (4.51)\\nIn practice, this penalty can be approximated by considerin g only the policy probabilities and\\nignoring the dynamics. This gives\\nPenalty =\\nT∑\\nt=1\\nlog πθ (at|st) −\\nT∑\\nt=1\\nlog πθ ref (at|st) (4.52)\\nBy including this penalty in the optimization objective, we encourage the current policy to remain\\nclose to the reference policy , limiting very large updates t hat could destabilize learning.\\nW e can incorporate this penalty into the clipped surrogate o bjective function, and obtain\\nUppo-clip(τ; θ) = Uclip(τ; θ) − βPenalty (4.53)\\nwhere βis the weight of the penalty . This training method is called proximal policy optimization\\n(PPO), which is one of the most popular reinforcement learning me thods used in LLMs and many\\nother ﬁelds [ Schulman et al. , 2017].\\nNow we can write the objective of training LLMs in the form of P PO.\\nthe parameter update at the point θ = θref on the optimization surface.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 191, 'page_label': '185'}, page_content='4.3 Human Preference Alignment: RLHF 185\\nU(x,y; θ) = Uppo-clip(x,y; θ) − βPenalty (4.54)\\nwhere\\nUppo-clip(x,y; θ) =\\nT∑\\nt=1\\nClip\\n( πθ (yt|x,y<t)\\nπθ ref (yt|x,y<t)\\n)\\nA(x,y<t,yt) (4.55)\\nPenalty = log Pr θ (y|x) − log Pr θ ref (y|x)\\n=\\nT∑\\nt=1\\nlog Pr θ (yt|x,y<t) −\\nT∑\\nt=1\\nlog Pr θ ref (yt|x,y<t) (4.56)\\nAlthough the notation here appears a bit tedious, the idea of PPO is simple: we develop an\\nobjective by combining the clipped likelihood ratio of the t arget and reference policies with an\\nadvantage function, and then impose a penalty that ensures p olicy updates are not too large. The\\nPPO-based RLHF is illustrated in Figure 4.9.\\nT o summarize, implementing RLHF requires building four mod els, all based on the Trans-\\nformer decoder architecture.\\n• Reward Model (rφ (·) where φ denotes the parameters). The reward model learns from\\nhuman preference data to predict the reward for each pair of i nput and output token se-\\nquences. It is a Transformer decoder followed by a linear lay er that maps a sequence (the\\nconcatenation of the input and output) to a real-valued rewa rd score.\\n• V alue Model or V alue Function (Vω (·) where ωdenotes the parameters). The value func-\\ntion receives reward scores from the reward model and is trai ned to predict the expected\\nsum of rewards that can be obtained starting from a state. It i s generally based on the same\\narchitecture as the reward model.\\n• Reference Model (πθ ref (·) = Pr θ ref (·) where θref denotes the parameters). The reference\\nmodel is the baseline LLM that serves as a starting point for p olicy training. In RLHF , it\\nrepresents the previous version of the model or a model train ed without human feedback. It\\nis used to perform sampling over the space of outputs and cont ribute to the loss computation\\nfor policy training.\\n• T arget Model or Policy (πθ (·) = Pr θ (·) where θ denotes the parameters). This policy\\ngoverns how the LLM decides the most appropriate next token g iven its context. It is trained\\nunder the supervision of both the reward model and the value m odel.\\nIn practice, these models need to be trained in a certain orde r. First, we need to initialize them\\nusing some other models. For example, the reward model and th e value model can be initialized\\nwith a pre-trained LLM, while the reference model and the tar get model can be initialized with a\\nmodel that has been instruction ﬁne-tuned. Note that, at thi s point, the reference model is ready for\\nuse and will not be further updated. Second, we need to collec t human preference data and train the\\nreward model on this data. Third, both the value model and the policy are trained simultaneously\\nusing the reward model. At each position in an output token se quence, we update the value model'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 192, 'page_label': '186'}, page_content='186 Alignment\\nReward Model\\nTraining\\nPolicy Training\\nReward Model\\nrφ (x,y)\\nT o Learn\\nLLM Policy\\nPrθ (yt|x,y<t )\\nT o Learn\\nV alue Function\\nVω (x,y<t )\\nT o Learn\\nRef Model\\nPrθ old (yt|x,y<t )\\nFixed\\nMinimizing the loss based on\\nthe Bradley-T erry model\\nmin\\nφ\\n− 1\\n|Dr |\\n∑\\n(x, ya , yb )∈Dr\\nlog σ (rφ (x, ya) − rφ (x, yb ))\\nHuman preference data Dr = {(x,ya ,yb )}\\nInput-only data D = {x}\\nx0 x1 · · · xm y1 · · · yt−1\\nState (x, y<t )\\nx1 x2 · · · y1 y2 · · · yt Action yt\\n(sampled with Prθ old )\\nEvaluate the state-action pair using the advantage\\nfunction or the TD error (based on the reward\\nmodel and the value function)\\nLLM Policy\\nLLM Policy\\nMinimizing the clipped PPO loss\\nwith the penalty\\nmin\\nθ\\n−\\n∑\\nx∈D, y∼Prθ old (·|x)\\n∑ T\\nt=1\\n[\\nClip\\n( Prθ (yt |x, y<t )\\nPrθ old (yt |x, y<t )\\n)\\nAt−\\nβ ·\\n(\\nlog Pr θ (yt |x, y<t )−\\nlog Pr θ old (yt |x, y<t )\\n) ]\\nV alue Function\\nMinimizing the MSE between the\\ncomputed return and the predicted\\nstate value\\nmin\\nω\\n1\\nM\\n∑\\nx∈D\\n∑ T\\nt=1\\n[\\nrt + γV ω (x, y<t+1 ) − Vω (x, y<t )\\n] 2\\n∗∗ rt = r(x, y<t+1 ) denotes the reward received as step t.\\n∗∗ At denotes the advantage at step t, and can be deﬁned as rt + γV ω (x, y<t+1 ) − Vω (x, y<t )\\nFig. 4.9: Illustration of RLHF . The ﬁrst step is to collect human prefe rence data and train the reward model using this\\ndata. Once the reward model is optimized, along with the refe rence model, we proceed to train both the policy and\\nthe value function. At each prediction step, we compute the s um of the PPO-based loss and update the parameters of\\nthe policy . This requires access to the reward model, the ref erence model, and the value function at hand. At the same\\ntime, we update the parameters of the value function by minim izing the MSE loss.\\nby minimizing the MSE error of value predition, and the polic y is updated by minimizing the PPO\\nloss.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 193, 'page_label': '187'}, page_content='4.4 Improved Human Preference Alignment 187\\n4.4 Improved Human Preference Alignment\\nIn the previous section, we reviewed the basic concepts of reinforcement learning and the general\\nframework of RLHF . In this section, we will discuss some reﬁn ements of RLHF and alternative\\nmethods to achieve human preference alignment.\\n4.4.1 Better Reward Modeling\\nIn Section\\n4.3.2, we highlighted the task of learning from human preferences as well as the use\\nof pairwise ranking loss for training reward models. Here we consider more methods for reward\\nmodeling. Our discussion will be relatively general, and si nce the reward model is widely used in\\nmany reinforcement learning problems, it will be easy for us to apply the methods discussed here\\nto RLHF and related applications.\\n4.4.1.1 Supervision Signals\\nThe training of reward models can broadly be seen as a rankingproblem, where the model learns\\nto assign scores to outputs so that their order reﬂects the pr eferences indicated by humans. There\\nare several methods to train a reward model from the perspect ive of ranking.\\nOne approach is to extend pairwise ranking to listwise ranki ng. For each sample in a dataset,\\nwe can use the LLM to generate multiple outputs, and ask human experts to order these outputs.\\nFor example, given a set of four outputs {y1 ,y2 ,y3 ,y4 }, one possible order of them can be\\ny2 ≻ y3 ≻ y1 ≻ y4 . A very simple method to model the ordering of the list is to ac cumulate the\\npairwise comparison loss. For example, we can deﬁne the list wise loss by accumulating the loss\\nover all pairs of outputs:\\nLlist = −E(x,Y )∼Dr\\n[ 1\\nN(N − 1)\\n∑\\nya∈Y, yb ∈Y\\nya̸=yb\\nlog Pr( ya ≻ yb |x)\\n]\\n(4.57)\\nwhere Y is a list of outputs, and N is the number of outputs in the list. Pr(ya ≻ yb |x) can be\\ndeﬁned using the Bradley-T erry model, that is, Pr(ya ≻ yb|x) = Sigmoid( r(x,ya) − r(x,yb )).\\nHere we omit the φsuperscript on the Pr(·) to keep the notation uncluttered.\\nAn extension to the Bradley-T erry model for listwise rankin g could involve a ranking mecha-\\nnism that takes into account the entire list of outputs rathe r than just pairwise comparisons. One\\nsuch model is the Plackett-Luce model , which generalizes the Bradley-T erry model to handle\\nmultiple items in a ranking [ Plackett, 1975]. In the Plackett-Luce model, for each item in a list,\\nwe deﬁne a “worth” for this item that reﬂects its relative str ength of being chosen over other items.\\nFor the reward modeling problem here, the worth of y in the list Y can be deﬁned as\\nα(y) = exp( r(x,y)) (4.58)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 194, 'page_label': '188'}, page_content='188 Alignment\\nThen the probability of selecting y from Y is given by\\nPr(y is selected |x,Y ) = α(y)∑\\ny′ ∈Y α(y′ )\\n= exp(r(x,y))∑\\ny′ ∈Y exp(r(x,y′ )) (4.59)\\nSuppose ˚Y is an ordered list yj1 ≻ yj2 ≻ · · · ≻ yjN . The overall log-probability of this\\nordered list can be deﬁned as the sum of the conditional log-p robabilities at each stage of selection,\\ngiven by\\nlog Pr( ˚Y|x) = log Pr( yj1 ≻ yj2 ≻ · · · ≻ yjN |x)\\n= log Pr( yj1 |x,{yj1 ,yj2 ,...,yjN }) +\\nlog Pr( yj2 |x,{yj2 ,...,yjN }) +\\n· · · +\\nlog Pr( yjN |x,{yjN })\\n=\\nN∑\\nk=1\\nlog Pr( yjk |x,˚Y≥k ) (4.60)\\nwhere ˚Y≥k represents the subset of the list of outputs that remain unse lected at the k-th stage, i.e.,\\n˚Y≥k = {yjk ,...,yjN }. Given the log-probability log Pr( ˚Y|x), we can deﬁne the loss function\\nbased on the Plackett-Luce model by\\nLpl = −E(x, ˚Y )∼Dr\\n[\\nlog Pr( ˚Y|x)\\n]\\n(4.61)\\nThere are also many other pairwise and listwise methods for m odeling rankings, such as\\nRankNet [ Burges et al. , 2005] and ListNet [ Cao et al. , 2007]. All these methods can be cate-\\ngorized into a large family of learning-to-rank approaches , and most of them are applicable to the\\nproblem of modeling human preferences. However, discussin g these methods is beyond the scope\\nof this chapter. Interested readers can refer to books on thi s topic for more details [ Liu, 2009; Li,\\n2011].\\nIn addition to pairwise and listwise ranking, using pointwi se methods to train reward models\\noffers an alternative way to capture human preferences. Unl ike methods that focus on the relative\\nrankings between different outputs, pointwise methods tre at each output independently . For ex-\\nample, human experts might assign a score to an individual ou tput, such as a rating on a ﬁve-point\\nscale. The objective is to adjust the reward model so that its outputs align with these scores. A\\nsimple way to achieve pointwise training is through regress ion techniques where the reward of\\neach output is treated as a target variable. Let ϕ(x,y) be the score assigned to y given x by\\nhumans. Pointwise reward models can be trained by minimizin g a loss function, often based on\\nmean squared error or other regression losses, between the p redicted reward r(x,y) and the actual\\nhuman feedback ϕ(x,y). For example, the loss function could be\\nLpoint = −E\\n[\\nϕ(x,y) − r(x,y)\\n] 2 (4.62)\\nWhile pointwise methods are conceptually simpler and can di rectly guide the reward model to'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 195, 'page_label': '189'}, page_content='4.4 Improved Human Preference Alignment 189\\npredict scores, they might not always be the best choice in RL HF . A problem is that these methods\\nmay struggle with high variance in human feedback, especial ly when different experts provide\\ninconsistent scores for similar outputs. Because they focu s on ﬁtting to absolute scores rather than\\nrelative differences, inconsistencies in scoring can lead to poor model performance. Moreover,\\nﬁtting to speciﬁc scored outputs might discourage generali zation, particularly given that training\\ndata is often very limited in RLHF . In contrast, methods that consider relative preferences can\\npromote the learning of more generalized patterns of succes s and failure. Nevertheless, there are\\nscenarios where pointwise methods might still be suitable. For example, in tasks where training\\ndata is abundant and the costs of obtaining accurate, consis tent annotations are low , pointwise\\nmethods can prove effective.\\nIn fact, to make the supervision signal for training the rewa rd model more robust, we can also\\nintroduce additional regularization terms into training. For example, if we consider the ﬁrst term\\nUppo-clip(x,y; θ) in Eq. ( 4.54) as a type of generalized reward, then the second term (i.e., the\\npenalty term) can be viewed as a form of regularization for th e reward model, except that here the\\ngoal is to train the policy rather than the reward model. Anot her example is that Eisenstein et al.\\n[2023] develop a regularization term based on the squared sum of re wards, and add it to the\\npairwise comparison loss in RLHF:\\nLreg = Lpair + ( −E(x, ya , yb )∼Dr\\n[\\nr(x,ya) + r(x,yb )\\n] 2)\\n= −E(x, ya, yb )∼Dr\\n[\\nlog Pr φ (ya ≻ yb|x)\\n]\\n−E(x, ya, yb )∼Dr\\n[\\nr(x,ya) + r(x,yb )\\n] 2 (4.63)\\nOptimizing with this regularization term can help mitigate the underdetermination of reward mod-\\nels5 .\\n4.4.1.2 Sparse Rewards vs. Dense Rewards\\nAs discussed in Section\\n4.3, the rewards in RLHF are very sparse: they are observed only a t the\\nend of sequences, rather than continuously throughout the g eneration process. Dealing with sparse\\nrewards has long been a concern in reinforcement learning, a nd has been one of the challenges in\\nmany practical applications. For example, in robotics, it o ften needs to shape the reward function\\nto ease optimization rather than relying solely on end-of-s equence rewards. V arious methods\\nhave been developed to address this issue. One common approa ch is reward shaping, where the\\noriginal function is modiﬁed to include intermediate rewar ds, thereby providing more immediate\\nfeedback. Also, one can adopt curriculum learning to sequen tially structure tasks in a way that the\\ncomplexity gradually increases. This can help models to mas ter simpler tasks ﬁrst, which prepares\\nthem for more complex challenges as their skills develop. Th ere are many such methods that can\\nmitigate the impact of sparse rewards, such as Monte Carlo me thods and intrinsic motivation. Most\\nof these methods are general and the discussion of them can be found in the broader literature on\\nreinforcement learning, such as Sutton and Barto [2018]’s book.\\nAlthough we do not discuss methods for mitigating sparse rew ards in detail here, an interesting\\nquestion arises: why are sparse rewards so successful in RLH F? Recall from Section 4.3.1 that\\nthe supervision signal received at each time step t is not the reward for the current action, but\\n5 A model is called underdetermined if there are multiple alte rnative sets of parameters that can achieve the same\\nobjective.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 196, 'page_label': '190'}, page_content='190 Alignment\\nrather some form of the accumulated rewards from t until the last time step. Such supervision\\nsignals are dense over the sequence, because the reward obta ined at the end of the sequence can\\nbe transferred back to that time step, regardless of which ti me step it is. In other words, the sparse\\nrewards are transformed into the dense supervision signals . Furthermore, from the perspective of\\nreward shaping, Ng et al. [1999] show that the reward at tcan be deﬁned as\\nr′(st,at,st+1 ) = r(st,at,st+1 ) + f(st,at,st+1 ) (4.64)\\nwhere r′(·) is the transformed reward function, r(·) is the original reward function, and f(·) is\\nthe shaping reward function. T o ensure the optimality of the policy under the transformed reward\\nfunction, the shaping reward function can be given in the for m\\nf(st,at,st+1 ) = γΦ( st+1 ) − Φ( st) (4.65)\\nwhere Φ( s) is called the potential value of the state s. If we deﬁne Φ( s) as the common value\\nfunction as in Eq. ( 4.15) and substitute Eq. ( 4.65) into Eq. ( 4.64), we obtain\\nr′(st,at,st+1 ) = r(st,at,st+1) + γV(st+1 ) − V(st) (4.66)\\nIt is interesting to see that this function is exactly the sam e as the advantage function used in PPO.\\nThis relates advantage-based methods to reward shaping: th e advantage is essentially a shaped\\nreward.\\nOn the other hand, one of the reasons for adopting end-of-seq uence rewards lies in the nature\\nof the RLHF tasks. Unlike traditional reinforcement learni ng environments where the agent in-\\nteracts with a dynamic environment, RLHF tasks often involv e complex decision-making based\\non linguistic or other high-level cognitive processes. The se processes do not lend themselves eas-\\nily to frequent and meaningful intermediate rewards becaus e the quality and appropriateness of\\nthe actions can only be fully evaluated after observing thei r impact in the larger context of the\\nentire sequence or task. In this case, the reward signals bas ed on human feedback, though very\\nsparse, are typically very informative and accurate. Conse quently , this sparsity , together with the\\nhigh informativeness and accuracy of the human feedback, ca n make the learning both robust and\\nefﬁcient.\\n4.4.1.3 Fine-grained Rewards\\nFor many applications, our objective will be more complex than merely evaluating an entire text.\\nFor example, in sentiment analysis, we often do not just dete rmine the sentiment of a text, but need\\nto analyze the sentiment in more detail by associating it wit h speciﬁc aspects of a topic discussed\\nin the text. Consider the sentence \"The camera of the phone is excellent, but the battery life is\\ndisappointing.\" In this example, we would need to separatel y analyze the sentiments expressed\\nabout the camera and the battery . Such analysis, known as asp ect-based sentiment analysis, helps\\nprovide a ﬁner-grained understanding of the customer revie w compared to general sentiment anal-\\nysis.\\nFor the problem of reward modeling, we often need to model dif ferent parts of a sequence as\\nwell. A simple and straightforward way to do this is to divide a sequence into different segments\\nand then compute the reward for each segment [\\nWu et al. , 2023]. Suppose that an output token'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 197, 'page_label': '191'}, page_content='4.4 Improved Human Preference Alignment 191\\nsequence y can be divided into ns segments {¯y1 ,..., ¯yns } by some criterion. W e can use the\\nreward model to evaluate each of these segments. By taking x, y and ¯yk as input to the reward\\nmodel, the reward score for the k-th segment is given by\\nrk = r(x,y,¯yk ) (4.67)\\nThen the reward score for the entire output sequence is given by\\nr(x,y) =\\nns∑\\nk=1\\nr(x,y,¯yk ) (4.68)\\nHere r(x,y) can be used to train the policy as usual.\\nA problem with this model is that training reward models at th e segment level is not as straight-\\nforward as learning from human preferences on entire texts, as it is difﬁcult to obtain segment-level\\nhuman preference data. For rating-like problems (e.g., we r ate a segment according to its level\\nof misinformation), one simple approach is to assign a ratin g score to each segment and train the\\nreward model using pointwise methods. For example, we can us e a strong LLM to rate the se-\\nquences ¯y1...¯yk−1 and ¯y1 ...¯yk , and obtain the scores s(¯y1 ...¯yk−1 ) and s(¯y1 ...¯yk ). W e can then\\ndeﬁne the score of the segment ¯yk as the difference between s(¯y1...¯yk ) and s(¯y1 ...¯yk−1)\\ns(¯yk ) = s(¯y1...¯yk ) − s(¯y1 ...¯yk−1 ) (4.69)\\nUsing these segment-level scores, we can train the reward mo del with a regression loss func-\\ntion\\nLrating = −E¯yk\\n[\\ns(¯yk ) − r(x,y,¯yk )\\n] 2 (4.70)\\nSometimes, alignment can be treated as a classiﬁcation prob lem, for example, we assess\\nwhether a segment has ethical issues. In this case, the segme nt can be labeled as ethical or\\nunethical, either by humans or using additional classiﬁers . Given the label of the segment, we\\ncan train the reward model using some classiﬁcation loss fun ction. For example, suppose that\\nr(x,y,¯yk ) = 1 if the segment is classiﬁed as unethical, and r(x,y,¯yk ) = −1 otherwise6 . The\\nhinge loss of training binary classiﬁcation models is given by\\nLhinge = max(0 ,1 − r(x,y,¯yk ) · ˆr) (4.71)\\nwhere ˆr∈ { 1,−1} denotes the ground truth label.\\nThe remaining issue here is how to split y into segments. One approach is to deﬁne a ﬁxed-\\nlength segmentation, where y is divided into equal-length chunks. However, this may not a lways\\nbe ideal, as the content of the sequence may not align well wit h ﬁxed boundaries. An alternative\\napproach is to segment y based on speciﬁc linguistic or semantic cues, such as senten ce bound-\\naries, topic shifts, or other meaningful structures in the t ext. Such a segmentation can be achieved\\nby using linguistic segmentation systems or prompting LLMs to identify natural breaks in the se-\\nquence. Another approach is to use dynamic segmentation met hods based on the complexity of\\n6 T o allow the reward model to output categories, we can replac e the linear layer described in Section 4.3.2 with a\\nSoftmax layer.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 198, 'page_label': '192'}, page_content='192 Alignment\\nthe sequence. For example, segments could be deﬁned where th ere is a signiﬁcant change in the\\nreward score, which might correspond to shifts in the task be ing modeled.\\n4.4.1.4 Combination of Reward Models\\nA reward model can be viewed as a proxy for the environment. Since the true environment is often\\ntoo complex or unknown, developing a perfect proxy for the en vironment is generally not possible.\\nAs a result, over-aligning LLMs with this imperfect proxy mi ght lead to decreased performance,\\nknown as the overoptimization problem [\\nStiennon et al. , 2020; Gao et al. , 2023a]7 . W e can also\\nexplain this through Goodhart’s law , which states: when a measure becomes a target, it ceases to\\nbe a good measure [Goodhart, 1984].\\nAddressing the overoptimization problem is not easy , and th ere is no mature solution yet. The\\nideal approach might be to develop an oracle reward model tha t can perfectly capture the true\\nobjectives of the task and prevent the agent from “tricking” . However, creating such a model is\\nextremely difﬁcult due to the complexity of the real-world e nvironment, as well as the challenge\\nof deﬁning all the relevant factors that contribute to the de sired outcome. Instead, a more practical\\napproach is to combine multiple reward models, thereby alle viating the misalignment between\\nthe training objective and the true objective that arises fr om using a single, speciﬁc reward model\\n[Coste et al. , 2024].\\nGiven a set of reward models, combining them is straightforw ard, and in some cases, we can\\nsimply treat this problem as an ensemble learning problem. A simple yet common approach is to\\naverage the outputs of these models to obtain a more precise r eward estimation:\\nrcombine = 1\\nN\\nK∑\\nk=1\\nwk · rk (x,y) (4.72)\\nwhere rk (·) is the k-th reward model in the ensemble, wk is the weight of rk (·), and K is the\\nnumber of reward models. This combined reward can then be use d to supervise the training of\\na policy . In fact, there are many ways to combine different mo dels, for example, one can make\\npredictions using Bayesian model averaging or develop a fus ion network to learn to combine the\\npredictions from different models. Alternatively , one can frame this task as a multi-objective\\noptimization problem, and use multiple reward models to tra in the policy simultaneously . These\\nmethods have been intensively discussed in the literature o n optimization and machine learning\\n[Miettinen, 1999; Bishop, 2006].\\nIn addition to model combination methods, another importan t issue is how to collect or con-\\nstruct multiple different reward models. One of the simples t approaches is to employ ensemble\\nlearning techniques, such as developing diverse reward mod els from different subsets of a given\\ndataset or from various data sources. For RLHF , it is also pos sible to construct reward models\\nbased on considerations of different aspects of alignment. For example, we can develop a reward\\nmodel to evaluate the factual accuracy of the output and anot her reward model to evaluate the\\n7 This problem is also called reward hacking or reward gaming [Krakovna et al. , 2020; Skalse et al. , 2022;\\nPan et al. , 2022], which refers to the phenomenon where the agent attempts to trick the reward model but fails to\\nalign its actions with the true intended objectives of the ta sk. Imagine a student who is assigned homework and is re-\\nwarded with points or praise for completing it. The student m ight then ﬁnd ways to ﬁnish the homework with minimal\\neffort to maximize the reward, such as copying and pasting so lutions from the internet or previous assignments, rather\\nthan solving the problems themselves.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 199, 'page_label': '193'}, page_content='4.4 Improved Human Preference Alignment 193\\nya ≻ yb\\nPreference\\nData\\nReward Model\\nV alue Function\\nPolicy\\ntraining with MLE Training\\nwith PPO\\n(a) RLHF (PPO)\\nya ≻ yb\\nPreference\\nData\\nPolicy\\ntraining with MLE\\n(b) DPO\\nFig. 4.10: Standard RLHF (PPO) vs. DPO. In RLHF , the human preference da ta is used to train a reward model, which\\nis then employed in training the policy as well as the value fu nction. In DPO, the use of human preference data is more\\ndirect, and the policy is trained on this data without the nee d for reward model training.\\ncompleteness of the output. These two models are complement ary to each other, and can be com-\\nbined to improve the overall evaluation of the output. Anoth er approach is to employ different\\noff-the-shelf LLMs as reward models. This approach is simpl e and practical, as there have been\\na lot of well-developed LLMs and we just need to use them with n o or little modiﬁcation. An\\ninteresting issue, though not closely related to the discus sion here, arises: can an LLM that aligns\\nwith other LLMs outperform those LLMs? Probably not at ﬁrst g lance. In part, this is because\\nthe target LLM merely imitates other LLMs based on limited su pervision and thus cannot capture\\nwell the nuances of the behaviors of these supervisors. Howe ver, given the strong generalization\\nability of LLMs, this approach can, in fact, be quite beneﬁci al. For example, using open-sourced\\nor commercial LLMs as reward models has demonstrated strong performance in aligning LLMs,\\neven achieving state-of-the-art results on several popula r tasks [ Lambert et al. , 2024].\\n4.4.2 Direct Preference Optimization\\nAlthough learning reward models is a standard step in reinforcement learning, it makes the entire\\ntraining process much more complex than supervised trainin g. Training a reliable reward model\\nis itself not an easy task, and a poorly trained reward model c an greatly affect the outcome of\\npolicy learning. W e now consider an alternative alignment m ethod, called direct preference op-\\ntimization (DPO), which simpliﬁes the training framework by eliminating th e need to explicitly\\nmodel rewards [\\nRafailov et al. , 2024]. This method directly optimizes the policy based on user\\npreferences, rather than developing a separate reward mode l. As a result, we can achieve human\\npreference alignment in a supervised learning-like fashio n. Figure 4.10 shows a comparison of\\nthe standard RLHF method and the DPO method.\\nBefore deriving the DPO objective, let us ﬁrst review the obj ective of policy training used in\\nRLHF . As discussed in Section 4.3.3, the policy is typically trained by optimizing a loss functi on'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 200, 'page_label': '194'}, page_content='194 Alignment\\nwith a penalty term. The DPO method assumes a simple loss func tion where the quality of the\\noutput y given the input x is evaluated by the reward model r(x,y). The training objective is thus\\ngiven by\\n˜θ = arg min\\nθ\\nEx∼D Ey∼π θ (·|x)\\n[\\n−r(x,y)\\ued19 \\ued18\\ued17 \\ued1a\\nloss\\n+β(log πθ (y|x) − log πθ ref (y|x))\\ued19 \\ued18\\ued17 \\ued1a\\npenalty\\n]\\n(4.73)\\nNote that in this optimization problem, only the term πθ (y|x) depends on the target policy πθ (·).\\nBoth the reward model r(x,y) and the reference model πθ ref (y|x) are assumed to be ﬁxed given\\nx and y. This is a strong assumption compared with PPO, but as will be shown later, it simpliﬁes\\nthe problem and crucial for deriving the DPO objective.\\nSince θis the variable we want to optimize, we rearrange the right-h and side of Eq. ( 4.73) to\\nisolate πθ (y|x) as an independent term:\\n˜θ = arg min\\nθ\\nEx∼D Ey∼π θ (·|x)\\n[\\nβlog πθ (y|x) − βlog πθ ref (y|x) − r(x,y)\\n]\\n= arg min\\nθ\\nEx∼D Ey∼π θ (·|x)\\n[\\nlog πθ (y|x) −\\n(\\nlog πθ ref (y|x) + 1\\nβr(x,y)\\n)]\\n= arg min\\nθ\\nEx∼D Ey∼π θ (·|x)\\n[\\nlog πθ (y|x)\\ued19 \\ued18\\ued17 \\ued1a\\ndependent on θ\\n− log πθ ref (y|x) exp\\n( 1\\nβr(x,y)\\n)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nnot dependent on θ\\n]\\n(4.74)\\nThis equation deﬁnes the objective function as the differen ce between the log-probability dis-\\ntribution function of y and another function of y. This form of the objective function seems not\\n“ideal”, as we usually prefer to see the difference between t wo distributions, so that we can in-\\nterpret this difference as some kind of divergence between t he distributions. A simple idea is\\nto convert the second term (i.e., log πθ ref (y|x) exp( 1\\nβ r(x,y))) into a log-probability distribution\\nover the domain of y. If we treat πθ ref (y|x) exp( 1\\nβ r(x,y)) as an unnormalized probability of y,\\nwe can convert it into a normalized probability by dividing i t by a normalization factor:\\nZ(x) =\\n∑\\ny\\nπθ ref (y|x) exp\\n( 1\\nβr(x,y)\\n)\\n(4.75)\\nHence we can deﬁne a probability distribution by\\nπ∗ (y|x) =\\nπθ ref (y|x) exp\\n( 1\\nβ r(x,y)\\n)\\nZ(x) (4.76)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 201, 'page_label': '195'}, page_content='4.4 Improved Human Preference Alignment 195\\nW e then rewrite Eq. ( 4.74) as\\n˜θ = arg min\\nθ\\nEx∼D Ey∼π θ (·|x)\\n[\\nlog πθ (y|x) − log\\nπθ ref (y|x) exp\\n( 1\\nβ r(x,y)\\n)\\nZ(x\\n)\\n− log Z(x)\\n]\\n= arg min\\nθ\\nEx∼D Ey∼π θ (·|x)\\n[\\nlog πθ (y|x) − log π∗(y|x) − log Z(x)\\n]\\n= arg min\\nθ\\nEx∼D\\n[\\nEy∼π θ (·|x)\\n[\\nlog πθ (y|x) − log π∗ (y|x)\\n]\\n−Ey∼π θ (·|x)\\n[\\nlog Z(x)\\n] ]\\n= arg min\\nθ\\nEx∼D\\n[\\nKL\\n(\\nπθ (·|x) || π∗(·|x)\\n)\\n\\ued19 \\ued18\\ued17 \\ued1a\\nKL divergence\\n− log Z(x)\\ued19 \\ued18\\ued17 \\ued1a\\nconstant wrt. θ\\n]\\n(4.77)\\nSince log Z(x) is independent of θ, it does not affect the result of the arg min θ operation,\\nand can be removed from the objective. Now we obtain a new trai ning objective which ﬁnds the\\noptimal policy πθ by minimizing the KL divergence between πθ (·|x) and π∗ (·|x)\\n˜θ = arg min\\nθ\\nEx∼D\\n[\\nKL\\n(\\nπθ (·|x) || π∗ (·|x)\\n) ]\\n(4.78)\\nClearly , the solution to this optimization problem is given by\\nπθ (y|x) = π∗(y|x)\\n=\\nπθ ref (y|x) exp\\n( 1\\nβ r(x,y))\\nZ(x\\n) (4.79)\\nGiven this equation, we can express the reward r(x,y) using the target model πθ (y|x), the\\nreference model πθ ref (y|x), and the normalization factor Z(x):\\nr(x,y) = β\\n(\\nlog πθ (y|x)\\nπθ ref (y|x) + log Z(x)\\n)\\n(4.80)\\nThis is interesting because we initially seek to learn the po licy πθ (·) using the reward model\\nr(x,y), but eventually obtain a representation of the reward model based on the policy . Given the\\nreward model deﬁned in Eq. ( 4.80), we can apply it to the Bradley-T erry model to calculate the\\npreference probability (also see Section 4.3.2):\\nPrθ (ya ≻ yb |x) = Sigmoid( r(x,ya) − r(x,yb))\\n= Sigmoid\\n(\\nβ\\n(\\nlog πθ (ya|x)\\nπθ ref (ya |x) + log Z(x)\\n)\\n−\\nβ\\n(\\nlog πθ (yb |x)\\nπθ ref (yb |x) + log Z(x)\\n) )\\n= Sigmoid\\n(\\nβlog πθ (ya|x)\\nπθ ref (ya|x) − βlog πθ (yb |x)\\nπθ ref (yb |x)\\n)\\n(4.81)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 202, 'page_label': '196'}, page_content='196 Alignment\\nThis formula is elegant because it converts the difference i n rewards into the difference in\\nratio functions, and we do not need to calculate the value of Z(x). A direct result is that we no\\nlonger need a reward model, but only need the target policy an d reference model to calculate the\\nprobability of preferences. Finally , we can train the targe t policy by minimizing the following\\nDPO loss function\\nLdpo (θ) = −E(x, ya, yb )∼Dr\\n[\\nlog Pr θ (ya ≻ yb |x)\\n]\\n(4.82)\\nThe form of this loss function is very similar to that used in t raining reward models in RLHF (see\\nEq. ( 4.36)). But it should be noted that the loss function here depends on the parameters of the\\npolicy (i.e., θ) rather than the parameters of the reward model (i.e., φ).\\nThe main advantage of DPO lies in its simplicity and efﬁcienc y . The DPO objective is very\\nstraightforward — it directly optimizes for preference-ba sed feedback, rather than relying on sep-\\narately developed reward models. Moreover, DPO is generall y more sample-efﬁcient, as it learns\\nfrom a ﬁxed dataset without the need for the computationally expensive sampling process used\\nin PPO. This makes DPO a popular method for human preference a lignment, especially when\\ndeveloping and applying reward models via reinforcement le arning is challenging.\\nDPO can broadly be viewed as an ofﬂine reinforcement learning method, where the training\\ndata is pre-collected and ﬁxed, and there is no exploration. In contrast, online reinforcement learn-\\ning methods like PPO, which require exploring new states thr ough interaction with the environ-\\nment (using the reward model as a proxy), also have their uniq ue advantages. One of the beneﬁts\\nof online reinforcement learning is that it allows the agent to continuously adapt to changes in\\nthe environment by learning from real-time feedback. This m eans that, unlike ofﬂine methods,\\nonline methods are not constrained by the static nature of pr e-collected data and can discover\\nnew problem-solving strategies. In addition, exploration can help the agent cover a wider range of\\nstate-action pairs, thus improving generalization. This c ould be an important advantage for LLMs,\\nas generalization is considered a critical aspect in applyi ng such large models.\\n4.4.3 Automatic Preference Data Generation\\nAlthough learning from human preferences is an effective and popular method for aligning LLMs,\\nannotating preference data is costly . Using human feedback does not only faces the problem of\\nlimited scalability , but it may also introduce bias because human feedback is inherently subjective.\\nAs a result, one can turn to AI feedback methods to address the se scalability and consistency issues\\nwithout the limitations associated with human annotators.\\nAs with data generation for instruction ﬁne-tuning, genera ting preference data using LLMs is\\nstraightforward. Given a set of inputs, we ﬁrst use an LLM to g enerate pairs of outputs. Then, we\\nprompt the LLM to label the preference between each pair of ou tputs, along with its corresponding\\ninput. Below is an example of prompting the LLM to generate a p reference label for a pair of\\nconsumer service responses.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 203, 'page_label': '197'}, page_content='4.4 Improved Human Preference Alignment 197\\nConsider a customer service scenario where a customer poses a request. Y ou\\nwill review two responses to this request. Please indicate w hich response is\\npreferred. Note that a good response should be courteous, cl ear, and concise. It\\nshould address the customer’s concern directly , provide he lpful information or a\\nsolution, and maintain a positive tone.\\nRequest:\\nHello, I noticed that my order hasn’t arrived yet, though it was scheduled to\\narrive several days ago. Could you please update me on its sta tus? Thank you!\\nResponse A:\\nI’m very sorry for the delay and understand how disappointing this can be. W e’re\\ndoing our best to sort this out quickly for you.\\nResponse B:\\nHey, stuff happens! Y our package will get there when it gets there, no need to\\nstress.\\nResponse A is preferred.\\nOnce we collect such preference labels, we can use them, alon g with the output pair and input,\\nto train the reward model. Of course, we can consider demonst rating a few examples or using\\nadvanced prompting techniques, such as CoT , to improve labe ling performance. For example, we\\ncan include in the prompt an example showing how and why one of the two responses is preferred\\nbased on a CoT rationale.\\nIn addition to preference labels, we can also obtain the prob ability associated with each label\\n[Lee et al. , 2023]. A simple method is to extract the probabilities for the lab el tokens, such as “ A ”\\nand “B”, from the probabilities output by the LLM. W e can then use the Softmax function or other\\nnormalization techniques to re-normalize these probabili ties into a distribution over the labels.\\nThese probabilities of preferred labels can serve as pointw ise supervision signals for training the\\nreward model, as discussed in Section 4.4.1.\\nFor data generation, although it is easy to scale up, it is oft en necessary to ensure the data is\\naccurate and diverse. Here, the data quality and diversity i ssues involve not only the labeling of\\npreferences but also the inputs and outputs of the model. The refore, we often need to use a variety\\nof techniques to obtain large-scale, high-quality data. Fo r example, one can generate diverse\\nmodel outputs and annotations by using different LLMs, prom pts, in-context demonstrations, and\\nso on [ Cui et al. , 2024]. Dubois et al. [2024] report that the variability in pairwise preference data\\nis important for training LLMs from either human or AI feedba ck.\\nWhile learning from AI feedback is highly scalable and gener ally objective, this method is\\nmore suited to well-deﬁned tasks where objective performan ce metrics are available. By contrast,\\nlearning from human feedback is more advantageous when alig ning AI systems with human val-\\nues, preferences, and complex real-world tasks that requir e understanding of subtle or subjective\\ncontext. These methods can be combined to train LLMs that ben eﬁt from both human insights\\nand the scalability of AI feedback.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 204, 'page_label': '198'}, page_content='198 Alignment\\n4.4.4 Step-by-step Alignment\\nSo far, our discussion of alignment has primarily focused onthe use of reward models for evalu-\\nating entire input-output sequence pairs. These methods ca n be easily adapted to scenarios where\\nthe correctness of an output can be examined by checking whet her the desired result is included.\\nFor example, in the task of calculating a mathematical expre ssion, a reward model can provide\\npositive feedback if the answer is correct, and negative fee dback if the answer is wrong. How-\\never, in many problems that require complex reasoning, simp ly examining the correctness of the\\nﬁnal result is insufﬁcient for learning. Imagine a student w ho is only given the ﬁnal answer to\\na challenging math problem. Knowing whether the ﬁnal answer is right or wrong does not help\\nthe student ﬁgure out where they went wrong and how to calcula te the correct answer. A better\\napproach would be to guide the student with a step-by-step br eakdown of the problem-solving\\nprocess and encourage understanding of the underlying conc epts and logic behind these steps.\\nIn Chapter 3, we studied CoT methods to prompt LLMs to explici tly write out intermediate\\nsteps or the reasoning process needed to reach a conclusion o r solve a problem. W e saw that\\nbreaking down a problem into smaller parts could make it easi er to understand the solution path\\nand increase the accuracy of the output. These methods can be naturally extended to the alignment\\nof LLMs, that is, we supervise the model during the intermedi ate steps of reasoning. Consider a\\nreasoning task where an LLM produces a sequence of reasoning steps y = {¯y1 ,..., ¯yns } for the\\ngiven input. The result of the reasoning is assumed to be incl uded in the last step ¯yns , and can\\nbe easily veriﬁed. For this reasoning problem,\\nUesato et al. [2022] categorize LLM ﬁne-tuning\\napproaches into two classes:\\n• Outcome-based Approaches . Supervision occurs only when the end result is veriﬁed. Thi s\\nis a standard method for learning from human feedback we have discussed in this chapter.\\nFor example, the LLM is optimized to maximize some form of the reward r(x,y).\\n• Process-based Approaches . Supervision is involved in all intermediate steps in addit ion to\\nthe last step. T o do this, we need to develop a model to give a su pervision signal at each\\nstep, and develop loss functions that can make use of such sup ervision signals.\\nFigure 4.11 shows two LLM outputs for an example math problem. Although t he LLM gives\\nthe correct ﬁnal answer in both cases, it makes mistakes duri ng the problem-solving process in the\\nsecond output. Outcome-based approaches overlook these mi stakes and give positive feedback for\\nthe entire solution. By contrast, process-based approache s can take these mistakes into account\\nand provide additional guidance on the detailed reasoning s teps.\\nAn important issue for process-based approaches is that we n eed to get step-level feedback\\nduring a (potentially) long reasoning path. W e can collect o r generate reasoning paths correspond-\\ning to problems from existing datasets. Human experts then a nnotate each step in these paths for\\ncorrectness. These annotations can be used to directly trai n LLMS or as rewards in reward mod-\\neling. However, in practice, richer annotations are often i ntroduced [ Lightman et al. , 2024]. In\\naddition to the correct and incorrect labels, a step can also be labeled as neutral to indicate that\\nwhile the step may be technically correct, it might still be p roblematic within the overall reason-\\ning process. Furthermore, to improve the efﬁciency of data a nnotation, techniques such as active\\nlearning can be employed. Identifying obvious errors usual ly does not signiﬁcantly contribute to'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 205, 'page_label': '199'}, page_content='4.4 Improved Human Preference Alignment 199\\nQuestion (from The International Junior Math Olympiad):\\nJessica has a lot of apps on her phone. The number of apps she ha s is divisible by 5. If she\\ndownloaded 10 more apps, she would still have less than 50 app s. What is the maximum\\noriginal number of apps Jessica could have?\\nResponse 1 :\\nxmust be a multiple of 5, i.e.,\\nx≡ 0 (mod 5) . x\\n¯y1 :\\nAfter downloading 10 apps, the total\\ntotal number of apps should still be less\\nthan50: x+ 10 <50. Solving this\\ninequality: x< 40.\\nx\\n¯y2 :\\nSince xmust be a multiple of 5 and\\nless than 40, the largest value of xis 35.x\\n¯y3 :\\nV erify the result. If x= 35 , then after\\ndownloading 10 apps, Jessica would\\nhave:35 + 10 = 45 which satisfy\\n45 <50.\\nx\\n¯y4 :\\nThe maximum original number of\\napps is35\\n. x\\n¯y5 :\\ncorrect\\nResponse 2 :\\nThe number of apps is divisible by 5,\\nso xmust be a multiple of 5. x\\n¯y1 :\\nIf she downloads 10 more apps, her\\nnew total will bex+ 10 , and this must\\nbe less than 50. Therefore, x+ 10 <50.\\nSimplifying this gives x< 40.\\nx\\n¯y2 :\\nxmust be less than or equal to\\nthe largest multiple of 5, which is 40. x\\n¯y3 :\\nBut x+ 10 should not be more\\nthan or equal to 50. So we need to\\nsubtract5 from 40.\\nx\\n¯y4 :\\nTherefore, the ﬁnal result is 35. x¯y5 :\\ncorrect\\nproblematic\\nproblematic\\nFig. 4.11: T wo LLM responses to a math problem. In response 1, both the ﬁn al result and all the reasoning steps are\\ncorrect. In response 2, the ﬁnal result is correct, but there are mistakes in the reasoning process (highlighted in red).\\nFor outcome-based approaches, both responses are consider ed correct. For process-based approaches, the mistakes in\\nresponse 2 can be considered in reward modeling.\\nlearning from reasoning mistakes. Instead, annotating steps that the model conﬁdently considers\\ncorrect but are actually problematic is often more effectiv e.\\nGiven a set of step-level annotated reasoning paths and corr esponding inputs, we can train\\na reward model to provide feedback for supervising policy le arning. The reward model can be\\ntreated as a classiﬁcation model, and so its architecture ca n be a Transformer decoder with a\\nSoftmax layer stacked on top. At step k, the reward model takes both the problem description\\n(denoted by x) and the reasoning steps generated so far (denoted by ¯y≤k ) as input and outputs\\na probability distribution over the label set {correct,incorrect} or {correct,incorrect,neutral}.\\nThen the learned reward model is used to evaluate reasoning p aths by assessing the correctness of\\neach step. A simple method to model correctness is to count th e number of steps that are classiﬁed\\nas correct, given by\\nr(x,y) =\\nns∑\\nk=1\\nδ(correct,C(x,¯y≤k )) (4.83)\\nwhere C(x,¯y≤k ) denotes the label with the maximum probability . W e can also u se log-probabilities'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 206, 'page_label': '200'}, page_content='200 Alignment\\nof classiﬁcation to deﬁne the reward of the entire path\\nr(x,y) =\\nns∑\\nk=1\\nlog Pr( correct|x,¯y≤k ) (4.84)\\nwhere Pr(correct|x,¯y≤k ) denotes the probability of the correct label generated by the reward\\nmodel. The reward score r(x,y) can then be used to train the policy in RLHF as usual.\\nWhile we restrict our discussion to math problems, the appro aches described here are general\\nand can be applied to a wide variety of tasks that involve mult i-step reasoning and decision-\\nmaking. Moreover, we can consider various aspects when asse ssing the quality of a step, rather\\nthan just its correctness. For example, in dialogue systems , responses must not only be accurate\\nbut also contextually appropriate across multiple turns of conversation. If a model provides a\\ncorrect response but fails to maintain coherence in the cont ext of the ongoing dialogue, step-\\nlevel feedback could help the model identify and correct suc h discrepancies. Also note that the\\nprocess-based approaches are related to the ﬁne-grained re ward modeling approaches discussed\\nin Section 4.4.1.3. All these approaches essentially aim to provide more detai led supervision to\\nLLMs by breaking their outputs into smaller, more manageabl e steps. However, process-based\\nfeedback focuses more on evaluating the correctness of a ste p based on its preceding steps, while\\nthe approaches in Section 4.4.1.3 emphasize evaluating each step independently .\\nThe idea of aligning LLMs step by step has great application p otential, especially considering\\nthe recent shift towards more complex reasoning tasks in the use of LLMs. For example, both\\nthe GPT -o1 and GPT -o3 models are designed with more advanced reasoning techniques (such\\nas long internal CoT) to solve challenging problems like sci entiﬁc and mathematical reasoning\\n[OpenAI, 2024]. These tasks often rely on long and complex reasoning paths , and therefore, it\\nseems essential to introduce detailed supervision signals in the reasoning process. Moreover, from\\na practical perspective, effective supervision on long rea soning paths not only improves reasoning\\nperformance, but it also helps the model eliminate redundan t or unnecessary reasoning steps,\\nthereby reducing reasoning complexity and improving efﬁci ency .\\n4.4.5 Inference-time Alignment\\nIn this section we explored a variety of methods to align models with human preferences and an-\\nnotations. However, one of the signiﬁcant limitations of ma ny such methods is that LLMs must\\nbe ﬁne-tuned. For RLHF and its variants, training LLMs with r eward models can be computa-\\ntionally expensive and unstable, leading to increased comp lexity and costs when applying these\\napproaches. In this case, we can consider aligning models at inference time, thus avoiding the\\nadditional complexity and effort involved.\\nOne simple way to achieve inference-time alignment is to use the reward model to select\\nthe best one from N alternative outputs generated by the LLM, a method known as Best-of-N\\nsampling (BoN sampling ). W e can consider BoN sampling as a form of reranking. In fact ,\\nreranking methods have been widely used in NLP tasks, such as machine translation, for a long\\ntime. They are typically applied in situations where traini ng complex models is costly . In such\\ncases, directly reranking the outputs allows for the incorp oration of these complex models at a\\nvery low cost\\n8 .\\n8 Reranking methods can also help us explore what are known as m odel errors and search errors, although these'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 207, 'page_label': '201'}, page_content='4.5 Summary 201\\nIn the BoN sampling process, the LLM takes the input sequence x and generates N different\\noutput sequences {ˆy1 ,..., ˆyN }:\\n{ˆy1 ,..., ˆyN } = argT opN\\ny\\n[Pr(y|x)] (4.85)\\nwhere the argT opN operation returns the top- N outputs that maximize the function Pr(y|x).\\nThese outputs can be generated in a variety of ways, dependin g on the search algorithm used by\\nthe model (e.g., sampling or beam search). Once the N-best output candidates are generated, the\\nreward model is used to evaluate and select the best one:\\nˆybest = max {r(x,ˆy1 ),...,r (x,ˆyN )} (4.86)\\nIt is worth noting that the result of BoN sampling is also inﬂu enced by the diversity of the\\nN-best list. This is a common issue with most reranking method s. T ypically , we wish the N-best\\noutput candidates to have relatively high quality but be suf ﬁciently different from each other. In\\nmany text generation systems, the N-best outputs are very similar, often differing by just one\\nor two words. The diversity issue is even more challenging in LLMs, as the N-best outputs\\ngenerated by an LLM can be different in their wordings, yet th eir semantic meanings are often\\nquite similar. In practice, one can adjust the model hyperpa rameters and/or adopt different LLMs\\nto generate more diverse output candidates for reranking. N evertheless, as with many practical\\nsystems, we need to make a trade-off between selecting high- quality candidates and ensuring\\nsufﬁcient variation in the generated outputs.\\nBoN sampling can be used for training LLMs as well. A closely r elated method is rejection\\nsampling. In this method, we ﬁrst select the “best” outputs from the N-best lists via the reward\\nmodel, and then take these selected outputs to ﬁne-tune the L LM. In this way , we can introduce\\nhuman preferences into the training of LLMs via a much simple r approach compared to RLHF .\\nMany LLMs have adopted rejection sampling for ﬁne-tuning [ Nakano et al. , 2021; T ouvron et al. ,\\n2023b].\\n4.5 Summary\\nIn this chapter, we have explored a range of techniques for aligning LLMs. In particular, we\\nhave discussed ﬁne-tuning methods that enable LLMs to follo w instructions and align them with\\nhuman preferences. One of the beneﬁts of ﬁne-tuning LLMs is c omputation efﬁciency . Unlike\\npre-training based on large-scale neural network optimiza tion, ﬁne-tuning is a post-training step\\nand so is less computationally expensive. Moreover, it is be tter suited to address problems that are\\nnot easily solved in pre-training, such as human value align ment. The widespread attention to the\\nalignment issue has also led to a surge of research papers on t his topic, which has posed challenges\\nin writing this chapter, as it is difﬁcult to cover all the lat est techniques. However, we have tried\\nto provide a relatively detailed introduction to the fundam ental approaches to alignment, such as\\nissues are not often discussed in the context of LLMs. For exa mple, suppose we have an old model and a new , more\\npowerful model. W e can use the new model to select the best out put from the N-best list of the old model as the oracle\\noutput. The performance difference between the oracle outp ut and the top-1 output of the original N-best list reﬂects\\nthe performance gain brought by the new model. If the perform ance gain is signiﬁcant, we can say that the old model\\nhas more model errors. If the gain is small, it may indicate th at the issue lies in search errors, as the best candidates\\nwere not found.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 208, 'page_label': '202'}, page_content='202 Alignment\\ninstruction ﬁne-tuning and RLHF .\\nWhile we have focused on LLM alignment techniques in this cha pter, the term AI alignment\\nis a wide-ranging concept. It generally refers to the proces s of ensuring that the behavior of an AI\\nsystem aligns with human values, goals, and expectations. T he idea of AI alignment can be traced\\nback to the early days of AI. A widely cited description of AI a lignment comes from an article by\\nthe mathematician and computer scientist Norbert Wiener [ Wiener, 1960]. The quote is as follows\\nIf we use, to achieve our purposes, a mechanical agency with w hose operation\\nwe cannot efﬁciently interfere ... we had better be quite sur e that the purpose\\nput into the machine is the purpose which we really desire.\\nAt that time, AI alignment was a distant concern for research ers. But today , it greatly inﬂu-\\nences the design of various AI systems. For example, in robot ics, alignment is critical to ensur-\\ning that autonomous robots safely interact with humans and t heir environments. In autonomous\\ndriving, cars must not only follow trafﬁc laws but also make c omplex, real-time decisions that\\nprioritize human safety , avoid accidents, and navigate eth ical dilemmas.\\nIn current AI research, alignment is usually achieved by dev eloping a surrogate objective that\\nis analogous to the real goal and steering the AI system towar ds this objective. However, designing\\nthe objective of AI alignment is very difﬁcult. One reason is that human values are diverse and\\noften context-dependent, making it difﬁcult to distill the m into a single, universally applicable\\nobjective function. Also, the complexity of real-world env ironments, where values and goals often\\nconﬂict or evolve over time, further complicates alignment efforts. Even if we could deﬁne an\\nappropriate objective, AI systems may ﬁnd unintended ways t o achieve it, leading to “misaligned”\\noutcomes that still technically satisfy the objective but i n a harmful or counterproductive way .\\nThese challenges have motivated and are motivating AI resea rch towards more aligned sys-\\ntems, either through developing new mechanisms for perceiv ing the world or more efﬁcient and\\ngeneralizable methods to adapt these systems to given tasks . More importantly , as AI systems\\nbecome more powerful and intelligent, especially given tha t recent advances in LLMs have shown\\nremarkable capabilities in dealing with many challenging p roblems, the need for AI alignment\\nhas become more urgent. Researchers have started to be conce rned with AI safety and warn the\\ncommunity that they need to develop and release AI systems wi th great caution to prevent these\\nsystems from being misaligned [ Russell, 2019; Bengio et al. , 2024].'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 209, 'page_label': '203'}, page_content='Bibliography\\n[Ainslie et al., 2020] Joshua Ainslie, Santiago Ontanon, Ch ris Alberti, V aclav Cvicek, Zachary Fisher,\\nPhilip Pham, Anirudh Ravula, Sumit Sanghai, Qifan W ang, and Li Y ang. Etc: Encoding long and\\nstructured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in\\nNatural Language Processing (EMNLP) , pages 268–284, 2020.\\n[Ainslie et al., 2023] Joshua Ainslie, James Lee-Thorp, Mic hiel de Jong, Y ury Zemlyanskiy, Federico\\nLebron, and Sumit Sanghai. Gqa: Training generalized multi -query transformer models from multi-\\nhead checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in N atural Language\\nProcessing, pages 4895–4901, 2023.\\n[Akyürek et al., 2023] Ekin Akyürek, Dale Schuurmans, Jacob Andreas, T engyu Ma, and Denny Zhou.\\nWhat learning algorithm is in-context learning? investiga tions with linear models. In Proceedings of\\nThe Eleventh International Conference on Learning Represe ntations, 2023.\\n[Alabdulmohsin et al., 2022] Ibrahim M Alabdulmohsin, Behn am Neyshabur, and Xiaohua Zhai. Revisit-\\ning neural scaling laws in language and vision. Advances in Neural Information Processing Systems , 35:\\n22300–22312, 2022.\\n[Allal et al., 2024] Loubna Ben Allal, Anton Lozhkov, and Dan iel van Strien. cosmopedia: how to create\\nlarge-scale synthetic data for pre-training. https://huggingface.co/blog/cosmopedia, 2024.\\n[Almazrouei et al., 2023] Ebtesam Almazrouei, Hamza Alobei dli, Abdulaziz Alshamsi, Alessandro Cap-\\npelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Gofﬁne t, Daniel Hesslow , Julien Launay, Quentin\\nMalartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pa nnier, and Guilherme Penedo. The falcon\\nseries of open language models. arXiv preprint arXiv:2311.16867 , 2023.\\n[Andreas et al., 2016] Jacob Andreas, Marcus Rohrbach, Trev or Darrell, and Dan Klein. Neural module\\nnetworks. In Proceedings of the IEEE conference on computer vision and pa ttern recognition , pages\\n39–48, 2016.\\n[Arjovsky et al., 2016] Martin Arjovsky, Amar Shah, and Y osh ua Bengio. Unitary evolution recurrent\\nneural networks. In International conference on machine learning , pages 1120–1128, 2016.\\n[Aschenbrenner, 2024] Leopold Aschenbrenner. Situationa l awareness: The decade ahead, 2024. URL\\nhttps://situational-awareness. ai/.\\n[Askell et al., 2021] Amanda Askell, Y untao Bai, Anna Chen, D awn Drain, Deep Ganguli, T om Henighan,\\nAndy Jones, Nicholas Joseph, Benjamin Mann, Nova DasSarma, Nelson Elhage, Zac Hatﬁeld-Dodds,\\nDanny Hernandez, Jackson Kernion, Kamal Ndousse, Catherin e Olsson, Dario Amodei, T om B. Brown,\\nJack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. A g eneral language assistant as a laboratory\\nfor alignment. arXiv preprint arXiv:2112.00861 , 2021.\\n[Bach et al., 2022] Stephen H. Bach, V ictor Sanh, Zheng Xin Y o ng, Albert W ebson, Colin Raffel, Nihal V .\\nNayak, Abheesht Sharma, T aewoon Kim, M. Saiful Bari, Thibau lt Févry, Zaid Alyafeai, Manan Dey,\\nAndrea Santilli, Zhiqing Sun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han W ang, Jason Alan\\nFries, Maged Saeed AlShaibani, Shanya Sharma, Urmish Thakk er, Khalid Almubarak, Xiangru T ang,\\nDragomir R. Radev, Mike Tian-Jian Jiang, and Alexander M. Ru sh. Promptsource: An integrated de-\\nvelopment environment and repository for natural language prompts. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics: System Demonstrations , pages 93–104, 2022.\\n[Bengio et al., 2003] Y oshua Bengio, Réjean Ducharme, Pasca l V incent, and Christian Jauvin. A neural\\nprobabilistic language model. Journal of Machine Learning Research , 3:1137–1155, 2003.\\n[Bengio et al., 2006] Y oshua Bengio, Pascal Lamblin, Dan Pop ovici, and Hugo Larochelle. Greedy layer-\\nwise training of deep networks. Advances in neural information processing systems , 19, 2006.\\n[Bengio et al., 2024] Y oshua Bengio, Geoffrey Hinton, Andre w Y ao, Dawn Song, Pieter Abbeel, Trevor\\nDarrell, Y uval Noah Harari, Y a-Qin Zhang, Lan Xue, Shai Shal ev-Shwartz, Gillian K. Hadﬁeld, Jeff\\n203'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 210, 'page_label': '204'}, page_content='204 Alignment\\nClune, T egan Maharaj, Frank Hutter, Atilim Gunes Baydin, Sh eila A. McIlraith, Qiqi Gao, Ashwin\\nAcharya, David Krueger, Anca Dragan, Philip T orr, Stuart Ru ssell, Daniel Kahneman, Jan Markus\\nBrauner, and Sören Mindermann. Managing extreme ai risks am id rapid progress. Science, 384(6698):\\n842–845, 2024.\\n[Bentivogli and Giampiccolo, 2011] Luisa Bentivogli and Da nilo Giampiccolo. Pascal recognizing textual\\nentailment challenge (rte-7) at tac 2011. https://tac.nist.gov/2011/RTE/, 2011.\\n[Besta et al., 2024] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski,\\nLukas Gianinazzi, Joanna Gajda, T omasz Lehmann, Hubert Nie wiadomski, Piotr Nyczyk, and T orsten\\nHoeﬂer. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of\\nthe AAAI Conference on Artiﬁcial Intelligence , volume 38, pages 17682–17690, 2024.\\n[Biderman et al., 2021] Stella Biderman, Sid Black, Charles Foster, Leo Gao, Eric Hallahan, Horace He,\\nBen W ang, and Phil W ang. Rotary embeddings: A relative revol ution. https://blog.eleuther.ai/\\nrotary-embeddings/, 2021.\\n[Bishop, 2006] Christopher M. Bishop. P attern Recognition and Machine Learning . Springer, 2006.\\n[Blum and Mitchell, 1998] A vrim Blum and T om Mitchell. Combi ning labeled and unlabeled data with\\nco-training. In Proceedings of the eleventh annual conference on Computati onal learning theory , pages\\n92–100, 1998.\\n[Bradley and T erry, 1952] Ralph Allan Bradley and Milton E. T erry. Rank analysis of incomplete block\\ndesigns: I. the method of paired comparisons. Biometrika, 39(3/4):324–345, 1952.\\n[Brandon et al., 2024] William Brandon, Mayank Mishra, Anir uddha Nrusimha, Rameswar Panda, and\\nJonathan Ragan Kelly. Reducing transformer key-value cach e size with cross-layer attention. arXiv\\npreprint arXiv:2405.12981 , 2024.\\n[Brill, 1992] Eric Brill. A simple rule-based part of speech tagger. In Speech and Natural Language:\\nProceedings of a W orkshop Held at Harriman, New Y ork, F ebrua ry 23-26, 1992 , 1992.\\n[Brown et al., 1993] Peter F . Brown, Stephen A. Della Pietra, V incent J. Della Pietra, and Robert L. Mercer.\\nThe mathematics of statistical machine translation: Param eter estimation. Computational Linguistics ,\\n19(2):263–311, 1993.\\n[Brown et al., 2020] T om Brown, Benjamin Mann, Nick Ryder, Me lanie Subbiah, Jared D Kaplan, Prafulla\\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry , Amanda Askell, Sandhini Agarwal, Ariel\\nHerbert-V oss, Gretchen Krueger, T om Henighan, Rewon Child , Aditya Ramesh, Daniel Ziegler, Jeffrey\\nWu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mat eusz Litwin, Scott Gray, Benjamin Chess,\\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radfo rd, Ilya Sutskever, and Dario Amodei.\\nLanguage models are few-shot learners. Advances in neural information processing systems , 33:1877–\\n1901, 2020.\\n[Bubeck et al., 2023] Sébastien Bubeck, V arun Chandrasekar an, Ronen Eldan, Johannes Gehrke, Eric\\nHorvitz, Ece Kamar, Peter Lee, Y in T at Lee, Y uanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid\\nPalangi, Marco Túlio Ribeiro, and Y i Zhang. Sparks of artiﬁc ial general intelligence: Early experiments\\nwith gpt-4. arXiv preprint arXiv:2303.12712 , 2023.\\n[Bulatov et al., 2022] A ydar Bulatov, Y ury Kuratov, and Mikh ail Burtsev. Recurrent memory transformer.\\nAdvances in Neural Information Processing Systems , 35:11079–11091, 2022.\\n[Burges et al., 2005] Chris Burges, T al Shaked, Erin Renshaw , Ari Lazier, Matt Deeds, Nicole Hamilton,\\nand Greg Hullender. Learning to rank using gradient descent . In Proceedings of the 22nd international\\nconference on Machine learning , pages 89–96, 2005.\\n[Burns et al., 2023] Collin Burns, Pavel Izmailov, Jan Hendr ik Kirchner, Bowen Baker, Leo Gao, Leopold\\nAschenbrenner, Y ining Chen, Adrien Ecoffet, Manas Jogleka r, Jan Leike, Ilya Sutskever, and Jeff Wu.\\nW eak-to-strong generalization: Eliciting strong capabil ities with weak supervision. arXiv preprint\\narXiv:2312.09390 , 2023a.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 211, 'page_label': '205'}, page_content='4.5 Summary 205\\n[Burns et al., 2023] Collin Burns, Jan Leike, Leopold Aschen brenner, Jeffrey Wu, Pavel Izmailov, Leo\\nGao, Bowen Baker, and Jan Hendrik Kirchner. W eak-to-strong generalization, 2023b. URL https://\\nhttps://openai.com/index/weak-to-strong-generalizat ion.\\n[Caballero et al., 2023] Ethan Caballero, Kshitij Gupta, Ir ina Rish, and David Krueger. Broken neural\\nscaling laws. In ICLR 2023 W orkshop on Mathematical and Empirical Understan ding of F oundation\\nModels, 2023.\\n[Cao et al., 2007] Zhe Cao, T ao Qin, Tie-Y an Liu, Ming-Feng Ts ai, and Hang Li. Learning to rank: from\\npairwise approach to listwise approach. In Proceedings of the 24th international conference on Machin e\\nlearning, pages 129–136, 2007.\\n[Chang et al., 2024] Kaiyan Chang, Songcheng Xu, Chenglong W ang, Y ingfeng Luo, T ong Xiao, and\\nJingbo Zhu. Efﬁcient prompting methods for large language m odels: A survey. arXiv preprint\\narXiv:2404.01077 , 2024.\\n[Charniak, 1997] Eugene Charniak. Statistical parsing wit h a context-free grammar and word statistics.\\nAAAI/IAAI, 2005(598-603):18, 1997.\\n[Chen et al., 2023] Banghao Chen, Zhaofeng Zhang, Nicolas La ngrené, and Shengxin Zhu. Unleashing\\nthe potential of prompt engineering in large language model s: a comprehensive review . arXiv preprint\\narXiv:2310.14735 , 2023a.\\n[Chen et al., 2023] Lichang Chen, Shiyang Li, Jun Y an, Hai W an g, Kalpa Gunaratna, V ikas Y adav, Zheng\\nT ang, V ijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxi a Jin. Alpagasus: Training a better alpaca\\nwith fewer data. arXiv preprint arXiv:2307.08701 , 2023b.\\n[Chen et al., 2024] Lichang Chen, Shiyang Li, Jun Y an, Hai W an g, Kalpa Gunaratna, V ikas Y adav, Zheng\\nT ang, V ijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxi a Jin. Alpagasus: Training a better alpaca\\nwith fewer data. In The T welfth International Conference on Learning Represen tations, 2024a.\\n[Chen et al., 2023] Shouyuan Chen, Sherman W ong, Liangjian C hen, and Y uandong Tian. Extending\\ncontext window of large language models via positional inte rpolation. arXiv preprint arXiv:2306.15595 ,\\n2023c.\\n[Chen et al., 2020] Tianlong Chen, Jonathan Frankle, Shiyu C hang, Sijia Liu, Y ang Zhang, Zhangyang\\nW ang, and Michael Carbin. The lottery ticket hypothesis for pre-trained bert networks. Advances in\\nneural information processing systems , 33:15834–15846, 2020.\\n[Chen et al., 2024] Zixiang Chen, Y ihe Deng, Huizhuo Y uan, Ka ixuan Ji, and Quanquan Gu. Self-play ﬁne-\\ntuning converts weak language models to strong language mod els. arXiv preprint arXiv:2401.01335 ,\\n2024b.\\n[Chevalier et al., 2023] Alexis Chevalier, Alexander W etti g, Anirudh Ajith, and Danqi Chen. Adapting\\nlanguage models to compress contexts. In Proceedings of the 2023 Conference on Empirical Methods\\nin Natural Language Processing , pages 3829–3846, 2023.\\n[Chi et al., 2022] T a-Chung Chi, Ting-Han Fan, Peter J Ramadg e, and Alexander Rudnicky. Kerple:\\nKernelized relative positional embedding for length extra polation. Advances in Neural Information Pro-\\ncessing Systems , 35:8386–8399, 2022.\\n[Chi et al., 2023] T a-Chung Chi, Ting-Han Fan, Alexander Rud nicky, and Peter Ramadge. Dissecting\\ntransformer length extrapolation via the lens of receptive ﬁeld analysis. In Proceedings of the 61st Annual\\nMeeting of the Association for Computational Linguistics ( V olume 1: Long P apers) , pages 13522–13537,\\n2023.\\n[Chiang et al., 2023] W ei-Lin Chiang, Zhuohan Li, Zi Lin, Y in g Sheng, Zhanghao Wu, Hao Zhang, Lian-\\nmin Zheng, Siyuan Zhuang, Y onghao Zhuang, Joseph E. Gonzale z, Ion Stoica, and Eric P . Xing. V icuna:\\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt q uality, March 2023. URL https://\\nlmsys.org/blog/2023-03-30-vicuna/ .\\n[Chowdhery et al., 2022] Aakanksha Chowdhery, Sharan Naran g, Jacob Devlin, Maarten Bosma, Gaurav'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 212, 'page_label': '206'}, page_content='206 Alignment\\nMishra, Adam Roberts, Paul Barham, Hyung W on Chung, Charles Sutton, Sebastian Gehrmann, Parker\\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abh ishek Rao, Parker Barnes, Y i T ay, Noam\\nShazeer, V inodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hu tchinson, Reiner Pope, James Bradbury,\\nJacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Y in, T o ju Duke, Anselm Levskaya, Sanjay Ghe-\\nmawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, V eda nt Misra, Kevin Robinson, Liam Fedus,\\nDenny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Bar ret Zoph, Alexander Spiridonov, Ryan\\nSepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andr ew M. Dai, Thanumalayan Sankara-\\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica More ira, Rewon Child, Oleksandr Polozov,\\nKatherine Lee, Zongwei Zhou, Xuezhi W ang, Brennan Saeta, Ma rk Diaz, Orhan Firat, Michele Catasta,\\nJason W ei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, S lav Petrov, and Noah Fiedel. Palm: Scaling\\nlanguage modeling with pathways. arXiv preprint arXiv:2204.02311 , 2022.\\n[Christiano et al., 2017] Paul F Christiano, Jan Leike, T om B rown, Miljan Martic, Shane Legg, and Dario\\nAmodei. Deep reinforcement learning from human preference s. Advances in neural information pro-\\ncessing systems , 30, 2017.\\n[Chu et al., 2023] Zheng Chu, Jingchang Chen, Qianglong Chen , W eijiang Y u, T ao He, Haotian W ang,\\nW eihua Peng, Ming Liu, Bing Qin, and Ting Liu. A survey of chai n of thought reasoning: Advances,\\nfrontiers and future. arXiv preprint arXiv:2309.15402 , 2023.\\n[Chung et al., 2022] Hyung W on Chung, Le Hou, S. Longpre, Barr et Zoph, Y i T ay, William Fedus,\\nEric Li, Xuezhi W ang, Mostafa Dehghani, Siddhartha Brahma, Albert W ebson, Shixiang Shane Gu,\\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery , Dasha V alter, Sharan Narang, Gau-\\nrav Mishra, Adams W ei Y u, V incent Zhao, Y anping Huang, Andre w M. Dai, Hongkun Y u, Slav Petrov,\\nEd Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denn y Zhou, Quoc V . Le, and Jason W ei.\\nScaling instruction-ﬁnetuned language models. arXiv preprint arXiv:2210.11416 , 2022.\\n[Clark et al., 2019] Kevin Clark, Minh-Thang Luong, Quoc V Le , and Christopher D Manning. Electra:\\nPre-training text encoders as discriminators rather than g enerators. In Proceedings of International\\nConference on Learning Representations , 2019.\\n[Cobbe et al., 2021] Karl Cobbe, V ineet Kosaraju, Mohammad B avarian, Mark Chen, Heewoo Jun, Lukasz\\nKaiser, Matthias Plappert, Jerry T worek, Jacob Hilton, Rei ichiro Nakano, Christopher Hesse, and John\\nSchulman. Training veriﬁers to solve math word problems. arXiv preprint arXiv:2110.14168 , 2021.\\n[Conneau et al., 2020] Alexis Conneau, Kartikay Khandelwal , Naman Goyal, V ishrav Chaudhary, Guil-\\nlaume W enzek, Francisco Guzmán, Édouard Grave, Myle Ott, Lu ke Zettlemoyer, and V eselin Stoyanov.\\nUnsupervised cross-lingual representation learning at sc ale. In Proceedings of the 58th Annual Meeting\\nof the Association for Computational Linguistics , pages 8440–8451, 2020.\\n[Coste et al., 2024] Thomas Coste, Usman Anwar, Robert Kirk, and David Krueger. Reward model ensem-\\nbles help mitigate overoptimization. In The T welfth International Conference on Learning Represen ta-\\ntions, 2024.\\n[Cui et al., 2024] Ganqu Cui, Lifan Y uan, Ning Ding, Guanming Y ao, Bingxiang He, W ei Zhu, Y uan\\nNi, Guotong Xie, Ruobing Xie, Y ankai Lin, Zhiyuan Liu, and Ma osong Sun. UL TRAFEEDBACK:\\nBoosting language models with scaled AI feedback. In Proceedings of the 41st International Conference\\non Machine Learning , volume 235, pages 9722–9744, 2024.\\n[Dai et al., 2023] Damai Dai, Y utao Sun, Li Dong, Y aru Hao, Shu ming Ma, Zhifang Sui, and Furu W ei.\\nWhy can gpt learn in-context? language models secretly perf orm gradient descent as meta-optimizers.\\nIn Findings of the Association for Computational Linguistics : ACL 2023 , pages 4005–4019, 2023.\\n[Dai et al., 2019] Zihang Dai, Zhilin Y ang, Y iming Y ang, Jaim e G Carbonell, Quoc Le, and Ruslan\\nSalakhutdinov. Transformer-xl: Attentive language model s beyond a ﬁxed-length context. In Proceed-\\nings of the 57th Annual Meeting of the Association for Comput ational Linguistics , pages 2978–2988,\\n2019.\\n[Dao et al., 2022] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. Flashattention: Fast\\nand memory-efﬁcient exact attention with io-awareness. Advances in Neural Information Processing'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 213, 'page_label': '207'}, page_content='4.5 Summary 207\\nSystems, 35:16344–16359, 2022.\\n[Dehghani et al., 2018] Mostafa Dehghani, Stephan Gouws, Or iol V inyals, Jakob Uszkoreit, and Łukasz\\nKaiser. Universal transformers. arXiv preprint arXiv:1807.03819 , 2018.\\n[Deletang et al., 2024] Gregoire Deletang, Anian Ruoss, Pau l-Ambroise Duquenne, Elliot Catt, Tim\\nGenewein, Christopher Mattern, Jordi Grau-Moya, Li Kevin W enliang, Matthew Aitchison, Laurent\\nOrseau, Marcus Hutter, and Joel V eness. Language modeling i s compression. In The T welfth Interna-\\ntional Conference on Learning Representations , 2024.\\n[Deng et al., 2022] Mingkai Deng, Jianyu W ang, Cheng-Ping Hs ieh, Y ihan W ang, Han Guo, Tianmin Shu,\\nMeng Song, Eric Xing, and Zhiting Hu. Rlprompt: Optimizing d iscrete text prompts with reinforcement\\nlearning. In Proceedings of the 2022 Conference on Empirical Methods in N atural Language Processing ,\\npages 3369–3391, 2022.\\n[Devlin et al., 2019] Jacob Devlin, Ming-W ei Chang, Kenton L ee, and Kristina T outanova. Bert: Pre-\\ntraining of deep bidirectional transformers for language u nderstanding. In Proceedings of the 2019\\nConference of the North American Chapter of the Association for Computational Linguistics: Human\\nLanguage T echnologies, V olume 1 (Long and Short P apers) , pages 4171–4186, 2019.\\n[Ding et al., 2024] Y iran Ding, Li Lyna Zhang, Chengruidong Z hang, Y uanyuan Xu, Ning Shang, Jiahang\\nXu, Fan Y ang, and Mao Y ang. Longrope: Extending llm context w indow beyond 2 million tokens. arXiv\\npreprint arXiv:2402.13753 , 2024.\\n[Dolan and Brockett, 2005] Bill Dolan and Chris Brockett. Au tomatically constructing a corpus of senten-\\ntial paraphrases. In Proceedings of Third International W orkshop on P araphrasi ng (IWP2005) , 2005.\\n[Dong et al., 2019] Li Dong, Nan Y ang, W enhui W ang, Furu W ei, X iaodong Liu, Y u W ang, Jianfeng\\nGao, Ming Zhou, and Hsiao-Wuen Hon. Uniﬁed language model pr e-training for natural language\\nunderstanding and generation. Advances in neural information processing systems , 32, 2019.\\n[Dong et al., 2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun,\\nJingjing Xu, and Zhifang Sui. A survey on in-context learnin g. arXiv preprint arXiv:2301.00234 , 2022.\\n[Dong et al., 2021] Y ihe Dong, Jean-Baptiste Cordonnier, an d Andreas Loukas. Attention is not all you\\nneed: Pure attention loses rank doubly exponentially with d epth. In International Conference on Machine\\nLearning, pages 2793–2803. PMLR, 2021.\\n[Drozdov et al., 2022] Andrew Drozdov, Nathanael Schärli, E kin Akyürek, Nathan Scales, Xinying Song,\\nXinyun Chen, Olivier Bousquet, and Denny Zhou. Composition al semantic parsing with large language\\nmodels. In Proceedings of The Eleventh International Conference on Le arning Representations , 2022.\\n[Dua et al., 2022] Dheeru Dua, Shivanshu Gupta, Sameer Singh , and Matt Gardner. Successive prompting\\nfor decomposing complex questions. In Proceedings of the 2022 Conference on Empirical Methods in\\nNatural Language Processing , pages 1251–1265, 2022.\\n[Dubey et al., 2024] Abhimanyu Dubey, Abhinav Jauhri, Abhin av Pandey, Abhishek Kadian, Ahmad Al-\\nDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Y ang , Angela Fan, et al. The llama 3 herd of\\nmodels. arXiv preprint arXiv:2407.21783 , 2024.\\n[Dubois et al., 2024] Y ann Dubois, Chen Xuechen Li, Rohan T ao ri, Tianyi Zhang, Ishaan Gulrajani, Jimmy\\nBa, Carlos Guestrin, Percy S Liang, and T atsunori B Hashimot o. Alpacafarm: A simulation framework\\nfor methods that learn from human feedback. Advances in Neural Information Processing Systems , 36,\\n2024.\\n[Eisenstein et al., 2023] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour,\\nDJ Dvijotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, and Peter Shaw .\\nHelping or herding? reward model ensembles mitigate but do n ot eliminate reward hacking. arXiv\\npreprint arXiv:2312.09244 , 2023.\\n[Elsken et al., 2019] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search:\\nA survey. Journal of Machine Learning Research , 20(55):1–21, 2019.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 214, 'page_label': '208'}, page_content='208 Alignment\\n[Erhan et al., 2010] Dumitru Erhan, Aaron Courville, Y oshua Bengio, and Pascal V incent. Why does\\nunsupervised pre-training help deep learning? In Proceedings of the thirteenth international conference\\non artiﬁcial intelligence and statistics , pages 201–208, 2010.\\n[Fan et al., 2019] Angela Fan, Edouard Grave, and Armand Joul in. Reducing transformer depth on demand\\nwith structured dropout. In Proceedings of International Conference on Learning Repre sentations, 2019.\\n[Fedus et al., 2022] William Fedus, Barret Zoph, and Noam Sha zeer. Switch transformers: Scaling to\\ntrillion parameter models with simple and efﬁcient sparsit y. The Journal of Machine Learning Research ,\\n23(1):5232–5270, 2022.\\n[Fernandes et al., 2023] Patrick Fernandes, Aman Madaan, Em my Liu, António Farinhas, Pedro Henrique\\nMartins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, T o ngshuang Wu, Graham Neubig, and\\nAndré F . T . Martins. Bridging the gap: A survey on integratin g (human) feedback for natural language\\ngeneration. T ransactions of the Association for Computational Linguis tics, 11:1643–1668, 2023.\\n[Franklin and Graesser, 1996] Stan Franklin and Art Graesse r. Is it an agent, or just a program?: A taxon-\\nomy for autonomous agents. In International workshop on agent theories, architectures, and languages ,\\npages 21–35. Springer, 1996.\\n[Frensch and Funke, 2014] Peter A Frensch and Joachim Funke. Complex problem solving: The European\\nperspective. Psychology Press, 2014.\\n[Gale et al., 2019] Trevor Gale, Erich Elsen, and Sara Hooker . The state of sparsity in deep neural networks.\\narXiv preprint arXiv:1902.09574 , 2019.\\n[Ganguli et al., 2023] Deep Ganguli, Amanda Askell, Nichola s Schiefer, Thomas I. Liao, Kamile Luko-\\nsiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherin e Olsson, Danny Hernandez, Dawn Drain,\\nDustin Li, Eli Tran-Johnson, Ethan Perez, Jackson Kernion, Jamie Kerr, Jared Mueller, Joshua Landau,\\nKamal Ndousse, Karina Nguyen, Liane Lovitt, Michael Sellit to, Nelson Elhage, Noemí Mercado, Nova\\nDasSarma, Oliver Rausch, Robert Lasenby, Robin Larson, Sam Ringer, Sandipan Kundu, Saurav Kada-\\nvath, Scott Johnston, Shauna Kravec, Sheer El Showk, T amera Lanham, Timothy T elleen-Lawton, T om\\nHenighan, Tristan Hume, Y untao Bai, Zac Hatﬁeld-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph,\\nSam McCandlish, T om Brown, Christopher Olah, Jack Clark, Sa muel R. Bowman, and Jared Kaplan.\\nThe capacity for moral self-correction in large language mo dels. arXiv preprint arXiv:2302.07459 , 2023.\\n[Gao et al., 2023] Leo Gao, John Schulman, and Jacob Hilton. S caling laws for reward model overopti-\\nmization. In International Conference on Machine Learning , pages 10835–10866. PMLR, 2023a.\\n[Gao et al., 2023] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alo n, Pengfei Liu, Y iming Y ang, Jamie\\nCallan, and Graham Neubig. Pal: Program-aided language mod els. In International Conference on\\nMachine Learning , pages 10764–10799. PMLR, 2023b.\\n[Gao et al., 2023] Y unfan Gao, Y un Xiong, Xinyu Gao, Kangxian g Jia, Jinliu Pan, Y uxi Bi, Y i Dai, Jiawei\\nSun, and Haofen W ang. Retrieval-augmented generation for l arge language models: A survey. arXiv\\npreprint arXiv:2312.10997 , 2023c.\\n[Garg et al., 2022] Shivam Garg, Dimitris Tsipras, Percy S Li ang, and Gregory V aliant. What can trans-\\nformers learn in-context? a case study of simple function cl asses. Advances in Neural Information\\nProcessing Systems , 35:30583–30598, 2022.\\n[Ge et al., 2024] Y uan Ge, Y ilun Liu, Chi Hu, W eibin Meng, Shim in T ao, Xiaofeng Zhao, Hongxia\\nMa, Li Zhang, Boxing Chen, Hao Y ang, Bei Li, T ong Xiao, and Jin gbo Zhu. Clustering and rank-\\ning: Diversity-preserved instruction selection through e xpert-aligned quality estimation. arXiv preprint\\narXiv:2402.18191 , 2024.\\n[Gemma T eam, 2024] Google DeepMind Gemma T eam. Gemma: Open M odels Based on Gemini Re-\\nsearch and T echnology, 2024.\\n[Goodhart, 1984] Charles AE Goodhart. Problems of monetary management: the UK experience . Springer,\\n1984.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 215, 'page_label': '209'}, page_content='4.5 Summary 209\\n[Gordon et al., 2021] Mitchell A Gordon, Kevin Duh, and Jared Kaplan. Data and parameter scaling laws\\nfor neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in N atural\\nLanguage Processing , pages 5915–5922, 2021.\\n[Gu and Dao, 2023] Albert Gu and Tri Dao. Mamba: Linear-time s equence modeling with selective state\\nspaces. arXiv preprint arXiv:2312.00752 , 2023.\\n[Gunasekar et al., 2023] Suriya Gunasekar, Y i Zhang, Jyoti A neja, Caio César T eodoro Mendes, Allie Del\\nGiorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann , Gustavo de Rosa, Olli Saarikivi, Adil\\nSalim, Shital Shah, Harkirat Singh Behl, Xin W ang, Sébastie n Bubeck, Ronen Eldan, Adam T auman\\nKalai, Y in T at Lee, and Y uanzhi Li. T extbooks are all you need . arXiv preprint arXiv:2306.11644 , 2023.\\n[Guo et al., 2024] Qingyan Guo, Rui W ang, Junliang Guo, Bei Li , Kaitao Song, Xu T an, Guoqing Liu, Jiang\\nBian, and Y ujiu Y ang. Connecting large language models with evolutionary algorithms yields powerful\\nprompt optimizers. In The T welfth International Conference on Learning Represen tations, 2024.\\n[Gupta and Berant, 2020] Ankit Gupta and Jonathan Berant. Gm at: Global memory augmentation for\\ntransformers. arXiv preprint arXiv:2006.03274 , 2020.\\n[Gupta et al., 2021] Ankit Gupta, Guy Dar, Shaya Goodman, Dav id Ciprut, and Jonathan Berant. Memory-\\nefﬁcient transformers via top-k attention. In Proceedings of the Second W orkshop on Simple and Efﬁcient\\nNatural Language Processing , pages 39–52, 2021.\\n[Han et al., 2021] Xu Han, Zhengyan Zhang, Ning Ding, Y uxian G u, Xiao Liu, Y uqi Huo, Jiezhong Qiu,\\nLiang Zhang, W entao Han, Minlie Huang, Qin Jin, Y anyan Lan, Y ang Liu, Zhiyuan Liu, Zhiwu Lu,\\nXipeng Qiu, Ruihua Song, Jie T ang, Ji-Rong W en, Jinhui Y uan, W ayne Xin Zhao, and Jun Zhu. Pre-\\ntrained models: Past, present and future. AI Open , 2:225–250, 2021.\\n[Han et al., 2024] Zeyu Han, Chao Gao, Jinyang Liu, Jeff Zhang , and Sai Qian Zhang. Parameter-efﬁcient\\nﬁne-tuning for large models: A comprehensive survey. arXiv preprint arXiv:2403.14608 , 2024.\\n[Harlap et al., 2018] Aaron Harlap, Deepak Narayanan, Amar P hanishayee, V ivek Seshadri, Nikhil Deva-\\nnur, Greg Ganger, and Phil Gibbons. Pipedream: Fast and efﬁc ient pipeline parallel dnn training. arXiv\\npreprint arXiv:1806.03377 , 2018.\\n[He et al., 2019] Kaiming He, Ross Girshick, and Piotr Dollár . Rethinking imagenet pre-training. In\\nProceedings of the IEEE/CVF International Conference on Co mputer V ision, pages 4918–4927, 2019.\\n[He et al., 2021] Pengcheng He, Xiaodong Liu, Jianfeng Gao, a nd W eizhu Chen. Deberta: Decoding-\\nenhanced bert with disentangled attention. In Proceedings of International Conference on Learning\\nRepresentations, 2021.\\n[Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin Gimpel . Gaussian error linear units (gelus).\\narXiv preprint arXiv:1606.08415 , 2016.\\n[Hendrycks et al., 2020] Dan Hendrycks, Xiaoyuan Liu, Eric W allace, Adam Dziedzic, Rishabh Krishnan,\\nand Dawn Song. Pretrained transformers improve out-of-dis tribution robustness. In Proceedings of the\\n58th Annual Meeting of the Association for Computational Li nguistics, pages 2744–2751, 2020.\\n[Hendrycks et al., 2021] Dan Hendrycks, Collin Burns, Steve n Basart, Andy Zou, Mantas Mazeika, Dawn\\nSong, and Jacob Steinhardt. Measuring massive multitask la nguage understanding. In Proceedings of\\nInternational Conference on Learning Representations , 2021.\\n[Hestness et al., 2017] Joel Hestness, Sharan Narang, Newsh a Ardalani, Gregory Diamos, Heewoo Jun,\\nHassan Kianinejad, Md Mostofa Ali Patwary, Y ang Y ang, and Y a nqi Zhou. Deep learning scaling is\\npredictable, empirically. arXiv preprint arXiv:1712.00409 , 2017.\\n[Hewitt, 2024] John Hewitt. Instruction following without instruction tuning, 2024. URL https://nlp.\\nstanford.edu/~johnhew/instruction-following. html.\\n[Hewitt et al., 2024] John Hewitt, Nelson F Liu, Percy Liang, and Christopher D Manning. Instruction\\nfollowing without instruction tuning. arXiv preprint arXiv:2409.14254 , 2024.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 216, 'page_label': '210'}, page_content='210 Alignment\\n[Hochreiter and Schmidhuber, 1997] Sepp Hochreiter and Jür gen Schmidhuber. Long short-term memory.\\nNeural computation , 9(8):1735–1780, 1997.\\n[Hoffmann et al., 2022] Jordan Hoffmann, Sebastian Borgeau d, Arthur Mensch, Elena Buchatskaya, Trevor\\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendric ks, Johannes W elbl, Aidan Clark, T om\\nHennigan, Eric Noland, Katie Millican, George van den Dries sche, Bogdan Damoc, Aurelia Guy, Si-\\nmon Osindero, Karen Simonyan, Erich Elsen, Jack W . Rae, Orio l V inyals, and Laurent Sifre. Training\\ncompute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\\n[Honovich et al., 2023] Or Honovich, Thomas Scialom, Omer Le vy, and Timo Schick. Unnatural instruc-\\ntions: Tuning language models with (almost) no human labor. In Proceedings of the 61st Annual Meeting\\nof the Association for Computational Linguistics (V olume 1 : Long P apers) , pages 14409–14428, 2023.\\n[Houlsby et al., 2019] Neil Houlsby, Andrei Giurgiu, Stanis law Jastrzebski, Bruna Morrone, Quentin\\nDe Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylva in Gelly. Parameter-efﬁcient transfer\\nlearning for NLP. In Proceedings of the 36th International Conference on Machin e Learning , pages\\n2790–2799. PMLR, 2019.\\n[Hu et al., 2022] Edward J Hu, yelong shen, Phillip W allis, Ze yuan Allen-Zhu, Y uanzhi Li, Shean W ang,\\nLu W ang, and W eizhu Chen. LoRA: Low-rank adaptation of large language models. In International\\nConference on Learning Representations , 2022.\\n[Huang, 2009] Liang Huang. Dynamic programming-based sear ch algorithms in NLP. In Proceedings\\nof Human Language T echnologies: The 2009 Annual Conference of the North American Chapter of the\\nAssociation for Computational Linguistics, Companion V ol ume: T utorial Abstracts , 2009.\\n[Huang et al., 2019] Y anping Huang, Y oulong Cheng, Ankur Bap na, Orhan Firat, Mia Xu Chen, Dehao\\nChen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Y onghui Wu, an d Zhifeng Chen. Gpipe: Efﬁcient\\ntraining of giant neural networks using pipeline paralleli sm. Advances in neural information processing\\nsystems, 32, 2019.\\n[Hutchins et al., 2022] DeLesley Hutchins, Imanol Schlag, Y uhuai Wu, Ethan Dyer, and Behnam\\nNeyshabur. Block-recurrent transformers. Advances in neural information processing systems , 35:\\n33248–33261, 2022.\\n[Jelinek, 1998] Frederick Jelinek. Statistical methods for speech recognition . MIT Press, 1998.\\n[Jiang et al., 2023] Albert Q Jiang, Alexandre Sablayrolles , Arthur Mensch, Chris Bamford, Deven-\\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gia nna Lengyel, Guillaume Lample, Lucile\\nSaulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, T even Le Scao, Thibaut Lavril,\\nThomas W ang, Timothée Lacroix, and William El Sayed. Mistra l 7b. arXiv preprint arXiv:2310.06825 ,\\n2023a.\\n[Jiang et al., 2023] Huiqiang Jiang, Qianhui Wu, Chin-Y ew Li n, Y uqing Y ang, and Lili Qiu. Llmlingua:\\nCompressing prompts for accelerated inference of large lan guage models. In Proceedings of the 2023\\nConference on Empirical Methods in Natural Language Proces sing, pages 13358–13376, 2023b.\\n[Jiang et al., 2020] Zhengbao Jiang, Frank F Xu, Jun Araki, an d Graham Neubig. How can we know what\\nlanguage models know? T ransactions of the Association for Computational Linguis tics, 8:423–438,\\n2020.\\n[Jiao et al., 2020] Xiaoqi Jiao, Y ichun Y in, Lifeng Shang, Xi n Jiang, Xiao Chen, Linlin Li, Fang W ang,\\nand Qun Liu. Tinybert: Distilling bert for natural language understanding. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2020 , pages 4163–4174, 2020.\\n[Joshi et al., 2017] Mandar Joshi, Eunsol Choi, Daniel S W eld , and Luke Zettlemoyer. Triviaqa: A large\\nscale distantly supervised challenge dataset for reading c omprehension. In Proceedings of the 55th\\nAnnual Meeting of the Association for Computational Lingui stics (V olume 1: Long P apers) , pages 1601–\\n1611, 2017.\\n[Joshi et al., 2020] Mandar Joshi, Danqi Chen, Y inhan Liu, Da niel S W eld, Luke Zettlemoyer, and Omer\\nLevy. Spanbert: Improving pre-training by representing an d predicting spans. T ransactions of the'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 217, 'page_label': '211'}, page_content='4.5 Summary 211\\nassociation for computational linguistics , 8:64–77, 2020.\\n[Jurafsky and Martin, 2008] Dan Jurafsky and James H. Martin . Speech and Language Processing (2nd\\ned.). Prentice Hall, 2008.\\n[Kahneman, 2011] Daniel Kahneman. Thinking, fast and slow . macmillan, 2011.\\n[Kaplan et al., 2020] Jared Kaplan, Sam McCandlish, T om Heni ghan, T om B Brown, Benjamin Chess, Re-\\nwon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Am odei. Scaling laws for neural language\\nmodels. arXiv preprint arXiv:2001.08361 , 2020.\\n[Katharopoulos et al., 2020] Angelos Katharopoulos, Apoor v Vyas, Nikolaos Pappas, and François Fleuret.\\nTransformers are rnns: Fast autoregressive transformers w ith linear attention. In International conference\\non machine learning , pages 5156–5165. PMLR, 2020.\\n[Khandelwal et al., 2020] Urvashi Khandelwal, Omer Levy, Da n Jurafsky, Luke Zettlemoyer, and Mike\\nLewis. Generalization through memorization: Nearest neig hbor language models. In International\\nConference on Learning Representations , 2020.\\n[Khot et al., 2023] Tushar Khot, Harsh Trivedi, Matthew Finl ayson, Y ao Fu, Kyle Richardson, Peter Clark,\\nand Ashish Sabharwal. Decomposed prompting: A modular appr oach for solving complex tasks. In\\nProceedings of The Eleventh International Conference on Le arning Representations , 2023.\\n[Kim et al., 2023] Sehoon Kim, Coleman Hooper, Thanakul W att anawong, Minwoo Kang, Ruohan\\nY an, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Mic hael W . Mahoney, Y akun Sophia\\nShao, and Amir Gholami. Full stack optimization of transfor mer inference: a survey. arXiv preprint\\narXiv:2302.14017 , 2023.\\n[Kirkpatrick et al., 2017] James Kirkpatrick, Razvan Pasca nu, Neil Rabinowitz, Joel V eness, Guillaume\\nDesjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago R amalho, Agnieszka Grabska-Barwinska,\\nDemis Hassabis, Claudia Clopath, Dharshan Kumaran, and Rai a Hadsell. Overcoming catastrophic\\nforgetting in neural networks. Proceedings of the national academy of sciences , 114(13):3521–3526,\\n2017.\\n[Koehn, 2010] Philipp Koehn. Statistical Machine T ranslation . Cambridge University Press, 2010.\\n[Kojima et al., 2022] T akeshi Kojima, Shixiang Shane Gu, Mac hel Reid, Y utaka Matsuo, and Y usuke\\nIwasawa. Large language models are zero-shot reasoners. Advances in neural information processing\\nsystems, 35:22199–22213, 2022.\\n[Korthikanti et al., 2023] V ijay Anand Korthikanti, Jared C asper, Sangkug Lym, Lawrence McAfee,\\nMichael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. R educing activation recomputation in\\nlarge transformer models. Proceedings of Machine Learning and Systems , 5, 2023.\\n[Krakovna et al., 2020] V ictoria Krakovna, Jonathan Uesato , Vladimir Mikulik, Matthew\\nRahtz, T om Everitt, Ramana Kumar, Zac Kenton, Jan Leike, and Shane Legg. Speciﬁ-\\ncation gaming: the ﬂip side of ai ingenuity. https://deepmind.google/discover/blog/\\nspecification-gaming-the-flip-side-of-ai-ingenuity , 2020.\\n[Kung and Peng, 2023] Po-Nien Kung and Nanyun Peng. Do models really learn to follow instructions?\\nan empirical study of instruction tuning. arXiv preprint arXiv:2305.11383 , 2023.\\n[Kwon et al., 2023] W oosuk Kwon, Zhuohan Li, Siyuan Zhuang, Y ing Sheng, Lianmin Zheng, Cody Hao\\nY u, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efﬁcient me mory management for large language\\nmodel serving with pagedattention. arXiv preprint arXiv:2309.06180 , 2023.\\n[Lake and Baroni, 2018] Brenden Lake and Marco Baroni. Gener alization without systematicity: On\\nthe compositional skills of sequence-to-sequence recurre nt networks. In International conference on\\nmachine learning , pages 2873–2882. PMLR, 2018.\\n[Lambert et al., 2024] Nathan Lambert, V alentina Pyatkin, J acob Morrison, LJ Miranda, Bill Y uchen\\nLin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, T om Zick, Y e jin Choi, Noah A. Smith, and Han-\\nnaneh Hajishirzi. Rewardbench: Evaluating reward models f or language modeling. arXiv preprint'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 218, 'page_label': '212'}, page_content='212 Alignment\\narXiv:2403.13787 , 2024.\\n[Lample and Conneau, 2019] Guillaume Lample and Alexis Conn eau. Cross-lingual language model\\npretraining. arXiv preprint arXiv:1901.07291 , 2019.\\n[Lan et al., 2020] Zhenzhong Lan, Mingda Chen, Sebastian Goo dman, Kevin Gimpel, Piyush Sharma, and\\nRadu Soricut. Albert: A lite bert for self-supervised learn ing of language representations. In Proceedings\\nof International Conference on Learning Representations , 2020.\\n[Lee et al., 2023] Harrison Lee, Samrat Phatale, Hassan Mans oor, Kellie Ren Lu, Thomas Mesnard, Johan\\nFerret, Colton Bishop, Ethan Hall, V ictor Carbune, and Abhi nav Rastogi. Rlaif: Scaling reinforcement\\nlearning from human feedback with ai feedback. arXiv preprint arXiv:2309.00267 , 2023.\\n[Lester et al., 2021] Brian Lester, Rami Al-Rfou, and Noah Co nstant. The power of scale for parameter-\\nefﬁcient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in N atural Lan-\\nguage Processing , pages 3045–3059, 2021.\\n[Lewis et al., 2020] Mike Lewis, Y inhan Liu, Naman Goyal, Mar jan Ghazvininejad, Abdelrahman Mo-\\nhamed, Omer Levy, V eselin Stoyanov, and Luke Zettlemoyer. B art: Denoising sequence-to-sequence\\npre-training for natural language generation, translatio n, and comprehension. In Proceedings of the 58th\\nAnnual Meeting of the Association for Computational Lingui stics, pages 7871–7880, 2020.\\n[Li et al., 2023] Bei Li, Rui W ang, Junliang Guo, Kaitao Song, Xu T an, Hany Hassan, Arul Menezes, T ong\\nXiao, Jiang Bian, and JingBo Zhu. Deliberate then generate: Enhanced prompting framework for text\\ngeneration. arXiv preprint arXiv:2305.19835 , 2023a.\\n[Li, 2011] Hang Li. Learning to Rank for Information Retrieval and Natural Lang uage Processing . Online\\naccess: Morgan & Claypool Synthesis Collection Five. Morga n & Claypool Publishers, 2011. ISBN\\n9781608457076.\\n[Li et al., 2022] Huayang Li, Y ixuan Su, Deng Cai, Y an W ang, an d Lemao Liu. A survey on retrieval-\\naugmented text generation. arXiv preprint arXiv:2202.01110 , 2022.\\n[Li et al., 2024] Shanda Li, Chong Y ou, Guru Guruganesh, Josh ua Ainslie, Santiago Ontanon, Manzil\\nZaheer, Sumit Sanghai, Y iming Y ang, Sanjiv Kumar, and Srina dh Bhojanapalli. Functional interpolation\\nfor relative positions improves long context transformers . In The T welfth International Conference on\\nLearning Representations , 2024.\\n[Li et al., 2023] Shenggui Li, Fuzhao Xue, Chaitanya Baranwa l, Y ongbin Li, and Y ang Y ou. Sequence\\nparallelism: Long sequence training from system perspecti ve. In Proceedings of the 61st Annual Meeting\\nof the Association for Computational Linguistics (V olume 1 : Long P apers) , pages 2391–2404, 2023b.\\n[Li and Liang, 2021] Xiang Lisa Li and Percy Liang. Preﬁx-tun ing: Optimizing continuous prompts for\\ngeneration. In Proceedings of the 59th Annual Meeting of the Association fo r Computational Linguistics\\nand the 11th International Joint Conference on Natural Lang uage Processing (V olume 1: Long P apers) ,\\npages 4582–4597, 2021.\\n[Li, 2023] Y inheng Li. A practical survey on zero-shot promp t design for in-context learning. In Proceed-\\nings of the 14th International Conference on Recent Advance s in Natural Language Processing , pages\\n641–647, 2023.\\n[Li et al., 2023] Y ucheng Li, Bo Dong, Frank Guerin, and Cheng hua Lin. Compressing context to enhance\\ninference efﬁciency of large language models. In Proceedings of the 2023 Conference on Empirical\\nMethods in Natural Language Processing , pages 6342–6353, 2023c.\\n[Lialin et al., 2023] Vladislav Lialin, V ijeta Deshpande, a nd Anna Rumshisky. Scaling down to scale up:\\nA guide to parameter-efﬁcient ﬁne-tuning. arXiv preprint arXiv:2303.15647 , 2023.\\n[Lightman et al., 2024] Hunter Lightman, V ineet Kosaraju, Y uri Burda, Harrison Edwards, Bowen Baker,\\nT eddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Kar l Cobbe. Let’s verify step by step. In The\\nT welfth International Conference on Learning Representat ions, 2024.\\n[Liu et al., 2024] Aixin Liu, Bei Feng, Bing Xue, Bingxuan W an g, Bochao Wu, Chengda Lu, Chenggang'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 219, 'page_label': '213'}, page_content='4.5 Summary 213\\nZhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepsee k-v3 technical report. arXiv preprint\\narXiv:2412.19437 , 2024a.\\n[Liu et al., 2022] Jiachang Liu, Dinghan Shen, Y izhe Zhang, W illiam B Dolan, Lawrence Carin, and\\nW eizhu Chen. What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside\\nOut (DeeLIO 2022): The 3rd W orkshop on Knowledge Extraction and Integration for Deep Learning\\nArchitectures, pages 100–114, 2022.\\n[Liu et al., 2023] Pengfei Liu, W eizhe Y uan, Jinlan Fu, Zheng bao Jiang, Hiroaki Hayashi, and Graham\\nNeubig. Pre-train, prompt, and predict: A systematic surve y of prompting methods in natural language\\nprocessing. ACM Computing Surveys , 55(9):1–35, 2023a.\\n[Liu et al., 2024] Tianqi Liu, Y ao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J Liu, and\\nJialu Liu. Statistical rejection sampling improves prefer ence optimization. In The T welfth International\\nConference on Learning Representations , 2024b.\\n[Liu, 2009] Tie-Y an Liu. Learning to rank for information re trieval. F oundations and T rends® in Informa-\\ntion Retrieval , 3(3):225–331, 2009.\\n[Liu et al., 2023] Xiao Liu, Y anan Zheng, Zhengxiao Du, Ming D ing, Y ujie Qian, Zhilin Y ang, and Jie\\nT ang. Gpt understands, too. AI Open , 2023b.\\n[Liu et al., 2023] Xiaoxia Liu, Jingyi W ang, Jun Sun, Xiaohan Y uan, Guoliang Dong, Peng Di, W enhai\\nW ang, and Dongxia W ang. Prompting frameworks for large lang uage models: A survey. arXiv preprint\\narXiv:2311.12785 , 2023c.\\n[Liu et al., 2024] Xinyu Liu, Runsong Zhao, Pengcheng Huang, Chunyang Xiao, Bei Li, Jingang W ang,\\nT ong Xiao, and Jingbo Zhu. Forgetting curve: A reliable meth od for evaluating memorization capability\\nfor long-context models. In Proceedings of the 2024 Conference on Empirical Methods in N atural\\nLanguage Processing , pages 4667–4682, 2024c.\\n[Liu et al., 2019] Y inhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy,\\nMike Lewis, Luke Zettlemoyer, and V eselin Stoyanov. Robert a: A robustly optimized bert pretraining\\napproach. arXiv preprint arXiv:1907.11692 , 2019.\\n[Longpre et al., 2023] Shayne Longpre, Le Hou, Tu V u, Albert W ebson, Hyung W on Chung, Y i T ay,\\nDenny Zhou, Quoc V . Le, Barret Zoph, Jason W ei, and Adam Rober ts. The ﬂan collection: Designing\\ndata and methods for effective instruction tuning. In International Conference on Machine Learning ,\\npages 22631–22648. PMLR, 2023.\\n[Ma et al., 2023] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxi an He, Liangke Gui, Graham Neubig,\\nJonathan May, and Luke Zettlemoyer. Mega: Moving average eq uipped gated attention. In The Eleventh\\nInternational Conference on Learning Representations , 2023.\\n[Ma et al., 2024] Xuezhe Ma, Xiaomeng Y ang, W enhan Xiong, Bei di Chen, Lili Y u, Hao Zhang, Jonathan\\nMay, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalo don: Efﬁcient llm pretraining and\\ninference with unlimited context length. arXiv preprint arXiv:2404.08801 , 2024.\\n[Madaan et al., 2024] Aman Madaan, Niket T andon, Prakhar Gup ta, Skyler Hallinan, Luyu Gao,\\nSarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye , Y iming Y ang, Shashank Gupta, Bod-\\nhisattwa Prasad Majumder, Katherine Hermann, Sean W elleck , Amir Y azdanbakhsh, and Peter Clark.\\nSelf-reﬁne: Iterative reﬁnement with self-feedback. Advances in Neural Information Processing Sys-\\ntems, 36, 2024.\\n[Manning, 2022] Christopher D Manning. Human language unde rstanding & reasoning. Daedalus, 151\\n(2):127–138, 2022.\\n[Marcus, 1993] Gary F Marcus. Negative evidence in language acquisition. Cognition, 46(1):53–85, 1993.\\n[Martins et al., 2022] Pedro Henrique Martins, Zita Marinho , and André FT Martins. ∞-former: Inﬁnite\\nmemory transformer-former: Inﬁnite memory transformer. I n Proceedings of the 60th Annual Meeting\\nof the Association for Computational Linguistics (V olume 1 : Long P apers) , pages 5468–5485, 2022.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 220, 'page_label': '214'}, page_content='214 Alignment\\n[Mavi et al., 2024] V aibhav Mavi, Anubhav Jangra, and Adam Ja towt. Multi-hop question answering.\\nF oundations and T rends® in Information Retrieval , 17(5):457–586, 2024.\\n[Michel et al., 2019] Paul Michel, Omer Levy, and Graham Neub ig. Are sixteen heads really better than\\none? Advances in neural information processing systems , 32, 2019.\\n[Micikevicius et al., 2018] Paulius Micikevicius, Sharan N arang, Jonah Alben, Gregory Diamos, Erich\\nElsen, David Garcia, Boris Ginsburg, Michael Houston, Olek sii Kuchaiev, Ganesh V enkatesh, and Hao\\nWu. Mixed precision training. In Proceedings of International Conference on Learning Repre sentations,\\n2018.\\n[Miettinen, 1999] Kaisa Miettinen. Nonlinear multiobjective optimization , volume 12. Springer Science\\n& Business Media, 1999.\\n[Mikolov et al., 2013] T omas Mikolov, Kai Chen, Greg Corrado , and Jeffrey Dean. Efﬁcient estimation\\nof word representations in vector space. In Proceedings of the International Conference on Learning\\nRepresentations (ICLR 2013) , 2013a.\\n[Mikolov et al., 2013] T omas Mikolov, Ilya Sutskever, Kai Ch en, Greg Corrado, and Jeffrey Dean. Dis-\\ntributed representations of words and phrases and their com positionality. In Proceedings of the 26th In-\\nternational Conference on Neural Information Processing S ystems - V olume 2 , pages 3111–3119, 2013b.\\n[Min et al., 2019] Sewon Min, V ictor Zhong, Luke Zettlemoyer , and Hannaneh Hajishirzi. Multi-hop read-\\ning comprehension through question decomposition and resc oring. In Proceedings of the 57th Annual\\nMeeting of the Association for Computational Linguistics , pages 6097–6109, 2019.\\n[Minaee et al., 2024] Shervin Minaee, T omas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard\\nSocher, Xavier Amatriain, and Jianfeng Gao. Large language models: A survey. arXiv preprint\\narXiv:2402.06196 , 2024.\\n[Mishra et al., 2022] Swaroop Mishra, Daniel Khashabi, Chit ta Baral, and Hannaneh Hajishirzi. Cross-\\ntask generalization via natural language crowdsourcing in structions. In Proceedings of the 60th Annual\\nMeeting of the Association for Computational Linguistics ( V olume 1: Long P apers) , pages 3470–3487,\\n2022.\\n[Mnih et al., 2016] V olodymyr Mnih, Adrià Puigdomènech Badi a, Mehdi Mirza, Alex Graves, Tim Harley,\\nTimothy P Lillicrap, David Silver, and Koray Kavukcuoglu. A synchronous methods for deep reinforce-\\nment learning. In Proceedings of the 33rd International Conference on Intern ational Conference on\\nMachine Learning , pages 1928–1937, 2016.\\n[Mohtashami and Jaggi, 2024] Amirkeivan Mohtashami and Mar tin Jaggi. Random-access inﬁnite context\\nlength for transformers. Advances in Neural Information Processing Systems , 36, 2024.\\n[Mu et al., 2024] Jesse Mu, Xiang Li, and Noah Goodman. Learni ng to compress prompts with gist tokens.\\nAdvances in Neural Information Processing Systems , 36, 2024.\\n[Munkhdalai et al., 2024] Tsendsuren Munkhdalai, Manaal Fa ruqui, and Siddharth Gopal. Leave no context\\nbehind: Efﬁcient inﬁnite context transformers with inﬁni- attention. arXiv preprint arXiv:2404.07143 ,\\n2024.\\n[Nakano et al., 2021] Reiichiro Nakano, Jacob Hilton, Suchi r Balaji, Jeff Wu, Long Ouyang, Christina\\nKim, Christopher Hesse, Shantanu Jain, V ineet Kosaraju, Wi lliam Saunders, Xu Jiang, Karl Cobbe, T yna\\nEloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman.\\nW ebgpt: Browser-assisted question-answering with human f eedback. arXiv preprint arXiv:2112.09332 ,\\n2021.\\n[Narayanan et al., 2021] Deepak Narayanan, Mohammad Shoeyb i, Jared Casper, Patrick LeGresley,\\nMostofa Patwary, V ijay Korthikanti, Dmitri V ainbrand, Pre thvi Kashinkunti, Julie Bernauer, Bryan\\nCatanzaro, Amar Phanishayee, and Matei Zaharia. Efﬁcient l arge-scale language model training on\\ngpu clusters using megatron-lm. In Proceedings of the International Conference for High P erfo rmance\\nComputing, Networking, Storage and Analysis , pages 1–15, 2021.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 221, 'page_label': '215'}, page_content='4.5 Summary 215\\n[Ng et al., 1999] Andrew Y Ng, Daishi Harada, and Stuart J Russ ell. Policy invariance under reward\\ntransformations: Theory and application to reward shaping . In Proceedings of the Sixteenth International\\nConference on Machine Learning , pages 278–287, 1999.\\n[OpenAI, 2024] OpenAI. Learning to reason with llms, Septem ber 2024. URL https://openai.com/\\nindex/learning-to-reason-with-llms/ .\\n[Ouyang et al., 2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diog o Almeida, Carroll L. W ainwright, Pamela\\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Al ex Ray, John Schulman, Jacob Hilton,\\nFraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, P eter W elinder, Paul F . Christiano, Jan\\nLeike, and Ryan Lowe. Training language models to follow ins tructions with human feedback. Advances\\nin Neural Information Processing Systems , 35:27730–27744, 2022.\\n[Pal et al., 2023] Koyena Pal, Jiuding Sun, Andrew Y uan, Byro n C W allace, and David Bau. Future lens:\\nAnticipating subsequent tokens from a single hidden state. In Proceedings of the 27th Conference on\\nComputational Natural Language Learning (CoNLL) , pages 548–560, 2023.\\n[Pan et al., 2022] Alexander Pan, Kush Bhatia, and Jacob Stei nhardt. The effects of reward misspeciﬁca-\\ntion: Mapping and mitigating misaligned models. In International Conference on Learning Representa-\\ntions, 2022.\\n[Pan et al., 2024] Liangming Pan, Michael Saxon, W enda Xu, De epak Nathani, Xinyi W ang, and\\nWilliam Y ang W ang. Automatically correcting large languag e models: Surveying the landscape of\\ndiverse automated correction strategies. T ransactions of the Association for Computational Linguis tics,\\n12:484–506, 2024.\\n[Parisi et al., 2022] Aaron Parisi, Y ao Zhao, and Noah Fiedel . T alm: T ool augmented language models.\\narXiv preprint arXiv:2205.12255 , 2022.\\n[Parisi et al., 2019] German I Parisi, Ronald Kemker, Jose L P art, Christopher Kanan, and Stefan W ermter.\\nContinual lifelong learning with neural networks: A review . Neural networks , 113:54–71, 2019.\\n[Parmar et al., 2018] Niki Parmar, Ashish V aswani, Jakob Usz koreit, Lukasz Kaiser, Noam Shazeer,\\nAlexander Ku, and Dustin Tran. Image transformer. In International conference on machine learn-\\ning, pages 4055–4064. PMLR, 2018.\\n[Penedo et al., 2023] Guilherme Penedo, Quentin Malartic, D aniel Hesslow , Ruxandra Cojocaru, Alessan-\\ndro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam A lmazrouei, and Julien Launay. The reﬁned-\\nweb dataset for falcon llm: outperforming curated corpora w ith web data, and web data only. arXiv\\npreprint arXiv:2306.01116 , 2023.\\n[Peng et al., 2024] Bowen Peng, Jeffrey Quesnelle, Honglu Fa n, and Enrico Shippole. Y aRN: Efﬁcient con-\\ntext window extension of large language models. In The T welfth International Conference on Learning\\nRepresentations, 2024.\\n[Pennington et al., 2014] Jeffrey Pennington, Richard Soch er, and Christopher D. Manning. Glove: Global\\nvectors for word representation. In Proceedings of Empirical Methods in Natural Language Proce ssing\\n(EMNLP), pages 1532–1543, 2014.\\n[Peters et al., 2018] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,\\nKenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the\\n2018 Conference of the North American Chapter of the Associa tion for Computational Linguistics: Hu-\\nman Language T echnologies, V olume 1 (Long P apers) , 2018.\\n[Plackett, 1975] Robin L Plackett. The analysis of permutat ions. Journal of the Royal Statistical Society\\nSeries C: Applied Statistics , 24(2):193–202, 1975.\\n[Prasad et al., 2023] Archiki Prasad, Peter Hase, Xiang Zhou , and Mohit Bansal. Grips: Gradient-free, edit-\\nbased instruction search for prompting large language mode ls. In Proceedings of the 17th Conference of\\nthe European Chapter of the Association for Computational L inguistics, pages 3845–3864, 2023.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 222, 'page_label': '216'}, page_content='216 Alignment\\n[Press et al., 2022] Oﬁr Press, Noah Smith, and Mike Lewis. Tr ain short, test long: Attention with lin-\\near biases enables input length extrapolation. In Proceedings of International Conference on Learning\\nRepresentations, 2022.\\n[Press et al., 2023] Oﬁr Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis.\\nMeasuring and narrowing the compositionality gap in langua ge models. In Findings of the Association\\nfor Computational Linguistics: EMNLP 2023 , pages 5687–5711, 2023.\\n[Pryzant et al., 2023] Reid Pryzant, Dan Iter, Jerry Li, Y in T at Lee, Chenguang Zhu, and Michael Zeng.\\nAutomatic prompt optimization with \"gradient descent\" and beam search. In The 2023 Conference on\\nEmpirical Methods in Natural Language Processing , 2023.\\n[Qiu et al., 2020] Xipeng Qiu, Tianxiang Sun, Y ige Xu, Y unfan Shao, Ning Dai, and Xuanjing Huang.\\nPre-trained models for natural language processing: A surv ey. Science China T echnological Sciences ,\\n63(10):1872–1897, 2020.\\n[Radford et al., 2018] Alec Radford, Karthik Narasimhan, Ti m Salimans, and Ilya Sutskever. Improving\\nlanguage understanding by generative pre-training. OpenAI, 2018.\\n[Radford et al., 2019] Alec Radford, Jeffrey Wu, Rewon Child , David Luan, Dario Amodei, and Ilya\\nSutskever. Language models are unsupervised multitask lea rners. OpenAI blog , 1(8), 2019.\\n[Radford et al., 2021] Alec Radford, Jong W ook Kim, Chris Hal lacy, Aditya Ramesh, Gabriel Goh, Sand-\\nhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin , Jack Clark, Gretchen Krueger, and Ilya\\nSutskever. Learning transferable visual models from natur al language supervision. In International\\nconference on machine learning , pages 8748–8763. PMLR, 2021.\\n[Rae et al., 2019] Jack W Rae, Anna Potapenko, Siddhant M Jaya kumar, Chloe Hillier, and Timothy P\\nLillicrap. Compressive transformers for long-range seque nce modelling. In International Conference on\\nLearning Representations , 2019.\\n[Rafailov et al., 2024] Rafael Rafailov, Archit Sharma, Eri c Mitchell, Christopher D Manning, Stefano\\nErmon, and Chelsea Finn. Direct preference optimization: Y our language model is secretly a reward\\nmodel. Advances in Neural Information Processing Systems , 36, 2024.\\n[Raffel et al., 2020] Colin Raffel, Noam Shazeer, Adam Rober ts, Katherine Lee, Sharan Narang, Michael\\nMatena, Y anqi Zhou, W ei Li, and Peter J. Liu. Exploring the li mits of transfer learning with a uniﬁed\\ntext-to-text transformer. Journal of Machine Learning Research , 21(140):1–67, 2020.\\n[Ramachandran et al., 2017] Prajit Ramachandran, Barret Zo ph, and Quoc V Le. Searching for activation\\nfunctions. arXiv preprint arXiv:1710.05941 , 2017.\\n[Rolnick et al., 2019] David Rolnick, Arun Ahuja, Jonathan S chwarz, Timothy Lillicrap, and Gregory\\nW ayne. Experience replay for continual learning. Advances in Neural Information Processing Systems ,\\n32, 2019.\\n[Rosenfeld et al., 2020] Jonathan S Rosenfeld, Amir Rosenfe ld, Y onatan Belinkov, and Nir Shavit. A con-\\nstructive prediction of the generalization error across sc ales. In Proceedings of International Conference\\non Learning Representations , 2020.\\n[Ruan et al., 2024] Junhao Ruan, Long Meng, W eiqiao Shan, T on g Xiao, and Jingbo Zhu. A survey of llm\\nsurveys. https://github.com/NiuTrans/ABigSurveyOfLLMs, 2024.\\n[Rubin et al., 2022] Ohad Rubin, Jonathan Herzig, and Jonath an Berant. Learning to retrieve prompts\\nfor in-context learning. In Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Languag e T echnologies, pages 2655–2671, 2022.\\n[Russell, 2019] Stuart Russell. Human Compatible: Artiﬁcial Intelligence and the Problem o f Controls .\\nV iking, 2019.\\n[Sanh et al., 2020] V ictor Sanh, Thomas W olf, and Alexander R ush. Movement pruning: Adaptive sparsity\\nby ﬁne-tuning. Advances in Neural Information Processing Systems , 33:20378–20389, 2020.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 223, 'page_label': '217'}, page_content='4.5 Summary 217\\n[Sanh et al., 2022] V ictor Sanh, Albert W ebson, Colin Raffel , Stephen Bach, Lintang Sutawika, Zaid\\nAlyafeai, Antoine Chafﬁn, Arnaud Stiegler, Arun Raja, Mana n Dey, M Saiful Bari, Canwen Xu, Urmish\\nThakker, Shanya Sharma Sharma, Eliza Szczechla, T aewoon Ki m, Gunjan Chhablani, Nihal Nayak,\\nDebajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han W ang, Matteo Manica, Sheng Shen,\\nZheng Xin Y ong, Harshit Pandey, Rachel Bawden, Thomas W ang, Trishala Neeraj, Jos Rozen, Abheesht\\nSharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan T eehan, T even Le Scao, Stella Bider-\\nman, Leo Gao, Thomas W olf, and Alexander M Rush. Multitask pr ompted training enables zero-shot\\ntask generalization. In Proceedings of International Conference on Learning Repre sentations, 2022.\\n[Schick et al., 2023] Timo Schick, Jane A. Y u, Zhengbao Jiang , Fabio Petroni, Patrick Lewis, Gautier Izac-\\nard, Qingfei Y ou, Christoforos Nalmpantis, Edouard Grave, and Sebastian Riedel. PEER: A collaborative\\nlanguage model. In Proceedings of The Eleventh International Conference on Le arning Representations ,\\n2023.\\n[Schick et al., 2024] Timo Schick, Jane Dwivedi-Y u, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric\\nHambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scia lom. T oolformer: Language models can\\nteach themselves to use tools. Advances in Neural Information Processing Systems , 36, 2024.\\n[Schmidhuber, 2015] Jürgen Schmidhuber. Deep learning in n eural networks: An overview . Neural net-\\nworks, 61:85–117, 2015.\\n[Schulman et al., 2015] John Schulman, Sergey Levine, Phili pp Moritz, Michael Jordan, and Pieter Abbeel.\\nTrust region policy optimization. In Proceedings of the 32nd International Conference on Intern ational\\nConference on Machine Learning-V olume 37 , pages 1889–1897, 2015.\\n[Schulman et al., 2017] John Schulman, Filip W olski, Praful la Dhariwal, Alec Radford, and Oleg Klimov.\\nProximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017.\\n[Sennrich et al., 2016] Rico Sennrich, Barry Haddow , and Ale xandra Birch. Improving neural machine\\ntranslation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association\\nfor Computational Linguistics (V olume 1: Long P apers) , pages 86–96, 2016.\\n[Seo et al., 2017] Minjoon Seo, Aniruddha Kembhavi, Ali Farh adi, and Hannaneh Hajishirzi. Bidirectional\\nattention ﬂow for machine comprehension. In Proceedings of International Conference on Learning\\nRepresentations, 2017.\\n[Shannon, 1951] Claude E Shannon. Prediction and entropy of printed english. Bell system technical\\njournal, 30(1):50–64, 1951.\\n[Shaw et al., 2018] Peter Shaw , Jakob Uszkoreit, and Ashish V aswani. Self-attention with relative position\\nrepresentations. In Proceedings of the 2018 Conference of the North American Cha pter of the Associ-\\nation for Computational Linguistics: Human Language T echn ologies, V olume 2 (Short P apers) , pages\\n464–468, 2018.\\n[Shazeer, 2019] Noam Shazeer. Fast transformer decoding: O ne write-head is all you need. arXiv preprint\\narXiv:1911.02150 , 2019.\\n[Shazeer, 2020] Noam Shazeer. Glu variants improve transfo rmer. arXiv preprint arXiv:2002.05202 , 2020.\\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Y e, Linjian M a, Zhewei Y ao, Amir Gholami, Michael W\\nMahoney, and Kurt Keutzer. Q-bert: Hessian based ultra low p recision quantization of bert. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelligence , volume 34, pages 8815–8821, 2020.\\n[Shoeybi et al., 2019] Mohammad Shoeybi, Mostofa Patwary, R aul Puri, Patrick LeGresley, Jared Casper,\\nand Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model\\nparallelism. arXiv preprint arXiv:1909.08053 , 2019.\\n[Skalse et al., 2022] Joar Skalse, Nikolaus Howe, Dmitrii Kr asheninnikov, and David Krueger. Deﬁning\\nand characterizing reward gaming. Advances in Neural Information Processing Systems , 35:9460–9471,\\n2022.\\n[Snell et al., 2022] Charlie Snell, Dan Klein, and Ruiqi Zhon g. Learning by distilling context. arXiv'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 224, 'page_label': '218'}, page_content='218 Alignment\\npreprint arXiv:2209.15189 , 2022.\\n[Socher et al., 2013] Richard Socher, Alex Perelygin, Jean W u, Jason Chuang, Christopher D Manning,\\nAndrew Y Ng, and Christopher Potts. Recursive deep models fo r semantic compositionality over a\\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in n atural language\\nprocessing, pages 1631–1642, 2013.\\n[Song et al., 2019] Kaitao Song, Xu T an, T ao Qin, Jianfeng Lu, and Tie-Y an Liu. Mass: Masked sequence\\nto sequence pre-training for language generation. In International Conference on Machine Learning ,\\npages 5926–5936. PMLR, 2019.\\n[Stiennon et al., 2020] Nisan Stiennon, Long Ouyang, Jeffre y Wu, Daniel Ziegler, Ryan Lowe, Chelsea\\nV oss, Alec Radford, Dario Amodei, and Paul F Christiano. Lea rning to summarize with human feedback.\\nAdvances in Neural Information Processing Systems , 33:3008–3021, 2020.\\n[Su et al., 2024] Jianlin Su, Murtadha Ahmed, Y u Lu, Shengfen g Pan, W en Bo, and Y unfeng Liu. Ro-\\nformer: Enhanced transformer with rotary position embeddi ng. Neurocomputing, 568:127063, 2024.\\n[Sun et al., 2020] Zhiqing Sun, Hongkun Y u, Xiaodan Song, Ren jie Liu, Y iming Y ang, and Denny Zhou.\\nMobilebert: a compact task-agnostic bert for resource-lim ited devices. In Proceedings of the 58th Annual\\nMeeting of the Association for Computational Linguistics , pages 2158–2170, 2020.\\n[Sutskever et al., 2014] Ilya Sutskever, Oriol V inyals, and Quoc V Le. Sequence to sequence learning with\\nneural networks. Advances in neural information processing systems , 27, 2014.\\n[Sutton and Barto, 2018] Richard S. Sutton and Andrew G. Bart o. Reinforcement Learning: An Introduc-\\ntion (2nd ed.) . The MIT Press, 2018.\\n[Szepesvári, 2010] Csaba Szepesvári. Algorithms for reinf orcement learning. Synthesis Lectures on Arti-\\nﬁcial Intelligence and Machine Learning , 4(1):1–103, 2010.\\n[T almor and Berant, 2018] Alon T almor and Jonathan Berant. T he web as a knowledge-base for answering\\ncomplex questions. arXiv preprint arXiv:1803.06643 , 2018.\\n[T aori et al., 2023] Rohan T aori, Ishaan Gulrajani, Tianyi Z hang, Y ann Dubois, Xuechen Li, Carlos\\nGuestrin, Percy Liang, and T atsunori B. Hashimoto. Stanfor d alpaca: An instruction-following llama\\nmodel. https://github.com/tatsu-lab/stanford_alpaca, 2023.\\n[T ay et al., 2020] Y i T ay, Mostafa Dehghani, Dara Bahri, and D onald Metzler. Efﬁcient transformers: A\\nsurvey. CoRR, abs/2009.06732, 2020.\\n[T eam et al., 2024] Gemma T eam, Morgane Riviere, Shreya Path ak, Pier Giuseppe Sessa, Cassidy Hardin,\\nSurya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Boba k Shahriari, Alexandre Ramé, et al.\\nGemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 , 2024.\\n[T eknium, 2023] T eknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants,\\n2023. URL https://huggingface.co/datasets/teknium/OpenHermes- 2.5.\\n[T ouvron et al., 2023] Hugo T ouvron, Thibaut Lavril, Gautie r Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Ro-\\ndriguez, Armand Joulin, Edouard Grave, and Guillaume Lampl e. Llama: Open and efﬁcient foundation\\nlanguage models. arXiv preprint arXiv:2302.13971 , 2023a.\\n[T ouvron et al., 2023] Hugo T ouvron, Louis Martin, Kevin Sto ne, Peter Albert, Amjad Almahairi,\\nY asmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel,\\nLukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem C ucurull, David Esiobu, Jude Fernan-\\ndes, Jeremy Fu, W enyin Fu, Brian Fuller, Cynthia Gao, V edanu j Goswami, Naman Goyal, Anthony\\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kar das, V iktor Kerkez, Madian Khabsa, Is-\\nabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Ann e Lachaux, Thibaut Lavril, Jenya Lee,\\nDiana Liskovich, Y inghai Lu, Y uning Mao, Xavier Martinet, T odor Mihaylov, Pushkar Mishra, Igor\\nMolybog, Y ixin Nie, Andrew Poulton, Jeremy Reizenstein, Ra shi Rungta, Kalyan Saladi, Alan Schel-\\nten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xi aoqing Ellen T an, Binh T ang, Ross T aylor,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 225, 'page_label': '219'}, page_content='4.5 Summary 219\\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Y an, Iliyan Zarov, Y uchen Zhang, Angela Fan,\\nMelanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robe rt Stojnic, Sergey Edunov, and Thomas\\nScialom. Llama 2: Open foundation and ﬁne-tuned chat models . arXiv preprint arXiv:2307.09288 ,\\n2023b.\\n[Uesato et al., 2022] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa\\nW ang, Antonia Creswell, Geoffrey Irving, and Irina Higgins . Solving math word problems with process-\\nand outcome-based feedback. arXiv preprint arXiv:2211.14275 , 2022.\\n[V aswani et al., 2017] Ashish V aswani, Noam Shazeer, Niki Pa rmar, Jakob Uszkoreit, Llion Jones, Aidan N\\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is al l you need. In Proceedings of Advances in\\nNeural Information Processing Systems , volume 30, 2017.\\n[V on Oswald et al., 2023] Johannes V on Oswald, Eyvind Niklas son, Ettore Randazzo, João Sacramento,\\nAlexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyr ov. Transformers learn in-context by\\ngradient descent. In Proceedings of International Conference on Machine Learni ng, pages 35151–\\n35174. PMLR, 2023.\\n[W ang et al., 2024] Chenglong W ang, Hang Zhou, Y imin Hu, Y ifu Huo, Bei Li, T ongran Liu, T ong Xiao,\\nand Jingbo Zhu. Esrl: Efﬁcient sampling-based reinforceme nt learning for sequence generation. In\\nProceedings of the AAAI Conference on Artiﬁcial Intelligen ce, pages 19107–19115, 2024.\\n[W ang et al., 2023] Liyuan W ang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of\\ncontinual learning: Theory, method and application. arXiv preprint arXiv:2302.00487 , 2023a.\\n[W ang et al., 2019] Qiang W ang, Bei Li, T ong Xiao, Jingbo Zhu, Changliang Li, Derek F W ong, and\\nLidia S Chao. Learning deep transformer models for machine t ranslation. In Proceedings of the 57th\\nAnnual Meeting of the Association for Computational Lingui stics, pages 1810–1822, 2019.\\n[W ang et al., 2022] Xuezhi W ang, Jason W ei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou.\\nRationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747 , 2022a.\\n[W ang et al., 2023] Xuezhi W ang, Jason W ei, Dale Schuurmans, Quoc V Le, Ed H Chi, Sharan Narang,\\nAakanksha Chowdhery, and Denny Zhou. Self-consistency imp roves chain of thought reasoning in lan-\\nguage models. In Proceedings of The Eleventh International Conference on Le arning Representations ,\\n2023b.\\n[W ang et al., 2022] Y izhong W ang, Swaroop Mishra, Pegah Alip oormolabashi, Y eganeh Kordi, Amirreza\\nMirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekar an, Anjana Arunkumar, David Stap, Es-\\nhaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson,\\nKirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel , Mehrad Moradshahi, Mihir Parmar, Mi-\\nrali Purohit, Neeraj V arshney, Phani Rohitha Kaza, Pulkit V erma, Ravsehaj Singh Puri, Rushang Karia,\\nSavan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Suj an Reddy A, Sumanta Patro, T anay Dixit,\\nand Xudong Shen. Super-naturalinstructions: Generalizat ion via declarative instructions on 1600+ nlp\\ntasks. In Proceedings of the 2022 Conference on Empirical Methods in N atural Language Processing ,\\npages 5085–5109, 2022b.\\n[W ang et al., 2023] Y izhong W ang, Hamish Ivison, Pradeep Das igi, Jack Hessel, Tushar Khot, Khy-\\nathi Raghavi Chandu, David W adden, Kelsey MacMillan, Noah A . Smith, Iz Beltagy, and Hannaneh\\nHajishirzi. How far can camels go? exploring the state of ins truction tuning on open resources. Ad-\\nvances in Neural Information Processing Systems , 36:74764–74786, 2023c.\\n[W ang et al., 2023] Y izhong W ang, Y eganeh Kordi, Swaroop Mis hra, Alisa Liu, Noah A Smith, Daniel\\nKhashabi, and Hannaneh Hajishirzi. Self-instruct: Aligni ng language models with self-generated in-\\nstructions. In Proceedings of the 61st Annual Meeting of the Association fo r Computational Linguistics\\n(V olume 1: Long P apers) , pages 13484–13508, 2023d.\\n[W ang et al., 2023] Zhenyi W ang, Enneng Y ang, Li Shen, and Hen g Huang. A comprehensive survey of\\nforgetting in deep learning beyond continual learning. arXiv preprint arXiv:2307.09218 , 2023e.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 226, 'page_label': '220'}, page_content='220 Alignment\\n[W arstadt et al., 2019] Alex W arstadt, Amanpreet Singh, and Samuel R Bowman. Neural network accept-\\nability judgments. T ransactions of the Association for Computational Linguis tics, 7:625–641, 2019.\\n[W ei et al., 2022] Jason W ei, Maarten Bosma, V incent Zhao, Ke lvin Guu, Adams W ei Y u, Brian Lester, Nan\\nDu, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In Proceedings\\nof International Conference on Learning Representations , 2022a.\\n[W ei et al., 2022] Jason W ei, Y i T ay, Rishi Bommasani, Colin R affel, Barret Zoph, Sebastian Borgeaud,\\nDani Y ogatama, Maarten Bosma, Denny Zhou, Donald Metzler, E d H. Chi, T atsunori Hashimoto, Oriol\\nV inyals, Percy Liang, Jeff Dean, and William Fedus. Emergen t abilities of large language models. arXiv\\npreprint arXiv:2206.07682 , 2022b.\\n[W ei et al., 2022] Jason W ei, Xuezhi W ang, Dale Schuurmans, M aarten Bosma, Brian Ichter, Fei Xia,\\nEd H. Chi, Quoc V . Le, and Denny Zhou. Chain-of-thought promp ting elicits reasoning in large language\\nmodels. Advances in Neural Information Processing Systems , 35:24824–24837, 2022c.\\n[W elleck et al., 2023] Sean W elleck, Ximing Lu, Peter W est, F aeze Brahman, Tianxiao Shen, Daniel\\nKhashabi, and Y ejin Choi. Generating sequences by learning to self-correct. In Proceedings of The\\nEleventh International Conference on Learning Representa tions, 2023.\\n[W eng, 2021] Lilian W eng. How to train really large models on many gpus? lilianweng.github.io, Sep\\n2021. URL https://lilianweng.github.io/posts/2021-09-25-train -large/.\\n[Wiener, 1960] Norbert Wiener. Some moral and technical con sequences of automation: As machines\\nlearn they may develop unforeseen strategies at rates that b afﬂe their programmers. Science, 131(3410):\\n1355–1358, 1960.\\n[Williams et al., 2018] Adina Williams, Nikita Nangia, and S amuel Bowman. A broad-coverage challenge\\ncorpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North\\nAmerican Chapter of the Association for Computational Ling uistics: Human Language T echnologies,\\nV olume 1 (Long P apers) , pages 1112–1122, 2018.\\n[Williams, 1992] Ronald J Williams. Simple statistical gra dient-following algorithms for connectionist\\nreinforcement learning. Machine learning , 8:229–256, 1992.\\n[Wingate et al., 2022] David Wingate, Mohammad Shoeybi, and T aylor Sorensen. Prompt compression\\nand contrastive conditioning for controllability and toxi city reduction in language models. In Findings\\nof the Association for Computational Linguistics: EMNLP 20 22, pages 5621–5634, 2022.\\n[Wu et al., 2024] Wilson Wu, John X Morris, and Lionel Levine. Do language models plan for future\\ntokens? arXiv preprint arXiv:2404.00859 , 2024.\\n[Wu et al., 2021] Y uhuai Wu, Markus Norman Rabe, DeLesley Hut chins, and Christian Szegedy. Memo-\\nrizing transformers. In Proceedings of International Conference on Learning Repre sentations, 2021.\\n[Wu et al., 2023] Zeqiu Wu, Y ushi Hu, W eijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu,\\nNoah A. Smith, Mari Ostendorf, and Hannaneh Hajishirzi. Fin e-grained human feedback gives better\\nrewards for language model training. In Thirty-seventh Conference on Neural Information Processi ng\\nSystems, 2023.\\n[Xia et al., 2024] Mengzhou Xia, Sadhika Malladi, Suchin Gur urangan, Sanjeev Arora, and Danqi Chen.\\nLess: Selecting inﬂuential data for targeted instruction t uning. arXiv preprint arXiv:2402.04333 , 2024.\\n[Xiao et al., 2024] Guangxuan Xiao, Y uandong Tian, Beidi Che n, Song Han, and Mike Lewis. Efﬁcient\\nstreaming language models with attention sinks. In Proceedings of The T welfth International Conference\\non Learning Representations , 2024.\\n[Xiao and Zhu, 2023] T ong Xiao and Jingbo Zhu. Introduction t o transformers: an nlp perspective. arXiv\\npreprint arXiv:2311.17633 , 2023.\\n[Xiao et al., 2013] T ong Xiao, Jingbo Zhu, and T ongran Liu. Ba gging and boosting statistical machine\\ntranslation systems. Artiﬁcial Intelligence , 195:496–527, 2013.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 227, 'page_label': '221'}, page_content='4.5 Summary 221\\n[Xiao et al., 2019] T ong Xiao, Y inqiao Li, Jingbo Zhu, Zhengt ao Y u, and T ongran Liu. Sharing attention\\nweights for fast transformer. In Proceedings of the T wenty-Eighth International Joint Conf erence on\\nArtiﬁcial Intelligence (IJCAI-19) , pages 5292–5298, 2019.\\n[Xie et al., 2022] Sang Michael Xie, Aditi Raghunathan, Perc y Liang, and T engyu Ma. An explanation\\nof in-context learning as implicit bayesian inference. In Proceedings of International Conference on\\nLearning Representations , 2022.\\n[Xin et al., 2020] Ji Xin, Raphael T ang, Jaejun Lee, Y aoliang Y u, and Jimmy Lin. Deebert: Dynamic early\\nexiting for accelerating bert inference. In Proceedings of the 58th Annual Meeting of the Association fo r\\nComputational Linguistics , pages 2246–2251, 2020.\\n[Xu et al., 2024] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang T ao,\\nQingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pr e-trained language models to follow\\ncomplex instructions. In The T welfth International Conference on Learning Represen tations, 2024.\\n[Y ang et al., 2024] An Y ang, Baosong Y ang, Beichen Zhang, Bin yuan Hui, Bo Zheng, Bowen Y u,\\nChengyuan Li, Dayiheng Liu, Fei Huang, Haoran W ei, et al. Qwe n2. 5 technical report. arXiv preprint\\narXiv:2412.15115 , 2024.\\n[Y ang et al., 2019] Zhilin Y ang, Zihang Dai, Y iming Y ang, Jai me Carbonell, Russ R Salakhutdinov, and\\nQuoc V Le. Xlnet: Generalized autoregressive pretraining f or language understanding. Advances in\\nneural information processing systems , 32, 2019.\\n[Y ao et al., 2024] Shunyu Y ao, Dian Y u, Jeffrey Zhao, Izhak Sh afran, T om Grifﬁths, Y uan Cao, and Karthik\\nNarasimhan. Tree of thoughts: Deliberate problem solving w ith large language models. Advances in\\nNeural Information Processing Systems , 36, 2024.\\n[Y arowsky, 1995] David Y arowsky. Unsupervised word sense d isambiguation rivaling supervised methods.\\nIn Proceedings of the 33rd annual meeting of the association fo r computational linguistics , pages 189–\\n196, 1995.\\n[Y u et al., 2023] Zihan Y u, Liang He, Zhen Wu, Xinyu Dai, and Ji ajun Chen. T owards better chain-of-\\nthought prompting strategies: A survey. arXiv preprint arXiv:2310.04959 , 2023.\\n[Zaheer et al., 2020] Manzil Zaheer, Guru Guruganesh, Kumar A vinava Dubey, Joshua Ainslie, C. Alberti,\\nS. Ontañón, Philip Pham, Anirudh Ravula, Qifan W ang, L. Y ang , and A. Ahmed. Big bird: Transformers\\nfor longer sequences. Advances in neural information processing systems , 33:17283–17297, 2020.\\n[Zellers et al., 2018] Rowan Zellers, Y onatan Bisk, Roy Schw artz, and Y ejin Choi. Swag: A large-scale\\nadversarial dataset for grounded commonsense inference. I n Proceedings of the 2018 Conference on\\nEmpirical Methods in Natural Language Processing , pages 93–104, 2018.\\n[Zhang and Sennrich, 2019] Biao Zhang and Rico Sennrich. Roo t mean square layer normalization. Ad-\\nvances in Neural Information Processing Systems , 32, 2019.\\n[Zhang et al., 2023] Zhuosheng Zhang, Y ao Y ao, Aston Zhang, X iangru T ang, Xinbei Ma, Zhiwei He,\\nY iming W ang, Mark Gerstein, Rui W ang, Gongshen Liu, and Hai Z hao. Igniting language intelli-\\ngence: The hitchhiker’s guide from chain-of-thought reaso ning to language agents. arXiv preprint\\narXiv:2311.11797 , 2023a.\\n[Zhang et al., 2023] Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought\\nprompting in large language models. In The Eleventh International Conference on Learning Represe n-\\ntations, 2023b.\\n[Zhao et al., 2024] Hao Zhao, Maksym Andriushchenko, France sco Croce, and Nicolas Flammarion. Long\\nis more for alignment: A simple but tough-to-beat baseline f or instruction ﬁne-tuning. arXiv preprint\\narXiv:2402.04833 , 2024.\\n[Zhao et al., 2023] W ayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi T ang, Xiaolei W ang, Y upeng Hou,\\nY ingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Y ifa n Du, Chen Y ang, Y ushuo Chen, Z. Chen,\\nJinhao Jiang, Ruiyang Ren, Y ifan Li, Xinyu T ang, Zikang Liu, Peiyu Liu, Jianyun Nie, and Ji rong W en.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 228, 'page_label': '222'}, page_content='222 Alignment\\nA survey of large language models. arXiv preprint arXiv:2303.18223 , 2023.\\n[Zhou et al., 2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Sri ni Iyer, Jiao Sun, Y uning Mao, Xuezhe Ma,\\nA via Efrat, Ping Y u, Lili Y u, Susan Zhang, Gargi Ghosh, Mike L ewis, Luke Zettlemoyer, and Omer\\nLevy. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206 , 2023a.\\n[Zhou et al., 2023] Denny Zhou, Nathanael Schärli, Le Hou, Ja son W ei, Nathan Scales, Xuezhi W ang, Dale\\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc V . Le, and Ed H. Chi. Least-to-most prompting enables\\ncomplex reasoning in large language models. In Proceedings of The Eleventh International Conference\\non Learning Representations , 2023b.\\n[Zhou et al., 2020] W angchunshu Zhou, Canwen Xu, T ao Ge, Juli an McAuley, Ke Xu, and Furu W ei. Bert\\nloses patience: Fast and robust inference with early exit. Advances in Neural Information Processing\\nSystems, 33:18330–18341, 2020.\\n[Zhou et al., 2023] Y ongchao Zhou, Andrei Ioan Muresanu, Ziw en Han, Keiran Paster, Silviu Pitis, Harris\\nChan, and Jimmy Ba. Large language models are human-level pr ompt engineers. In The Eleventh\\nInternational Conference on Learning Representations , 2023c.\\n[Zoph and Le, 2016] Barret Zoph and Quoc Le. Neural architect ure search with reinforcement learning. In\\nProceedings of International Conference on Learning Repre sentations, 2016.\\n[Zoph et al., 2020] Barret Zoph, Golnaz Ghiasi, Tsung-Y i Lin , Y in Cui, Hanxiao Liu, Ekin Dogus Cubuk,\\nand Quoc Le. Rethinking pre-training and self-training. Advances in neural information processing\\nsystems, 33:3833–3845, 2020.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 229, 'page_label': '223'}, page_content='Index\\nk-NN, 74\\nk-NN LM, 76\\nk-NN language modeling, 76\\nk-nearest neighbors, 74\\nA2C, 175\\naction-value function, 171\\nadvantage, 175\\nadvantage actor-critic, 175\\nAgent, 47\\nALiBi, 85\\nalignment, 46\\nattention with linear biases, 85\\nautomated machine learning, 137\\nautomatic prompt design, 137\\nAutoML, 137\\nautonomous agents, 134\\nBART ,19\\nBERT ,1\\nBest-of-N sampling, 197\\nBoN sampling, 197\\nBradley-T erry model, 178\\ncalculation annotation, 113\\ncatastrophic forgetting, 34\\ncausal language modeling, 9\\nchain of thought, 113\\nchain-of-thought prompting, 53\\ncompletion, 6\\ncompositional generalization, 122\\nCoT , 113\\nCOT prompting, 53\\ncross-lingual language models, 28\\ncumulative reward, 172\\ndeliberate-then-generate, 126\\ndemonstrations, 6\\ndirect preference optimization, 190\\nDocument Rotation, 20\\nDPO, 190\\nDTG, 126\\nemergent abilities, 63\\nexternal memories, 74\\nExtrapolation, 81\\nfew-shot COT prompting, 54\\ngated linear unit, 58\\ngaussian error linear unit, 58\\nGeLU, 58\\nGLU, 58\\nGPT , 1\\nGQA, 80\\nGrouped query attention, 80\\nhard prompts, 140\\nhuman preference alignment, 152\\nICL, 53\\nICT , 6\\nimportance sampling, 180\\nin-context learning, 6, 53, 95\\ninput inversion, 163\\ninstruction alignment, 152\\ninstruction ﬁne-tuning, 43, 154\\ninterference, 30\\ninternal memories, 74\\nInterpolation, 82\\nirreducible error, 63\\nkey-value cache, 68\\nKV cache, 68\\nlabel mapping, 105\\nLearning from Human Feedback, 47\\nleast-to-most prompting, 119\\nlong-context LLMs, 66\\nmasked language modeling, 1, 9\\nmBERT ,28\\nmemory-based methods, 74\\nMQA, 79\\nmulti-lingual BERT , 28\\nmulti-query attention, 79\\nNAS, 137\\nneural architecture search, 137\\nnext sentence prediction, 13\\nNSP , 13\\nofﬂine reinforcement learning, 193\\none-shot COT prompting, 54\\nOutcome-based Approaches, 195\\n223'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 10.01.2', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-16T20:13:48-05:00', 'moddate': '2025-01-16T20:13:48-05:00', 'title': '', 'subject': '', 'author': '', 'keywords': '', 'source': 'LLMs.pdf', 'total_pages': 231, 'page': 230, 'page_label': '224'}, page_content='224 Alignment\\noveroptimization problem, 189\\nPerformance Estimation, 137\\nperformance function, 172\\nperformance gap recovered, 167\\npermuted language modeling, 11\\nPGR, 167\\nPlackett-Luce model, 184\\nPPO, 50, 181\\npreﬁx ﬁne-tuning, 144\\npreﬁx language modeling, 16\\nproblem decomposition, 116\\nProcess-based Approaches, 195\\nprompt embeddings, 148\\nprompt engineering, 95\\nprompt optimization, 137\\nPrompt Search Space, 137\\nprompting engineering, 51\\nproximal policy optimization, 50, 181\\nQ-value function, 171\\nRAG, 76\\nratio function, 180\\nrectiﬁed linear unit, 58\\nreinforcement learning from human feedback,\\n47, 153\\nrejection sampling, 198\\nrelation extraction, 108\\nReLU, 58\\nretrieval-augmented generation, 76\\nreturn, 172\\nreward gaming, 189\\nreward hacking, 189\\nReward Model, 47\\nRLHF , 47, 153\\nRoBERT a,26\\nsample efﬁcient, 164\\nscaling laws, 63\\nself-consistency , 129\\nself-instruct, 160\\nself-supervised learning, 3\\nself-training, 3\\nSentence Reordering, 19\\nSequence Encoding Models, 3\\nSequence Generation Models, 3\\nSFT , 47, 152\\nsingle-round prediction, 155\\nsoft prompts, 140\\nSpan Masking, 19\\nstate-value function, 171\\nStrong Ceiling Performance, 167\\nSub-problem Generation, 118\\nSub-problem Solving, 118\\nsuperﬁcial alignment hypothesis, 164\\nSupervised Fine-tuning, 47\\nsupervised ﬁne-tuning, 152\\nsupervised learning, 2\\nsurrogate objective, 180\\nT5, 15\\nTD, 176\\ntemporal difference, 176\\ntext completion, 109\\ntext transformation, 109\\nT oken Deletion, 19\\nT oken Masking, 19\\nTransformers, 1\\ntranslation language modeling, 29\\ntrust regions, 181\\nunsupervised learning, 2\\nW eak Performance, 167\\nweak-to-strong generalization, 166\\nW eak-to-strong Performance, 167\\nXLMs, 28\\nzero-shot COT , 54\\nzero-shot learning, 45')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading PDF File\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader1 = PyPDFLoader(\"LLMs.pdf\")\n",
    "docs = loader1.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web based loader \n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(web_paths=(\"https://en.wikipedia.org/wiki/Speech\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           class_ = (\"vector-header-container\",\"cnotice-message-container\")\n",
    "                       ))\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Speech'}, page_content='\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\nDonate Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## arXiv\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "docs = ArxivLoader(query='1605.08386', load_max_docs = 2).load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2016-05-26', 'Title': 'Heat-bath random walks with Markov bases', 'Authors': 'Caprice Stanley, Tobias Windisch', 'Summary': 'Graphs on lattice points are studied whose edges come from a finite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on\\nfibers of a fixed integer matrix can be bounded from above by a constant. We\\nthen study the mixing behaviour of heat-bath random walks on these graphs. We\\nalso state explicit conditions on the set of moves so that the heat-bath random\\nwalk, a generalization of the Glauber dynamics, is an expander in fixed\\ndimension.'}, page_content='arXiv:1605.08386v1  [math.CO]  26 May 2016\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nAbstract. Graphs on lattice points are studied whose edges come from a ﬁnite set of\\nallowed moves of arbitrary length. We show that the diameter of these graphs on ﬁbers of a\\nﬁxed integer matrix can be bounded from above by a constant. We then study the mixing\\nbehaviour of heat-bath random walks on these graphs. We also state explicit conditions\\non the set of moves so that the heat-bath random walk, a generalization of the Glauber\\ndynamics, is an expander in ﬁxed dimension.\\nContents\\n1.\\nIntroduction\\n1\\n2.\\nGraphs and statistics\\n3\\n3.\\nBounds on the diameter\\n4\\n4.\\nHeat-bath random walks\\n8\\n5.\\nAugmenting Markov bases\\n14\\nReferences\\n19\\n1. Introduction\\nA ﬁber graph is a graph on the ﬁnitely many lattice points F ⊂Zd of a polytope where\\ntwo lattice points are connected by an edge if their diﬀerence lies in a ﬁnite set of allowed\\nmoves M ⊂Zd. The implicit structure of these graphs makes them a useful tool to explore\\nthe set of lattice points randomly: At the current lattice point u ∈F, an element m ∈±M\\nis sampled and the random walk moves along m if u + m ∈F and stays at u otherwise.\\nThe corresponding Markov chain is irreducible if the underlying ﬁber graph is connected and\\nthe set M is called a Markov basis for F in this case. This paper investigates the heat-bath\\nversion of this random walk: At the current lattice point u ∈F, we sample m ∈M and move\\nto a random element in the integer ray (u + Z · m) ∩F. The authors of [6] discovered that\\nthis random walk can be seen as a discrete version of the hit-and-run algorithm [15, 26, 16]\\nthat has been used frequently to sample from all the points of a polytope – not only from its\\nlattice points. The popularity of the continuous version of the hit-and-run algorithm has not\\nspread to its discrete analog, and not much is known about its mixing behaviour. One reason\\nis that it is already challenging to guarantee that all points in the underlying set F can be\\nDate: September 12, 2018.\\n2010 Mathematics Subject Classiﬁcation. Primary: 05C81, Secondary: 37A25, 11P21.\\nKey words and phrases. Heat-bath random walks, sampling, lattice points, Markov bases.\\n1\\n2\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nreached by a random walk that uses moves from M, whereas for the continuous version, a\\nrandom sampling from the unit sphere suﬃces. However, in many situations where a Markov\\nbasis is known, the heat-bath random walk is evidently fast. For instance, it was shown in [3]\\nthat the heat-bath random walk on contingency tables mixes rapidly when the number of\\ncolumns is ﬁxed. To work around the connectedness issue, a discrete hit-and-run algorithm\\nwas introduced in [1] for arbitrary ﬁnite sets F ⊂Zd. At each step in this random walk, a\\nsubordinate and unrestricted random walk starts at the current lattice point u ∈F and uses\\nthe unit vectors to collect a set of proposals S ⊂Zd. The random walk then moves from u\\nto a random point in S ∩F.\\nRandom walks of the heat-bath type, such as the one presented above, have been studied\\nrecently in [8] in a more general context. In this paper, we explore the mixing behaviour of\\nheat-bath random walks on the lattice points of polytopes with Markov bases. Throughout,\\nwe assume that a Markov basis has been found already and refer to the relevant literature\\nfor their computation [24, 25, 11, 17, 10, 21]. We call the underlying graph of the heat-bath\\nrandom walk a compressed ﬁber graph (Deﬁnition 2.5) and determine in Section 3 bounds on\\nits graph-diameter. We prove that for any A ∈Zm×d with kerZ(A)∩Nd = {0}, the diameter of\\ncompressed ﬁber graphs on {u ∈Nd : Au = b} that use a ﬁxed Markov bases M ⊂kerZ(A) is\\nbounded from above by a constant as b varies (Theorem 3.15). In contrast, we show that the\\ndiameter of conventional ﬁber graphs grow linearly under a dilation of the underlying polytope\\n(Remark 3.9). This gives rise to slow mixing results for conventional ﬁber walks as observed\\nin [27]. In Section 4, we study in more detail the combinatorial and analytical structure\\nof the transition matrices of heat-bath random walks on lattice points and prove upper and\\nlower bounds on their second largest eigenvalues. We also discuss how the distribution on the\\nmoves M aﬀects the speed of convergence (Example 4.21). Theorem 5.8 establishes with the\\ncanonical path approach from [23] an upper bound on the second largest eigenvalue when the\\nMarkov basis is augmenting (Deﬁnition 5.1) and the stationary distribution is uniform. From\\nthat, we conclude fast mixing results for random walks on lattice points in ﬁxed dimension.\\nAcknowledgements. CS was partially supported by the US National Science Foundation\\n(DMS 0954865). TW gratefully acknowledges the support received from the German National\\nAcademic Foundation.\\nConventions and Notation. The natural numbers are N := {0, 1, 2, . . .} and for any N ∈N,\\nN>N := {n ∈N : n > N} and N≥N := {N} ∪N>N. For n ∈N>0, let [n] := {1, . . . , n}. Let\\nM ⊂Qd be a ﬁnite set, then Z·M := {λm : m ∈M, λ ∈Z} and NM is the aﬃne semigroup\\nin Zd generated by M. For an integer matrix A ∈Zm×d with columns a1, . . . , ad ∈Zm,\\nwe write NA := N{a1, . . . , ad}. A graph is always undirected and can have multiple loops.\\nThe distance of two nodes u, v which are contained in the same connected component of a\\ngraph G, i.e. the number of edges in a shortest path between u and v in G, is denoted by\\ndistG(u, v). We set distG(u, v) := ∞if u and v are disconnected. A mass function on a ﬁnite\\nset Ωis a map f : Ω→[0, 1] such that P\\nω∈Ωf(ω) = 1. A mass function f on Ωis positive\\nif f(ω) > 0 for all ω ∈Ω. A set F ⊂Zd is normal if it there exists a polytope P ⊂Qd such\\nthat P ∩Zd = F.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n3\\n2. Graphs and statistics\\nWe ﬁrst introduce the statistical framework in which this paper lives and recall important\\naspects of the interplay between graphs and statistics. A random walk on a graph G = (V, E)\\nis a map H : V × V →[0, 1] such that for all v ∈V , P\\nu∈V H(v, u) = 1 and such that\\nH(v, u) = 0 if {v, u} ̸∈E. When there is no ambiguity, we represent a random walk as an\\n|V | × |V |-matrix, for example when it is clear how the elements of V are ordered. Fix a\\nrandom walk H on G. Then H is irreducible if for all v, u ∈V there exists t ∈N such that\\nHt(v, u) > 0. The random walk H is reversible if there exists a mass function µ : V →[0, 1]\\nsuch that µ(u) · H(u, v) = µ(v) · H(v, u) for all u, v ∈V and symmetric if H is a symmetric\\nmap. A mass function π : V →[0, 1] is a stationary distribution of H if π ◦H = π. For\\nsymmetric random walks, the uniform distribution on V is always a stationary distribution.\\nIf |V | = n, then we denote the eigenvalues of H by 1 = λ1(H) ≥λ2(H) ≥· · · ≥λn(H) ≥−1\\nand we write λ(H) := max{λ2(H), −λn(H)} for the second largest eigenvalue modulus of H.\\nAny irreducible random walk has a unique stationary distribution [14, Corollary 1.17] and\\nλ(H) ∈[0, 1] measures the convergence rate: the smaller λ(H), the faster the convergence.\\nThe aim of this paper is to study random walks on lattice points that use a set of moves.\\nTypically, this is achieved by constructing a graph on the set of lattice points as follows\\n(compare to [7, Section 1.3] and [24, Chapter 5]).\\nDeﬁnition 2.1. Let F ⊂Zd be a ﬁnite set and M ⊂Zd. The graph F(M) is the graph on\\nF where two nodes u, v ∈F are adjacent if u −v ∈M or v −u ∈M.\\nA normal set F ⊂Zd is ﬁnite and satisﬁes F = convQ(F)∩Zd. A canonical class of normal\\nsets that arise in many applications, is given by the ﬁbers of an integer matrix:\\nDeﬁnition 2.2. Let A ∈Zm×d and b ∈NA. The set FA,b := {u ∈Nd : Au = b} is the b-ﬁber\\nof A. The collection of all ﬁbers of A is PA := {FA,b : b ∈NA}. For M ⊂kerZ(A), the graph\\nFA,b (M) is a ﬁber graph.\\nLet F, M ⊂Zd be ﬁnite. If the membership in F can be veriﬁed eﬃciently – for instance\\nwhen F is given implicitly by linear equations and inequalities – then it is possible to explore\\nF randomly using M as follows: At a given node v ∈F, a uniform element m ∈M is\\nselected. If v + m ∈M, then the random walk moves along m to v + m and if v + m ̸∈M,\\nthe we stay at v. Formally, we obtain the following random walk.\\nDeﬁnition 2.3. Let F ⊂Zd and M ⊂Zd be two ﬁnite sets. The simple walk is the random\\nwalk on F(M) where the probability to traverse between to adjacent nodes u and v is |±M|−1\\nand the probability to stay at a node u is |{m ∈±M : u + m ̸∈F}| · | ± M|−1.\\nThe simple walk is symmetric and hence the uniform distribution is a stationary distribu-\\ntion (see also [27, Section 2]). To ensure convergence, the random walk has to be irreducible,\\nthat is, the underlying graph has to be connected. The following deﬁnition is a slight adaption\\nof the generalized Markov basis as deﬁned in [21, Deﬁnition 1].\\nDeﬁnition 2.4. Let P be a collection of ﬁnite subsets of Zd. A ﬁnite set M ⊂Zd is a\\nMarkov basis of P, if for all F ∈P, F(M) is a connected graph.\\n4\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nWe refer to [6, Theorem 3.1] for a proof that for collections PA, a ﬁnite Markov basis\\nalways exists and can be computed with tools from commutative algebra (see also [11] for\\nmore on the computation of Markov bases). We now introduce a construction of graphs on\\nlattice points that also give rise to implementable random walks, but whose edges have far\\nmore reach.\\nDeﬁnition 2.5. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The compression of the graph\\nF(M) is the graph Fc(M) := F(Z · M).\\nFigure 1. Compressing graphs.\\nCompressing a graph F(M) preserves its connectedness: F(M) is connected if and only\\nif Fc(M) is connected.\\n3. Bounds on the diameter\\nIn general knowledge of the diameter of the graph underlying a Markov chain can provide\\ninformation about the mixing time. For random walks on ﬁber graphs, the chains which we\\nconsider, the underlying graph coincides with the ﬁber graph. In this section, we determine\\nlower and upper bounds on the diameter of ﬁber graphs and their compressed counterparts.\\nFor a ﬁnite set M ⊂Zd and any norm ∥· ∥on Rd, let ∥M∥:= maxm∈M ∥m∥.\\nLemma 3.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, then\\ndiam(F(M)) ≥\\n1\\n∥M∥· max{∥u −v∥: u, v ∈F}.\\nProof. If F(M) is not connected, then the statement holds trivially, so assume that M is a\\nMarkov basis for F. Let u′, v′ ∈F such that ∥u′ −v′∥= max{∥u −v∥: u, v ∈F} and let\\nm1, . . . , mr ∈M so that u′ = v′+Pr\\ni=1 mi is a path of minimal length, then ∥u′−v′∥≤r·∥M∥\\nand the claim follows from diam(F(M)) ≥distF(M)(u′, v′) = r.\\n□\\nRemark 3.2. Let F ⊂Zd be a normal set. For all l ∈{−1, 0, 1}d and u, v ∈F we have\\n(u −v)T l ≤∥u −v∥1 and thus widthl(F) := max{(u −v)T l : u, v ∈F} ≤max{∥u −v∥1 :\\nu, v ∈F}. Suppose that u′, v′ ∈F are such that ∥u′ −v′∥1 = max{∥u −v∥1 : u, v ∈F} and\\nlet l′\\ni := sign(u′\\ni −v′\\ni) for i ∈[d], then\\n∥u′ −v′∥1 = (u′ −v′)T · l′ ≤widthl′(F) ≤max{∥u −v∥1 : u, v ∈F} = ∥u′ −v′∥1.\\nThe lattice width of F is width(F) := minl∈Zd widthl(F) and thus Lemma 3.1 gives\\n∥M∥1 · diam(F(M)) ≥width(F).\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n5\\nDeﬁnition 3.3. Let P be a collection of ﬁnite subsets of Zd.\\nA ﬁnite set M ⊂Zd is\\nnorm-like for P if there exists a constant C ∈N such that for all F ∈P and all u, v ∈F,\\ndistF(M)(u, v) ≤C · ∥u −v∥. The set M is ∥· ∥-norm-reducing for P if for all F ∈P and all\\nu, v ∈F there exists m ∈M such that u + m ∈F and ∥u + m −v∥< ∥u −v∥.\\nThe property of being norm-like does not depend on the norm, whereas being norm-\\nreducing does.\\nNorm-reducing sets are always norm-like, and norm-like sets are in turn\\nalways Markov bases, but the reverse of both statements is false in general (Example 3.4 and\\nExample 3.5). For collections PA however, every Markov basis is norm-like (Proposition 3.7).\\nExample 3.4. For any n ∈N, consider the normal set Fn := ([2]×[n]×{0})∪{(2, n, 1)} with\\nthe Markov basis {(0, 1, 0), (0, 0, 1), (−1, 0, −1)}. The distance between (1, 1, 0) and (2, 1, 0)\\nin Fn(M) is 2n and thus M is not norm-like for {Fn : n ∈N} (see also Figure 2).\\nExample 3.5. Let d ∈N and consider A := (1, . . . , 1) ∈Z1×d, then the set M := {e1 −ei :\\n2 ≤i ≤d} is a Markov basis for the collection PA. However, M is not ∥·∥p-norm-reducing for\\nany d ≥3 and any p ∈[1, ∞]. For instance, consider e2 and e3 in FA,1 (M). The only move\\nfrom M that can be applied on e2 is e1−e2, but ∥(e2+e1−e2)−e3)∥p = ∥e2−e3∥p. On the other\\nhand, in the case we cannot ﬁnd a move that decreases the 1-norm of two nodes u, v ∈FA,b\\nby 1, we can ﬁnd instead two moves m1, m2 ∈M such that u + m1, u + m1 + m2 ∈FA,b and\\n∥u + m1 + m2 −v∥= ∥u −v∥−2. Thus, the graph-distance of any two elements u and v in\\nFA,b (M) is at most ∥u −v∥1 and hence M is norm-like for PA.\\nFigure 2. The graph from Example 3.4\\nRemark 3.6. Let P be a collection of ﬁnite subsets of Zd and M ⊂Zd be norm-like for P.\\nIt follows from the deﬁnition that there exists a constant C ∈Q≥0 such that for all F ∈P\\ndiam(F(M)) ≤C · max{∥u −v∥: u, v ∈F}.\\nThe proof of our next results uses the Graver basis GA ⊂Zd for an integer matrix A ∈Zm×d\\nwith kerZ(A) ∩Nd = {0}. We refer to [4, Chapter 3] for a precise deﬁnition.\\nProposition 3.7. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and M ⊂kerZ(A) be a Markov\\nbasis of PA. Then M is norm-like for PA.\\nProof. Let M be a Markov basis for PA. The Graver basis GA for A is a ﬁnite set which\\nis ∥· ∥1-norm-reducing for PA. Thus, deﬁne C := maxg∈GA diam(FA,Ag+ (M)). Now, pick\\nu, v ∈FA,b arbitrarily and let u = v + Pr\\ni=1 gi be a walk from u to v in FA,b (GA) of minimal\\nlength. Since the Graver basis is norm-reducing for FA,b, there always exists a path of length\\nat most ∥u −v∥1 and hence r ≤∥u −v∥1. Every gi can be replaced by a path in FA,Ag+\\ni (M)\\n6\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nof length at most C and these paths stay in FA,b. This gives a path of length C · r, hence\\ndistFA,b(M)(u, v) ≤C∥u −v∥1.\\n□\\nProposition 3.8. Let P ⊂Zd be a polytope with dim(P ∩Zd) > 0 and let M be a Markov\\nbasis for Fi := (i · P) ∩Zd for all i ∈N. There exists a constant C′ ∈Q>0 such that for all\\ni ∈N, C′ · i ≤diam(Fi(M)). If M is norm-like for {Fi : i ∈N}, then there exists a constant\\nC ∈Q>0 such that diam(Fi(M)) ≤C · i for all i ∈N.\\nProof. For the lower bound on the diameter, it suﬃces to show the existence of C′ such that\\nC′ · i ≤max{∥u −v∥: u, v ∈Fi} for all i ∈N due to Lemma 3.1. Since dim(P ∩Zd) > 0,\\nwe can pick distinct w, w′ ∈P ∩Zd. For all i ∈N, i · w, i · w′ ∈Fi and hence i · ∥w −w′∥≤\\nmax{∥u −v∥: u, v ∈Fi}.\\nTo show the upper bound, assume that M is norm-like. It suﬃces to show that there\\nexists C ∈Q≥0 such that max{∥u −v∥: u, v ∈Fi} ≤i · C by Remark 3.6.\\nNow, let\\nv1, . . . , vr ∈Qd such that P = convQ(v1, . . . , vr) and deﬁne C := max{∥vs −vt∥: s ̸= t}.\\nSince Fi = (i·P)∩Zd ⊂convQ(iv1, . . . , ivr) for all i ∈N, we have max{∥u−v∥: u, v ∈Fi} ≤\\nmax{∥ivs −ivt∥: s ̸= t} ≤C · i.\\n□\\nRemark 3.9. Let A ∈Zm×n with kerZ(A) ∩Nd = {0} and let M be a Markov basis for PA.\\nThen M is norm-like due to Proposition 3.7 and thus for all b ∈NA there exists C, C′ ∈Q≥0\\nsuch that\\ni · C′ ≤diam(FA,ib (M)) ≤i · C\\nfor all i ∈N. This generalizes for instance [20, Proposition 2.10] and [27, Example 4.7], where\\nlinear diameters on a ray in NA have been observed. This also implies that the construction\\nof expanders from [27, Section 4] works for every right-hand side b ∈NA.\\nRemark 3.10. Let A ∈Zm×d with kerZ(A)∩Nd = {0}, b ∈NA, and let M be a Markov basis\\nfor PA. Proposition 3.8 provides a new proof that the simple walk on (FA,ib (M))i∈N cannot\\nmix rapidly. The lower bound on the diameter from Proposition 3.8 implies, in general, the\\nfollowing upper bound on the edge-expansion (see for example [9, Proposition 1.30]):\\nh(FA,i·b (M)) ≤|M|\\n\\x12\\nexp\\n\\x12log |FA,i·b|\\nD · i\\n\\x13\\n−1\\n\\x13\\n.\\nIn particular, the edge-expansion cannot be bounded from below by Ω( 1\\np(i))i∈N for a polyno-\\nmial p ∈Q[t] and since (|FA,i·b|)i∈N ∈O(ir)i∈N, the simple walk cannot mix rapidly. In [27],\\nit was shown that the edge-expansion can be bounded from above by O(1\\ni )i∈N, which cannot\\nbe concluded from the upper expression.\\nWe now turn our attention to the diameter of compressed ﬁber graphs.\\nIn particular,\\nwe want to know for which collections of normal sets is their diameter bounded. In general,\\ncompressing a ﬁber graph does not necessarily have an eﬀect on the diameter (Example 3.11).\\nAlthough a low diameter is a necessary condition for good mixing, it is not suﬃcient. For\\ninstance, let Gn be the disjoint union of two complete graphs Kn connected by a single edge.\\nThen diam(Gn) = 3, but h(Gn) ≤1\\nn implies that the simple walk does not mix rapidly.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n7\\nExample 3.11. For any n ∈N, let Fn := {(0, 0), (0, 1), (1, 1), (1, 2), . . . , (n, n)} ⊂Z2. The\\nunit vectors M = {e1, e2} are a Markov basis for {Fn : n ∈N}. However, Fc\\nn(M) = Fn(M)\\nand thus diam(Fc\\nn(M)) = diam(Fn(M)) = 2n is unbounded.\\nLemma 3.12. Let A ∈Zm×d and z ∈kerZ(A). There exists r ∈[2d −2], distinct elements\\ng1, . . . , gr ∈GA, and λ1, . . . , λr ∈N>0 such that z = Pr\\ni=1 λigi and gi ⊑z for all i ∈[r]\\nProof. This is [4, Lemma 3.2.3], although it only becomes clear from the original proof of [22,\\nTheorem 2.1] that the appearing elements are all distinct.\\n□\\nProposition 3.13. Let A ∈Zm×d and P :=\\n\\x08\\n{x ∈Zd : Ax = b, l ≤x ≤u} : l, u ∈Zd, b ∈Zm\\t\\n.\\nThen for all F ∈P, diam(Fc(GA)) ≤2d −2.\\nProof. Let s, t ∈{x ∈Zd : Ax = b, l ≤x ≤u}, then s−t ∈kerZ(A) and thus s = t+Pr\\ni=1 λigi\\nwith r ≤2d −2, λ1, . . . , λr ∈N>0, and distinct g1, . . . , gr ∈GA such that gi ⊑s −t according\\nto Lemma 3.12. It’s now a consequence from [4, Lemma 3.2.4] that all intermediate points\\nt + Pk\\ni=1 λigi for k ≤r are in {x ∈Zd : Ax = b, l ≤x ≤u}.\\n□\\nLemma 3.14. Let F ⊂Zd be ﬁnite and let Fi := (i · convQ(F)) ∩Zd for i ∈N. For all\\nu, v ∈F, distFc\\ni (M)(iu, iv) ≤distF(M)(u, v) for all i ∈N.\\nProof. The statement is trivially true if u and v are disconnected in F(M). Thus, assume\\nthe contrary and let u = v + Pk\\nj=1 mj with mj ∈M be a path in F(M) of length k =\\ndistF(M)(u, v) and let i ∈N. Clearly, i · u = i · v + i · Pk\\nj=1 mj = i · v + Pk\\nl=1 i · mj, so\\nit is left to prove that the elements traversed by this paths are in Fi. Let l ∈[k], since\\nv + Pl\\nj=1 mj ∈F, we have i · v + Pl\\nj=1 i · mj ∈i · F ⊆Fi. Hence, this is a path in Fc\\ni (M)\\nof length k = distF(M)(u, v).\\n□\\nWe are ready to prove that the diameter of compressed ﬁber graphs coming from an integer\\nmatrix can be bounded for all right-hand sides simultaneously.\\nTheorem 3.15. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let M be a Markov basis for\\nPA. There exists a constant C ∈N such that diam(Fc(M)) ≤C for all F ∈PA.\\nProof. Our proof relies on basic properties of the Graver basis GA of A. For any g ∈GA,\\nlet Fg := FA,Ag+ and let K := max{distFg(M)(g+, g−) : g ∈GA}.\\nWe show that the\\ndiameter of any compressed ﬁber graph of A is bounded from above by (2d −2) · K. Let\\nb ∈NA arbitrary and choose elements u, v ∈FA,b. According to Proposition 3.13, there\\nexists r ∈[2d −2], g1, . . . , gr ∈GA and λ1, . . . , λr ∈Z such that u = v + Pr\\ni=1 λigi, and\\nv + Pl\\ni=1 λigi ∈Nd for all l ∈[r].\\nAccording to Lemma 3.14, for any i ∈[r] there are\\nmi\\n1, . . . , mi\\nki ∈M and α1, . . . , αki ∈Z such that λig+\\ni = λig−\\ni + Pki\\nj=1 αjmi\\nj is a path in the\\ncompression of FA,Aλig+\\ni (M) of length ki ≤K. Lifting these paths for every i ∈[r] yields a\\npath u = v + Pr\\ni=1\\nPki\\nj=1 αjmi\\nj in Fc\\nA,b (M) of length r · K ≤(2d −2) · K.\\n□\\n8\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n4. Heat-bath random walks\\nIn this section, we establish the heat-bath random walk on compressed ﬁber graphs. We\\nrefer to [8] for a more general introduction on random walks of heat-bath type. Let F ⊂Zd\\nbe ﬁnite set. For any u ∈F and m ∈Zd, the ray in F through u along m is denoted by\\nRF,m(u) := (u + m · Z) ∩F. Additionally, given a mass function π : F →[0, 1], we deﬁne\\nHπ\\nF,m(x, y) :=\\n(\\nπ(y)\\nπ(RF,m(x))\\n, if y ∈RF,m(x)\\n0\\n, otherwise\\nfor x, y ∈F. For M ⊂Zd and a mass function f : M →[0, 1], the heat-bath random walk is\\n(4.1)\\nHπ,f\\nF,M =\\nX\\nm∈M\\nf(m) · Hπ\\nF,m.\\nThe underlying graph of the heat-bath random walk is the compression Fc(M) and in this\\nsection, we assume throughout that for all m ∈M and λ ∈Z \\\\ {−1, 1}, λ · m ̸∈M. Let us\\nﬁrst recall the basic properties of this random walk (compare also to [6, Lemma 2.2]).\\nAlgorithm 1 Heat-bath random walk on compressed ﬁber graphs\\nInput: F ⊂Zd, M ⊂Zd, v ∈F, mass functions f : M →[0, 1] and π : F →[0, 1], r ∈N\\n1: procedure HeatBath:\\n2:\\nv0 := v\\n3:\\nFOR s = 0; s = s + 1, s < r\\n4:\\nSample m ∈M according to f\\n5:\\nSample vs+1 ∈RF,m(vs) according to RF,m(vs) →[0, 1], y 7→\\nπ(y)\\nπ(RF,m(vs))\\n6:\\nRETURN v1, . . . , vr\\nProposition 4.1. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. Let f : M →[0, 1] and π : F →\\n(0, 1) be mass functions. Then Hπ,f\\nF,M is aperiodic, has stationary distribution π, is reversible\\nwith respect to π, and all of its eigenvalues are non-negative. The random walk is irreducible\\nif and only if {m ∈M : f(m) > 0} is a Markov basis for F.\\nProof. Since for any u ∈F and any m ∈M, Hπ\\nF,m(u, u) > 0, there are halting states and\\nthus Hπ,f\\nF,M is aperiodic. By deﬁnition, π(x)Hπ\\nF,m(x, y) = π(y)Hπ\\nF,m(y, x) and thus Hπ,f\\nF,M\\nis reversible with respect to π and π is a stationary distribution.\\nThe statement on the\\neigenvalues is exactly [8, Lemma 1.2]. Let M′ = {m ∈M : f(m) > 0} and f ′ = f|M′, then\\nHπ,f\\nF,M = Hπ,f′\\nF,M′ and thus the heat-bath random walk is irreducible if and only if M′ is a\\nMarkov basis for F.\\n□\\nRemark 4.2. Analyzing the speed of convergence of random walks with second largest\\neigenvalues does not take the computation time of a single transition into account. From\\na computational point of view, the diﬀerence of the simple walk and the heat-bath random\\nwalk is Step 4 of Algorithm 1. However, we argue that Step 4 can be done eﬃciently in\\nmany cases. For instance, a hard normalizing constant of π cancels out. If π is the uniform\\ndistribution, then one needs to sample uniformly from RF,m(v) in Step 4, which can be done\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n9\\neﬃciently. If the input of Algorithm 1 is a normal set F = {u ∈Zd : Au ≤b} that is given\\nin H-representation, then the length of the ray RF,m(v) can be computed with a number of\\nrounding, division, and comparing operations that is linear in the number of rows of A.\\nThere are situations in which the heat-bath random walk provides no speed-up compared\\nwith the simple walk (Example 4.3). Intuitively, adding more moves to the set of allowed\\nmoves should improve the mixing time of the random walk. In general, however, this is not\\ntrue for the heat-bath walk (Example 4.4).\\nExample 4.3. For n ∈N, consider the normal set\\nFn :=\\n\\x1a\\x14\\n0\\n1\\n1\\n· · ·\\n1\\n1\\n0\\n0\\n· · ·\\n0\\n\\x15\\n,\\n\\x14\\n1\\n0\\n1\\n· · ·\\n1\\n0\\n1\\n0\\n· · ·\\n0\\n\\x15\\n, . . . ,\\n\\x14\\n1\\n1\\n· · ·\\n1\\n0\\n0\\n0\\n· · ·\\n0\\n1\\n\\x15\\x1b\\n⊂Q2×n.\\nIn the language of [7, Section 1.1], Fn is precisely the ﬁber of the 2 × n independence model\\nwhere row sums are (n −1, 1) and column sums are (1, 1, . . . , 1).\\nThe minimal Markov\\nbasis of the independence model, often referred to as the basic moves, is precisely the set\\nMn := {v −u : u, v ∈Fn} \\\\ {0}. In particular, the ﬁber graph Fn(Mn) is the complete\\ngraph on n nodes. All rays along basic moves have length 2 and thus the transition matrices\\nof the simple random walk and the heat-bath random walk coincide. There are n · (n −1)\\nmany basic moves and the transition matrix of both random walks is\\n1\\nn(n −1)\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb+ (n(n −1) −n)\\nn(n −1)\\n· In.\\nThe second largest eigenvalue is 1 −\\n1\\nn−1 which implies that for n →∞, neither the simple\\nwalk nor the heat-bath random walk are rapidly mixing.\\nExample 4.4. Let F = [2] × [5] ⊂Z2, M = {e1, e2, 2e1 + e2}, and let π be the uniform\\ndistribution on F.\\nSince {e2, 2e1 + e2} is not a Markov basis for F, any mass function\\nf : M →[0, 1] must have f(e1) > 0 in order to make the corresponding heat-bath random\\nwalk irreducible.\\nComparing the second largest eigenvalue modulus of heat-bath random\\nwalks that sample from {e1, e2} and M uniformly, we obtain\\nλ\\n\\x121\\n2Hπ\\nF,e1 + 1\\n2Hπ\\nF,e2\\n\\x13\\n= 1\\n2 < 2\\n3 = λ\\n\\x121\\n3Hπ\\nF,e1 + 1\\n3Hπ\\nF,e2 + 1\\n3Hπ\\nF,2e1+e2\\n\\x13\\n.\\nSo, adding 2e1 + e2 to the set of allowed moves slows the walk down. This phenomenon does\\nnot appear for the simple walk on F, where the second largest eigenvalue modulus improves\\nfrom ≈0.905 to ≈0.888 when adding the move 2e1 + e2 to the Markov basis.\\n=\\n+\\n+\\nFigure 3. Decomposition of the graph in Example 4.4\\n10\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nRemark 4.5. Let F ⊂Zd be ﬁnite and M = {m1, . . . , md} ⊂Zd be a linearly independent\\nMarkov basis of F. If the moves are selected uniformly, then the heat-bath random walk on\\nF coincides with the Glauber dynamics on F. To see it, choose u ∈F and let\\nF′ := {λ ∈Zd : u + λ1m1 + · · · + λdmd ∈F}.\\nIt is easy to check that F′ is unique up to translation and depends only on F, u, and M.\\nSince the vectors in M are linearly independent, every element of F can be represented by\\na unique choice of coeﬃcients in F′. Thus, the heat-bath random walk on F using M is\\nequivalent to the heat-bath random walk on on F′ using the unit vectors as moves. For any\\nunit vector ei ∈Zd, the ray through an element v ∈F′ is {w ∈F : wj = vj∀j ̸= i} and this\\nis precisely the form desired in the Glauber dynamics [14, Section 3.3.2].\\nFor the remainder of this section, we primarily focus on heat-bath random walks Hπ,f\\nF,M\\nthat converge to the uniform distribution π on a ﬁnite, but not necessarily normal, set\\nF. We particularly aim for bounds on its second largest eigenvalue by making use of the\\ndecomposition from equation 4.1. Our ﬁrst observations consider its summands Hπ\\nF,m that\\ncan be well understood analytically (Proposition 4.6) and combinatorially (Proposition 4.7).\\nProposition 4.6. Let F ⊂Zd be a ﬁnite set, m ∈Zd, and π : F →[0, 1] be the uniform\\ndistribution. Let R1, . . . , Rk be the disjoint rays through F along m. Then\\n1. Hπ\\nF,m is symmetric and idempotent.\\n2. img(Hπ\\nF,m) = spanR\\nnP\\nx∈R1 ex, P\\nx∈R2 ex, . . . , P\\nx∈Rk ex\\no\\n.\\n3. ker(Hπ\\nF,m) = Lk\\ni=1 spanR {ex −ey : x, y ∈Ri, x ̸= y}.\\n4. rank(Hπ\\nF,m) = k and dim ker(Hπ\\nF,m) = |F| −k.\\n5. The spectrum of Hπ\\nF,m is {0, 1}.\\nProof. Symmetry of Hπ\\nF,m follows from the deﬁnition. By assumption, F is the disjoint union\\nof R1, . . . , Rk and hence there exists a permutation matrix S such that SHπ\\nF,mST is a block\\nmatrix whose building blocks are the matrices\\n1\\n|Ri|\\n\\uf8ee\\n\\uf8ef\\uf8f0\\n1\\n. . .\\n1\\n...\\n...\\n1\\n. . .\\n1\\n\\uf8f9\\n\\uf8fa\\uf8fb∈Q|Ri|×|Ri|.\\nThus, Hπ\\nF,m is idempotent and the rank of Hπ\\nF,m is k. A basis of its image and its kernel can\\nbe read oﬀdirectly and idempotent matrices can only have the eigenvalues 0 and 1.\\n□\\nProposition 4.7. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and let V1, . . . , Vc ⊆F be the nodes of the connected components of F(M), then\\n\\\\\\nm∈M\\nimg(Hπ\\nF,m) = spanR\\n\\uf8f1\\n\\uf8f2\\n\\uf8f3\\nX\\nx∈V1\\nex, . . . ,\\nX\\nx∈Vc\\nex\\n\\uf8fc\\n\\uf8fd\\n\\uf8fe.\\nProof. It is clear by Proposition 4.6 that the set on the right-hand side is contained in any\\nimg(Hπ\\nF,m) since any Vi decomposes disjointly into rays along m ∈M. To show the other\\ninclusion, write M = {m1, . . . , mk} and let for any i ∈[k], Ri\\n1, . . . , Ri\\nni be the disjoint rays\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n11\\nthrough F parallel to mi. In particular, {Ri\\n1, . . . , Ri\\nni} is a partition of F for any i ∈[k]. Let\\nv ∈T\\nm∈M img(Hπ\\nF,m). Again by Proposition 4.6, there exists for any i ∈[k], λi\\n1, . . . , λi\\nni ∈Q\\nsuch that\\nv =\\nni\\nX\\nj=1\\nX\\nx∈Ri\\nj\\nλi\\njex.\\nNotice that if two distinct Markov moves mi and mi′ and two indices j ∈[ni] and j′ ∈[ni′]\\nsatisfy Ri\\nj ∩Ri′\\nj′ ̸= ∅, then λi\\nj = λi′\\nj′. We show that for any i ∈[k] and any a ∈[c], λi\\nj = λi\\nj′\\nwhen Ri\\nj and Ri\\nj′ are a subset of Va. This implies the proposition. So take distinct x, x′ ∈Va\\nand assume that x and x′ lie on diﬀerent rays of mi and let that be x ∈Ri\\nj and x′ ∈Ri\\nj′ with\\nj ̸= j′. Since x and x′ are in the same connected component Va of F(M), let yi0, . . . , yir ∈F\\nbe the nodes on a minimal path in Fc(M) with yi0 = x and yir = x′. For any s ∈[r], yis\\nand yis−1 are contained in the same ray Rks\\nts coming from a Markov move mks. In particular,\\nRts−1\\nks−1 ∩Rks\\nts ̸= ∅and due to our observation made above λi\\nj = λk1\\nt1 = λk2\\nt2 = · · · = λkr\\ntr = λi\\nj′\\nwhich ﬁnishes the proof.\\n□\\nDeﬁnition 4.8. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and M′ ⊆M. Let V be the set of\\nconnected components of F(M\\\\M′) and R be the set of all rays through F along all elements\\nof M′. The ray matrix of F(M) along M′ is AF(M, M′) := (|R ∩V |)R∈R,V ∈V ∈NR×V.\\nExample 4.9. Let F = [3]×[3], M = {e1, e2, e1 +e2}, and M′ = {e1, e2}. Then F(M\\\\M′)\\nhas ﬁve connected components and the ray matrix of F(M) along M′ is\\nAF(M, M′) =\\n\\uf8ee\\n\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8ef\\uf8f0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n0\\n1\\n1\\n1\\n0\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n1\\n1\\n1\\n0\\n0\\n\\uf8f9\\n\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fa\\uf8fb\\n.\\nRemark 4.10. Let F ⊂Z2, then the rays through F along e1 are the connected components\\nof F({e1, e2} \\\\ {e2}) and the rays through F along e2 are the connected components of\\nF({e1, e2} \\\\ {e1}), thus AF(M, e1) = AF(M, e2)T .\\nProposition 4.11. Let F ⊂Zd and M ⊂Zd be ﬁnite sets, π : F →[0, 1] be the uniform\\ndistribution, and M′ ⊆M. Then\\nker(AF(M, M′)) ∼=\\n\\\\\\nm∈M\\\\M′\\nimg(Hπ\\nF,m) ∩\\n\\\\\\nm∈M′\\nker(Hπ\\nF,m).\\nProof. Let V1, . . . , Vc be the connected components of F(M \\\\ M′) and R1, . . . , Rr be the\\nrays along elements in M′. Let I := T\\nm∈M\\\\M′ img(Hπ\\nF,m) and K := T\\nm∈M′ ker(Hπ\\nF,m). By\\nProposition 4.7, any element of I has the form v = Pc\\ni=1(λi\\nP\\nx∈Vi ex) for λ1, . . . , λc ∈Q.\\nAssume additionally that v ∈ker(Hπ\\nF,m) for m ∈M′ and let Ri1, . . . Rij be the rays which\\nbelong to m, then for any k ∈[j], 0 = P\\nx∈Rik vx = Pc\\nj=1 λj|Rik∩Vj|. Put diﬀerently, a vector\\nλ ∈Rc is in the kernel of (|Ri ∩Vj|)i∈[r],j∈[c] if and only if Pc\\ni=1(λi\\nP\\nx∈Vi ex) ∈I ∩K.\\n□\\n12\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nConditions on the kernel of the ray matrix allow us to give a lower bound on the second\\nlargest eigenvalue of the heat-bath random walk.\\nProposition 4.12. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and π be the uniform distribution.\\nLet M′ ⊆M such that ker(AF(M, M′)) ̸= {0}, then λ(Hπ,f\\nF,M) ≥1 −P\\nm∈M′ f(m) for any\\nmass function f : M →[0, 1].\\nProof. Using the isomorphism from Proposition 4.11, we can choose a non-zero v ∈QP such\\nthat Hπ\\nF,mv = v for all m ∈M \\\\ M′ and Hπ\\nF,mv = 0 for all m ∈M′. In particular\\nHπ,f\\nF,Mv =\\nX\\nm∈M\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M′\\nf(m)Hπ\\nF,mv =\\nX\\nm∈M\\\\M\\nf(m)v.\\nSince f is a mass function, 1 −P\\nm∈M′ f(m) is an eigenvalue of Hπ,f\\nF,M.\\n□\\nDeﬁnition 4.13. Let F ⊂Zd and m, m′ ∈Zd not collinear.\\nThe pair (m, m′) has the\\nintersecting ray property in F if the following holds: For any pair of rays R1, R2 parallel\\nto m and any pair of rays R′\\n1, R′\\n2 parallel to m′ where both R1 ∩R′\\n1 and R2 ∩R′\\n2 are not\\nempty, then R1 ∩R′\\n2 ̸= ∅implies R′\\n1 ∩R2 ̸= ∅and |R1| · |R′\\n1|−1 = |R2| · |R′\\n2|−1. For a\\nﬁnite set M ⊂Zd, the graph Fc(M) has the intersecting ray property if all (m, m′) have the\\nintersecting ray property in F.\\nExample 4.14. The compressed ﬁber graph on [n1] × · · · × [nd] ⊂Zd that uses the unit\\nvectors {e1, . . . , ed} as moves has the intersecting ray property. On the other hand, consider\\nF = {u ∈N2 : u1 + u2 ≤1} and take the rays R1 := {(0, 0), (0, 1)} and R2 := {(1, 0)}\\nthat are parallel to e2 and the rays R′\\n1 := {(0, 1)} and R′\\n2 := {(0, 0), (1, 0)} that are parallel\\nto e1. Then R1 ∩R′\\n1 = {(1, 0)} and R2 ∩R′\\n2 = {(0, 1)}, but R1 ∩R′\\n2 = {(0, 0)} ̸= ∅and\\nR′\\n1 ∩R2 = ∅.\\nProposition 4.15. Let m, m′ ∈Zd not collinear and F ⊂Zd be a ﬁnite set. The matrices\\nHπ\\nF,m and Hπ\\nF,m′ commute if and only if (m, m′) have the intersecting ray property in F.\\nProof. Let u1, u2 ∈F. Then\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 =\\n(\\n|RF,m(u1)|−1 · |RF,m′(u2)|−1,\\nif RF,m(u1) ∩RF,m′(u2) ̸= ∅\\n0,\\notherwise\\n.\\nLet R1 := RF,m(u1), R′\\n1 := RF,m′(u1), R2 := RF,m(u2), and R′\\n2 := RF,m′(u2) Thus,\\n(Hπ\\nF,m · Hπ\\nF,m′)u1,u2 = (Hπ\\nF,m′ · Hπ\\nF,m)u1,u2. It is easy to see that the matrices commute if and\\nonly if (m, m′) have the intersecting ray property.\\n□\\nLemma 4.16. Let H1, . . . , Hn ∈Rn×n be pairwise commuting matrices. Then any eigenvalue\\nof Pn\\ni=1 Hi has the form λ1 + · · · + λn where λi is an eigenvalue of Hi.\\nProof. This is a straightforward extension of the case n = 2 in [12, Theorem 2.4.8.1] and\\nrelies on the fact that commuting matrices are simultaneously triangularizable.\\n□\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n13\\nProposition 4.17. Let F ⊂Zd and M ⊂Zd be ﬁnite sets and suppose there exists m ∈M\\nsuch that (m, m′) has the intersecting ray property in F for all m′ ∈M′ := M \\\\ {m}. Let\\nV1, . . . , Vc be the connected components of F(M′), πi : Vi →[0, 1] the uniform distribution,\\nand f ′ = (1 −f(m))−1 · f|M′, then\\nλ(Hπ,f\\nF,M) ≤f(m) + (1 −f(m)) · max{λ(Hπi,f′\\nVi,M′) : i ∈[c]}.\\nProof. Let H := Hπ,f′\\nF,M′ be the heat-bath random walk on F(M) that samples moves from M′\\naccording to f ′, then Hπ,f\\nF,M = f(m)·Hπ\\nF,m +(1−f(m))·H. By assumption, all pairs (m, m′)\\nwith m′ ∈M′ have the intersecting ray property and thus the matrices Hπ\\nF,m and H commute\\naccording to Proposition 4.15. The eigenvalues of all involved matrices are non-negative and\\nthus Lemma 4.16 implies that the second largest eigenvalue of Hπ,f\\nF,M has the form λ + λ′\\nwhere λ ∈{0, f(m)} by Proposition 4.6 and where λ′ is an eigenvalue of (1 −f(m)) · H. The\\nmatrix H is a block matrix whose building blocks are the matrices Hπ,f′\\nVi,M′ = Hπi,f′\\nVi,M′ and thus\\nthe statement follows.\\n□\\nProposition 4.18. Let F ⊂Zd and M ⊂Zk be ﬁnite sets. If F(M) has the intersecting\\nray property, then λ(Hπ,f\\nF,M) ≤1 −min(f).\\nProof. Let M = {m1, . . . , mk}.\\nThe intersecting ray property and Proposition 4.15 give\\nthat the matrices f(m1) · Hπ\\nF,mi, . . . , f(mk) · Hπ\\nF,mk commute pairwise. According to Propo-\\nsition 4.6, the eigenvalues of f(mi) · Hπ\\nF,mi are {0, f(mi)}. Lemma 4.16 gives that the second\\nlargest eigenvalue of Hπ,f\\nF,M, which equals the second largest eigenvalue modulus since all of\\nits eigenvalues are non-negative, fulﬁlls λ(Hπ,f\\nF,M) = P\\ni∈I f(mi) for a subset I ⊆[k]. Since\\nλ(Hπ,f\\nF,M) < 1 and Pk\\ni=1 f(mi) = 1, we have I ̸= [k] and the claim follows.\\n□\\nProposition 4.19. Let n1, . . . , nd ∈N>1, F = [n1]×· · ·×[nd], and M = {e1, . . . , ed}. Then\\nfor any positive mass function f : M →[0, 1], λ(Hπ,f\\nF,M) = 1 −min(f).\\nProof. It is easy to verify that Fc(M) has the intersecting ray property and thus Proposi-\\ntion 4.18 shows λ(Hπ,f\\nF,M) ≤1−min(f). Assume that min(f) = f(ei). The connected compo-\\nnents of Fc({e1, . . . , ed} \\\\ {ei}) are the layers Vj := {u ∈F : ui = j} for any j ∈[ni] and the\\nrays through F parallel are Rk := {(0, k)+s·ei : s ∈[ni]} for k = (k1, . . . , ki−1, ki+1, . . . , kd) ∈\\n[n1] × · · · × [ni−1] × [ni+1] × · · · × [nd]. In particular, any ray intersects any connected com-\\nponent exactly once.\\nThus, the matrix (|Rk ∩Vj|)k,j is the all-ones matrix, which has a\\nnon-trivial kernel. Proposition 4.12 implies λ(Hπ,f\\nF,M) ≥1 −f(ei).\\n□\\nRemark 4.20. In the special case n := n1 = · · · = nd and f : {e1, . . . , ed} →[0, 1] the\\nuniform distribution in Proposition 4.19, the heat-bath random walk on [n]d is known as\\nRook’s walk in the literature. In this case, Proposition 4.19 is exactly [13, Proposition 2.3].\\nIn [18], upper bounds on the mixing time of the Rook’s walk were obtained with path-coupling.\\nThe stationary distribution of the heat-bath random walk is independent of the actual\\nmass function on the Markov moves. The problem of ﬁnding the mass function which leads\\n14\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nto the fastest mixing behaviour can be formulated as the following optimization problem:\\n(4.2)\\narg min\\n(\\nλ(Hπ,f\\nF,M) : f : M →(0, 1),\\nX\\nm∈M\\nf(m) = 1\\n)\\n.\\nIt follows from Proposition 4.19 that the optimal value of (4.2) for F = [n1] × · · · × [nd],\\nM = {e1, . . . , ed}, and the uniform distribution π on F is the uniform distribution on M.\\nAnother example where the uniform distribution is the optimal solution to (4.2), but where\\nthe veriﬁcation is more involved, is presented in Example 4.21.\\nExample 4.21. Let F = [2] × [5] as in Example 4.4 and consider M = {e1, 2e1 + e2}. We\\ninvestigate for which µ ∈(0, 1), the transition matrix µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2 has the\\nsmallest second largest eigenvalue modulus. Its characteristic polynomial in Q[µ, x] is\\n−1\\n25x4(x −1)(µ + x −1)6(−5x2 + 5x + 2µ2 −2µ)(−5x2 + 5x + 4µ2 −4µ)\\nand hence its eigenvalues are\\nx1(µ) := 1,\\nx2(µ) := 1 −µ,\\nx3(µ) := 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx4(µ) := 1\\n2\\n\"\\n1 −\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n,\\nx5(µ) := 1\\n2\\nh\\n1 +\\np\\n1 + 4(µ2 −µ)\\ni\\n,\\nx6(µ) := 1\\n2\\nh\\n1 −\\np\\n1 + 4(µ2 −µ)\\ni\\n.\\nIt is straightforward to check that x5(µ) > 1\\n2 > x6(µ), x3(µ) > 1\\n2 > x4(µ). Since µ2 −µ < 0\\nfor u ∈(0, 1) and x3(µ) ≥x6(µ). We can show that x4(µ) ≥x2(µ) and thus\\nλ(µHπ\\nF,e1 + (1 −µ)Hπ\\nF,2e1+e2) = 1\\n2\\n\"\\n1 +\\nr\\n1 + 8\\n5(µ2 −µ)\\n#\\n.\\nThe fastest heat-bath random walk on F(M) which converges to uniform is thus obtained for\\nµ = 1\\n2, i.e. when the moves are selected uniformly. The second largest eigenvalue in this case\\nis\\n1\\n10(5 +\\n√\\n15) ≈0.887, which is larger than the second largest eigenvalue of the heat-bath\\nwalk that selects uniformly from {e1, e2} (see Proposition 4.19).\\n5. Augmenting Markov bases\\nIt follows from our investigation in Section 3 that the diameter of all compressed ﬁber\\ngraphs coming from a ﬁxed integer matrix A ∈Zm×d can be bounded from above by a\\nconstant. However, Markov moves can be used twice in a minimal path which can make the\\ndiameter of the compressed ﬁber graph larger than the size of the Markov basis. The next\\ndeﬁnition puts more constraints on the Markov basis and postulates the existence of a path\\nthat uses every move from the Markov basis at most once.\\nDeﬁnition 5.1. Let F ⊂Zd be a ﬁnite set and M = {m1, . . . , mk} ⊂Zd. An augmenting\\npath between distinct u, v ∈F of length r ∈N is a path in Fc(M) of the form\\nu →u + λi1mi1 →u + λi1mi1 + λi2mi2 →· · · →u +\\nr\\nX\\nk=1\\nλikmik = v\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n15\\nwith distinct indices i1, . . . , ir ∈[k]. An augmenting path is minimal for u, v ∈F if there\\nexists no shorter augmenting path between u and v in Fc(M).\\nA Markov basis M for\\nF is augmenting if there is an augmenting path between any distinct nodes in F.\\nThe\\naugmentation length AM(F) of an augmenting Markov basis M is the maximum length of\\nall minimal augmenting paths in Fc(M).\\nNot every Markov basis is augmenting (see Example 3.11), but the diameter of compressed\\nﬁber graphs that use an augmenting Markov basis is at most the number of the moves. For\\nﬁber graphs coming from an integer matrix, an augmenting Markov basis for all of its ﬁbers\\ncan be computed (Remark 5.2).\\nRemark 5.2. Let A ∈Zm×d with kerZ(A) ∩Nd = {0} and let b ∈NA. The Graver basis\\nis an augmenting Markov basis for FA,b for any b ∈NA. We claim that when A is totally\\nunimodular, then AGA(FA,b) ≤d2(rank(A) + 1).\\nIn particular, the augmentation length\\nis independent of the right-hand side b.\\nLet u, v ∈FA,b be arbitrary and for i ∈N, let\\nli := min{ui, vi}, wi := max{ui, vi}, and ci := sign(ui −vi) ∈{−1, 0, 1}. Then v is the unique\\noptimal value of the linear integer optimization problem\\nmin{cT x : Ax = b, l ≤x ≤w, x ∈Zd}.\\nA discrete steepest decent as deﬁned in [5, Deﬁnition 3] using Graver moves needs at most\\n∥c∥1 · d · (rank(A) + 1) ≤d2 · (rank(A) + 1) many augmentations from u to reach the optimal\\nvalue v. We refer to [5, Corollary 8] which ensures that every Graver move is used at most\\nonce. Note that in [5], x is constrained to x ≥0 instead to x ≥l, but their argument works\\nfor any lower bound.\\nExample 5.3. Fix d ∈N and consider A and M from Example 3.5. We show that M is an\\naugmenting Markov basis for FA,b for any b ∈N. Let u, v ∈FA,b be distinct, then there exists\\ni ∈[d] such that ui > vi or ui < vi, thus, we can walk from u to u′ := u + (ui −vi)(e1 −ei)\\nor from v to v′ := v + (vi −ui)(e1 −ei). In any case, after that augmentation, the pairs\\n(u′, v) and (v′, u) coincide in the ith coordinate and thus we ﬁnd an augmenting path by\\ninduction on the dimension d. We have used at most d −1 many edges in these paths and\\nhence AM(FA,b) ≤d −1 for all b ∈N.\\nWe now show that the augmentation length is essentially bounded from below by the\\ndimension of the node set and hence the bound observed in Example 5.3 cannot be improved.\\nWe ﬁrst need the following lemma.\\nLemma 5.4. Let v1, . . . , vk ∈Qd such that any v ∈spanQ {v1, . . . , vk} can be represented by\\na linear combination of r vectors. Then dim(spanQ {v1, . . . , vk}) ≤r.\\nProof. Let B ⊂P(v1, . . . , vk) the set of all subsets of cardinality r. By our assumption,\\n∪B∈BspanQ {B} = spanQ {v1, . . . , vk}. Since dim(spanQ {B}) ≤r for all B ∈B and since B\\nis ﬁnite, the claim follows.\\n□\\nProposition 5.5. Let P ⊂Qd be polytope and let M ⊂Zd be an augmenting Markov basis\\nfor Fi := (i · P) ∩Zd for all i ∈N. Then dim(P) ≤maxi∈N AM(Fi).\\n16\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nProof. Without restricting generality, we can assume that 0 ∈P. Let V := spanQ {P} be\\nthe Q-span of P, then dim(P) = dim(V ). We must have dim(spanQ {M}) = dim(V ) since\\ndim(P) = dim(convQ(Fi)) for i suﬃciently large and since M is a Markov basis for Fi. Deﬁne\\nr := maxi∈N AM(Fi) and choose any non-zero v ∈V and u ∈relint(P) ⊂Qd. Then there\\nexists δ ∈Q>0 such that u + δv ∈P. Thus, 1\\nδu + v ∈1\\nδP. Let c ∈N≥1 such that i := c\\nδ ∈N\\nand w := c\\nδu ∈Zd. Then w + cv = c(1\\nδ u + v) ∈(i · P) ∩Zd = Fi. By assumption, there exists\\nan augmenting path from w to w + cv using only r elements from M. Put diﬀerently, the\\nelement cv from V can be represented by a linear combination of r vectors from M. Since v\\nwas chosen arbitrarily, Lemma 5.4 implies dim(P) = dim(V ) ≤r.\\n□\\nRemark 5.6. It is a consequence from Proposition 5.5 that for any matrix A ∈Zm×d with\\nkerZ(A) ∩Nd = {0} and an augmenting Markov basis M, there exists F ∈PA such that\\nAM(F) ≥dim(kerZ(A)).\\nLet us now shortly recall the framework from [23] which is necessary to prove our main\\ntheorem. Let G = (V, E) be a graph. For any ordered pair of distinct nodes (x, y) ∈V × V ,\\nlet px,y ⊆E be a path from x to y in G and let Γ := {px,y : (x, y) ∈V × V, x ̸= y} be\\nthe collection of these paths, then Γ is a set of canonical paths. Let for any edge e ∈E,\\nΓe := {p ∈Γ : e ∈p} be the set of paths from Γ that use e. Now, let H : V × V →[0, 1] be a\\nsymmetric random walk on G and deﬁne\\nρ(Γ, H) := max{|p| : p ∈Γ}\\n|V |\\n· max\\n{u,v}∈E\\n|Γ{u,v}|\\nH(u, v).\\nObserve that symmetry of H is needed to make ρ(Γ, H) well-deﬁned. This can be used to\\nprove the following upper bound on the second largest eigenvalue.\\nLemma 5.7. Let G be a graph, H be a symmetric random walk on G, and Γ be a set of\\ncanonical paths in G. Then λ2(H) ≤1 −\\n1\\nρ(Γ,H).\\nProof. The stationary distribution of H is the uniform distribution and thus the statement\\nis a direct consequence of [23, Theorem 5], since ρ(Γ, H) is an upper bound on the constant\\ndeﬁned in [23, equation 4].\\n□\\nTheorem 5.8. Let F ⊂Zd be ﬁnite and let M := {m1, . . . , mk} ⊂Zd be an augmenting\\nMarkov basis. Let π be the uniform and f be a positive distribution on F and M respectively.\\nFor i ∈[k], let ri := max{|RF,mi(u)| : u ∈F} and suppose that r1 ≥r2 ≥· · · ≥rk. Then\\nλ(Hπ,f\\nM,F) ≤1 −\\n|F| · min(f)\\nAM(F) · AM(F)! · 3AM(F)−1 · 2|M| · r1r2 · · · rAM(F)\\n.\\nProof. Choose for any distinct u, v ∈F an augmenting path pu,v of minimal length in Fc(M)\\nand let Γ be the collection of all these paths. Let u + µmk = v be an edge in Fc(M), then\\nour goal is to bound |Γ{u,v}| from above. Let S := {S ⊆[r] : |S| ≤AM(F), k ∈S} and take\\nany path px,y ∈Γ{u,v}. Then there exists S := {i1, . . . , is} with s := |S| ≤AM(F) such that\\nx + Ps\\nk=1 λikmik = y. Since px,y uses the edge {u, v}, there is j ∈[s] such that ij = k and\\nλij = µ. Since |λik| ≤rik, there are at most\\ns! · (2ri1 + 1) · · · (2rij−1 + 1) · (2rij+1 + 1) · · · (2ris + 1) ≤s! · 3s−1\\nY\\nt∈S\\\\{k}\\nrt\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n17\\npaths in Γ{u,v} that uses the edge {u, v} and the moves mi1, . . . , mij−1, mij+1 . . . , mis. Since\\nall the paths are minimal, they have length at most AM(F) so indeed every path in Γ has\\nthat form.\\n|Γu,v|\\nHπ,f\\nF,M(u, v)\\n≤3AM(F)−1\\nP\\nS∈S\\n\\x10\\n|S|! Q\\nt∈S\\\\{k} rt\\n\\x11\\nf(mij) ·\\n1\\n|Rmij (u)|\\n≤3AM(F)−1 · AM(F)! · |S| · r1r2 . . . rAM(F)\\nf(mij)\\n,\\nwhere we have used the assumption r1 ≥r2 ≥· · · ≥rk. Bounding |S| rigorously from above\\nby 2|M|, the claim follows from Lemma 5.7.\\n□\\nDeﬁnition 5.9. Let F ⊂Zd and M ⊂Zd be ﬁnite sets. The longest ray through F along\\nvectors of M is RF,M := arg max{|RF,m(u)| : m ∈M, u ∈F}.\\nCorollary 5.10. Let (Fi)i∈N be a sequence of ﬁnite sets in Zd and let πi be the uniform\\ndistribution on Fi. Let M ⊂Zd be an augmenting Markov basis for Fi with AM(Fi) ≤\\ndim(Fi) and suppose that (|RFi,M|)dim(Fi))i∈N ∈O(|Fi|)i∈N.\\nThen for any positive mass\\nfunction f : M →[0, 1], there exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. This is a straightforward application of Theorem 5.8.\\n□\\nCorollary 5.11. Let P ⊂Zd be a polytope, Fi := (i · P) ∩Zd for i ∈N, and let πi be the\\nuniform distribution on Fi. Suppose that M ⊂Zd is an augmenting Markov basis {Fi : i ∈N}\\nsuch that AM(Fi) ≤dim(P) for all i ∈N. Then for any positive mass function f : M →[0, 1],\\nthere exists ǫ > 0 such that λ(Hπi,f\\nFi,M) ≤1 −ǫ for all i ∈N.\\nProof. Let r := dim(P). We ﬁrst show that (|RFi,M|)i∈N ∈O(i)i∈N. Write M = {m1, . . . , mk}\\nand denote by li := max{|(u + mi · Z) ∩P| : u ∈P} be the length of the longest ray through\\nthe polytope P along mi. It suﬃces to prove that i · (lk + 1) is an upper bound on the\\nlength of any ray along mk through Fi. For that, let u ∈Fi such that u + λmk ∈Fi for\\nsome λ ∈N, then 1\\ni u + λ\\ni mk ∈P and thus ⌊λ\\ni ⌋≤lk, which gives λ ≤i · (lk + 1). With\\nC := max{l1, . . . , lk} + 1 we have |RFi,M| ≤C · i. Ehrhart’s theorem [2, Theorem 3.23]\\ngives (|Fi|)i∈N ∈Ω(ir)i∈N and since |RFi,M| ≤C · i, we have (|RFi,M|r)i∈N ∈O(|Fi|)i∈N. An\\napplication of Corollary 5.10 proves the claim.\\n□\\nExample 5.12. Fix d, r ∈N and let Cd,r := {u ∈Zd : ∥u∥1 ≤r} be the set of integers of the\\nd-dimensional cross-polytope with radius r. The set Md = {e1, . . . , ed} is a Markov basis for\\nCd,r for any r ∈N. We show that Md is an augmenting Markov basis whose augmentation\\nlength is at most d. For that, let u, v ∈Cd,r distinct elements. We claim that there exists\\ni ∈[d] such that xi ̸= vi and ui + (vi −ui) ∈Cd,r. Let S ⊆[d] be the set of indices where u\\nand v diﬀer and let s = r −||u||1. If |S| = 1, then the result is clear so suppose |S| ≥2. If\\nthe result doesn’t hold then for all i ∈S, |vi| −|ui| > s. It follows that\\n∥v∥1 =\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\n|vi| >\\nX\\ni/∈S\\n|ui| +\\nX\\ni∈S\\ns + |ui| = |Suv| · s + ∥u∥1 = (|S| −1) · s + r.\\nBut we assumed that v ∈Cd,r. It follows that for any pair of points u, v in Cd,r, there is a\\nwalk, using the unit vectors as moves, that uses each move at most once. Corollary 5.10 yield\\nthat for any d ∈N, the second largest eigenvalue modulus of the heat-bath random walk on\\nCd,r with uniform as stationary distribution can be strictly bounded away from 1 for r →∞.\\n18\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\nThe bound on the second largest eigenvalue in Theorem 5.8 is quite general and can be\\nimproved vastly, provided one has better control over the paths. For example, this can be\\nachieved for hyperrectangles intersected with a halfspace.\\nProposition 5.13. Let a ∈Nd\\n>0, b ∈N, F = {u ∈Nd : aT · u ≤b}, and M := {e1, . . . , ed}.\\nIf π and f are the uniform distributions on F and M respectively, then\\nλ(Hπ,f\\nF,M) ≤1 −|F|\\nd2\\nd\\nY\\ni=1\\nai\\nb .\\nProof. Observe that M is a Markov basis for F since all nodes are connected with 0 ∈F.\\nLet u, v ∈F be distinct. We ﬁrst show that there exists k ∈[d] such that uk ̸= vk and\\nu + (vk −uk)ek ∈F. If u ≤v, the statement trivially holds. Otherwise, there exists k ∈[d]\\nsuch that uk > vk and the vector obtained by replacing the kth coordinate of u by vk remains\\nin F. Now, consider for the following path between u and v: Choose the smallest index\\nk ∈[d] such that uk ̸= vk and such that u + (vk −uk) · ek ∈F and proceed recursively with\\nu + (vk −uk) and v. This gives a path pu,v between u and v of length at most d. Let Γ be\\nthe collection of all these paths. We want to apply Lemma 5.7. Thus, let x ∈F and consider\\nthe edge x →x + c · es. Let us count the paths pu,v that use that edge. Let u, v ∈F and let\\nk1, . . . , kr ∈[d] be distinct indices such that\\nu →u + (vk1 −uk1)ek1 →u + (vk1 −uk1)ek1 + (vk2 −uk2)ek2 →· · · →v\\nrepresents the path pu,v constructed by the upper rule.\\nAssume that pu,v uses the edge\\n{x, x + ces} and let kl = s and (vkl −ukl) = c. In particular,\\nu + (vk1 −uk1)ek1 + · · · + (vkl−1 −ukl−1)ekl−1 = x\\nx + (vkl −ukl)ekl + · · · + (vkr −ukr)ekr = v.\\nWe see that vkt = xkt for all t < l and that ukt = xkt for all t ≥l. In particular, vkl =\\nukl + c = xkl + c is also ﬁxed. The coordinates ukt and vkt are bounded from above by\\nb\\nakt\\nfor all t ∈[r], and hence there can be at most\\n l−1\\nY\\nt=1\\nb\\nakt\\n!\\n·\\n \\nr\\nY\\nt=l+1\\nb\\nakt\\n!\\n.\\nSince k1, . . . , kt are distinct coordinate indices, we have\\n|Γx,x+c·es|\\nHπ,f\\nF,M(x, x + c · es)\\n≤d ·\\nd\\nY\\ni=1\\nb\\nai\\n.\\nLemma 5.7 ﬁnishes the proof.\\n□\\nIn ﬁxed dimension, Proposition 5.13 leads to rapid mixing, but for d →∞, no statement\\ncan be made. In [19], it was shown that the simple walk with an additional halting probability\\non {u ∈Nd : atu ≤b} ∩{0, 1}d has mixing time in O(d4.5+ǫ). For zero-one polytopes, simple\\nand heat-bath walk coincide and we are conﬁdent that a similar statement holds without the\\nrestriction on zero-one polytopes.\\nHEAT-BATH RANDOM WALKS WITH MARKOV BASES\\n19\\nThe heat-bath random walk mixes rapidly when an augmenting Markov basis with a small\\naugmentation length is used.\\nWe think that it is interesting to question how might an\\naugmenting Markov bases be obtained and how their augmentation length can be improved.\\nQuestion 5.14. Let M be an augmenting Markov basis of A. Can we ﬁnd ﬁnitely many\\nmoves m1, . . . , mk such that the augmentation length of M∪{m1, . . . , mk} on FA,b is at most\\ndim(kerZ(A)) for all b ∈NA?\\nReferences\\n1. Stephen Baumert, Archis Ghate, Seksan Kiatsupaibul, Yanfang Shen, Robert L. Smith, and Zelda B.\\nZabinsky, Discrete Hit-and-Run for Sampling Points from Arbitrary Distributions Over Subsets of Integer\\nHyperrectangles, Operations Research 57 (2009), no. 3, 727–739.\\n2. Matthias Beck and Sinai Robins, Computing the Continuous Discretely, Springer, New York, 2007.\\n3. Mary Cryan, Martin Dyer, Leslie Ann Goldberg, Mark Jerrum, and Russell Martin, Rapidly Mixing\\nMarkov Chains for Sampling Contingency Tables with a Constant Number of Rows, SIAM Journal on\\nComputing 36 (2006), no. 1, 247–278.\\n4. Jes´us A. De Loera, Raymond Hemmecke, and Matthias K¨oppe, Algebraic and Geometric Ideas in the\\nTheory of Discrete Optimization, MPS-SIAM Series on Optimization, SIAM, Cambridge, 2013.\\n5. Jes´us A. De Loera, Raymond Hemmecke, and Jon Lee, On Augmentation Algorithms for Linear and\\nInteger-Linear Programming: From Edmonds–Karp to Bland and Beyond, SIAM Journal on Optimization\\n25 (2015), no. 4, 2494–2511.\\n6. Persi Diaconis and Bernd Sturmfels, Algebraic algorithms for sampling from conditional distributions, The\\nAnnals of statistics 26 (1998), no. 1, 363–397.\\n7. Mathias Drton, Bernd Sturmfels, and Seth Sullivant, Lectures on algebraic statistics, Oberwolfach Semi-\\nnars, vol. 39, Springer, Berlin, 2009, A Birkh¨auser book.\\n8. Martin Dyer, Catherine Greenhill, and Mario Ullrich, Structure and eigenvalues of heat-bath Markov\\nchains, Linear Algebra and its Applications 454 (2014), 57–71.\\n9. Giles Gardam, Expander Graphs and Kazhdan’ s Property (T), Bachelor’s thesis, University of Sidney,\\n2012.\\n10. Hisayuki Hara, Akimichi Takemura, and Ruriko Yoshida, On connectivity of ﬁbers with positive marginals\\nin multiple logistic regression, Journal of Multivariate Analysis (2010), 1–26.\\n11. Raymond Hemmecke and Peter N. Malkin, Computing generating sets of lattice ideals and Markov bases\\nof lattices, Journal of Symbolic Computation 44 (2009), no. 10, 1463–1476.\\n12. Roger A. Horn, Matrix Analysis, 2nd ed., Cambridge University Press, New York, 2013.\\n13. Steven S. Kim, Mixing Time of a Rook’s Walk, Undergraduate certiﬁcate paper (2012).\\n14. David A. Levin, Yuval Peres, and Elisabeth L. Wilmer, Markov chains and mixing times, American\\nMathematical Society, Providence, RI, 2009.\\n15. L´aszl´o Lov´asz, Hit-and-run mixes fast, Mathematical Programming 86 (1999), no. 3, 443–461.\\n16. L´aszl´o Lov´asz and Santosh Vempala, Hit-and-Run from a Corner, SIAM Journal on Computing 35 (2006),\\nno. 4, 985–1005.\\n17. Peter N. Malkin, Computing Markov bases, Gr¨obner bases, and extreme rays, Phd thesis, 2007, p. 223.\\n18. Cam McLeman, Peter T. Otto, John Rahmani, and Matthew Sutter, Mixing times for the Rook’s walk\\nvia path coupling, to appear in Involve (2016), 1–12.\\n19. Ben Morris and Alistair Sinclair, Random Walks on Truncated Cubes and Sampling 0-1 Knapsack Solutions,\\nSIAM Journal on Computing 34 (2004), no. 1, 195–226.\\n20. Samu Potka, Higher connectivity of ﬁber graphs of Gr¨obner bases, Journal of Algebraic Statistics 4 (2013),\\nno. 1, 93–107.\\n21. Johannes Rauh and Seth Sullivant, Lifting Markov bases and higher codimension toric ﬁber products,\\nJournal of Symbolic Computation 74 (2016), 276–307.\\n22. Andr´as Seb¨o, Hilbert Bases, Caratheodory’s Theorem and Combinatorial Optimization, (1990), 431–455.\\n20\\nCAPRICE STANLEY AND TOBIAS WINDISCH\\n23. Alistair Sinclair, Improved Bounds for Mixing Rates of Markov Chains and Multicommodity Flow, Com-\\nbinatorics, Probability and Computing 1 (1992), no. 4, 351–370.\\n24. Bernd Sturmfels, Gr¨obner bases and convex polytopes, American Mathematical Society, Providence, R.I.,\\n1996.\\n25. Seth Sullivant, Markov bases of binary graph models, Annals of Combinatorics 7 (2003), 441–466.\\n26. Santosh S. Vempala, Geometric Random Walks: A Survey, MSRI Combinatorial and Computational\\nGeometry 52 (2005), 573–612.\\n27. Tobias Windisch, Rapid mixing and Markov bases, preprint, arXiv:1505.03018 (2015), 1–18.\\nNC State University, Raleigh, NC 27695, USA\\nE-mail address: crstanl2@ncsu.edu\\nOtto-von-Guericke Universit¨at, Magdeburg, Germany\\nE-mail address: windisch@ovgu.de\\n')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
